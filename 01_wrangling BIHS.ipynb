{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Explore and visualize the dataset.\n",
    "- Build clean the data set and missing values.\n",
    "- Generate a set of insights from the dataset.\n",
    "\n",
    "### The key question\n",
    "\n",
    "Do natural disasters worsen the gender-based nutritional disparity among children in Bangladesh?\n",
    "\n",
    "### Problem Formulation:\n",
    "\n",
    "We have a regression problem at hand where we will try to run a diff in diff to see if we can find evidence for a gender bais as.\n",
    "\n",
    "### Proposed approach\n",
    "\n",
    "Since it is a regression problem we will first start with the parametric model linear regression with a diff-in-diff approache.\n",
    "\n",
    "### Overall solution design :\n",
    "\n",
    "The potential solution design would look like this:\n",
    "\n",
    "- Checking the data description to get the idea of basic statistics or summary of data.\n",
    "- Univariate analysis to see how data is spread out, getting to know about the outliers.\n",
    "- Bivariate analysis to see how different attributes vary with the dependent variable.\n",
    "- Outlier treatment if needed.\n",
    "- Missing value treatment using appropriate techniques.\n",
    "- Feature engineering - transforming features, creating new features if possible.\n",
    "- Choosing the model evaluation technique - 1) R Squared 2) RMSE can be any other metrics related to regression analysis.\n",
    "- Splitting the data and proceeding with modeling.\n",
    "\n",
    "\n",
    "### Measures of success :\n",
    "\n",
    "R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries and overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the basic libraries we will require for the project\n",
    "\n",
    "# Import libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# Import libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Slightly advanced library for data visualization            \n",
    "import seaborn as sns      \n",
    "\n",
    "# Import necessary modules\n",
    "import geopandas as gpd\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "\n",
    "# import module for geoencoding\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# add sleep time\n",
    "from time import sleep\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up the color sheme:\n",
    "import mapclassify as mc\n",
    "\n",
    "# to compute zscores: https://pypi.org/project/cgmzscore/\n",
    "# Resource R: https://rdrr.io/github/WorldHealthOrganization/anthroplus/man/anthroplus_zscores.html\n",
    "#from cgmzscore.src.main import z_score_lhfa\n",
    "#from cgmzscore.src.main import z_score_wfa\n",
    "#import ast\n",
    "#https://github.com/ewheeler/pygrowup\n",
    "#from pygrowup import Observation\n",
    "#from decimal import Decimal\n",
    "from dbfread import DBF\n",
    "import datetime\n",
    "# Release memory using gc : The gc module to manually trigger garbage collection. \n",
    "# Garbage collection is the process of freeing memory that is no longer being used by the program. \n",
    "# By manually triggering garbage collection, you can release memory that is no longer needed.\n",
    "import gc\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def return_non_na(curr_data,col):\n",
    "#     return curr_data[col].replace(['HWHAZWHO', 'HWWAZWHO', 'HWWHZWHO','HWBMIZWHO','Missing'], np.NaN, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def reverse_geocode(geolocator, latlon, sleep_sec):\n",
    "    \"\"\"\n",
    "    This function attempts to perform reverse geocoding using the provided geolocator\n",
    "    and latitude-longitude coordinates. It handles potential errors and retries on timeouts.\n",
    "\n",
    "    Args:\n",
    "        geolocator (object): A geocoding library object used for reverse geocoding.\n",
    "        latlon (str): A string in the format \"latitude,longitude\" representing the location.\n",
    "        sleep_sec (int): The maximum number of seconds to sleep between retries on timeouts.\n",
    "\n",
    "    Returns:\n",
    "        object: The result of the reverse geocoding request (may vary depending on the geocoder library).\n",
    "                - On success, returns the geocoded information.\n",
    "                - On timeout, retries up to sleep_sec seconds and returns the result.\n",
    "                - On service error or other exceptions, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Attempt reverse geocoding using the geolocator in English language\n",
    "        return geolocator.reverse(latlon, language='en')\n",
    "    except GeocoderTimedOut:\n",
    "        # Handle timeout error\n",
    "        logging.info('TIMED OUT: GeocoderTimedOut: Retrying...')\n",
    "        # Introduce a random sleep between 1 and sleep_sec seconds to avoid overwhelming the service\n",
    "        sleep(randint(1 * 100, sleep_sec * 100) / 100)\n",
    "        # Retry the reverse geocoding with the same parameters\n",
    "        return reverse_geocode(geolocator, latlon, sleep_sec)\n",
    "    except GeocoderServiceError as e:\n",
    "        # Handle service error (e.g., connection refused)\n",
    "        logging.info('CONNECTION REFUSED: GeocoderServiceError encountered.')\n",
    "        logging.error(e)  # Log the detailed error for debugging\n",
    "        return None  # Indicate failure\n",
    "    except Exception as e:\n",
    "        # Handle unexpected exceptions\n",
    "        logging.info('ERROR: Terminating due to exception {}'.format(e))\n",
    "        return None  # Indicate failure\n",
    "    \n",
    "\n",
    "def get_local_information(curr_data, sleep_sec):\n",
    "    \"\"\"\n",
    "    This function enriches a DataFrame with local address information based on latitude and longitude data.\n",
    "\n",
    "    Args:\n",
    "        curr_data (pandas.DataFrame): A DataFrame with columns 'LATNUM' and 'LONGNUM' containing latitude and longitude values.\n",
    "        sleep_sec (int): Number of seconds to sleep between retries for error handling.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with additional columns for city, village, county, state_district, state, and country.\n",
    "        \"\"\"\n",
    "\n",
    "    # Initialize the Nominatim geocoder with a user agent string (important for API usage guidelines)\n",
    "    user_agent = 'bibouPaultest_{}'.format(randint(10000, 99999)) + '@gmail.com'\n",
    "    geolocator = Nominatim(user_agent=user_agent)\n",
    "\n",
    "    # Iterate through each row of the DataFrame\n",
    "    for curr_line in range(curr_data.index.max() + 1):\n",
    "\n",
    "        # Extract latitude and longitude values for the current row\n",
    "        Latitude = str(curr_data.loc[curr_line, \"LATNUM\"])\n",
    "        Longitude = str(curr_data.loc[curr_line, \"LONGNUM\"])\n",
    "\n",
    "        # Perform reverse geocoding to get address information\n",
    "        location = reverse_geocode(geolocator, Latitude + \",\" + Longitude, sleep_sec)  # Uses a custom function for retries\n",
    "\n",
    "        # Extract relevant address components from the geocoding result\n",
    "        if location:\n",
    "            address = location.raw['address']\n",
    "            city = address.get('city')\n",
    "            village = address.get('village')\n",
    "            county = address.get('county')\n",
    "            state_district = address.get('state_district')\n",
    "            state = address.get('state')\n",
    "            country = address.get('country')\n",
    "\n",
    "        # Update the DataFrame with the extracted address information\n",
    "            curr_data.loc[curr_line, 'city'] = city\n",
    "            curr_data.loc[curr_line, 'village'] = village\n",
    "            curr_data.loc[curr_line, 'county'] = county\n",
    "            curr_data.loc[curr_line, 'state_district'] = state_district\n",
    "            curr_data.loc[curr_line, 'state'] = state\n",
    "            curr_data.loc[curr_line, 'country'] = country\n",
    "\n",
    "        else:\n",
    "            # Update the DataFrame with the extracted address information\n",
    "            curr_data.loc[curr_line, 'city'] = np.NAN\n",
    "            curr_data.loc[curr_line, 'village'] = np.NAN\n",
    "            curr_data.loc[curr_line, 'county'] = np.NAN\n",
    "            curr_data.loc[curr_line, 'state_district'] = np.NAN\n",
    "            curr_data.loc[curr_line, 'state'] = np.NAN\n",
    "            curr_data.loc[curr_line, 'country'] = np.NAN\n",
    "    \n",
    "    # Return the DataFrame with the added address information\n",
    "    return curr_data\n",
    "\n",
    "\n",
    "def correct_id(curr_row, nbr_max):\n",
    "    \"\"\"\n",
    "    This function ensures that the input string `curr_row` is formatted to a specified length `nbr_max` \n",
    "    by adding leading zeros if necessary.\n",
    "\n",
    "    Parameters:\n",
    "    curr_row (str): The current row ID that needs to be corrected.\n",
    "    nbr_max (int): The maximum length that the corrected ID should have.\n",
    "    \"\"\"\n",
    "    # Check if the length of the current row ID is less than the maximum allowed length\n",
    "    if len(curr_row) < nbr_max:\n",
    "        # Calculate the number of leading zeros needed to reach the maximum length\n",
    "        nbr_resid = nbr_max - len(curr_row) \n",
    "        # Add the leading zeros to the beginning of the current row ID and return as a string\n",
    "        if curr_row.count('.') == 0:\n",
    "            return str(nbr_resid * '0' + curr_row)\n",
    "        else:\n",
    "            # Add extra leading zeros if the current row ID contains a period (.)\n",
    "            return str((nbr_resid + 2) * '0' + curr_row)\n",
    "    # Check if the length of the current row ID is exactly equal to the maximum allowed length\n",
    "    elif len(curr_row) == nbr_max:\n",
    "        # Return the current row ID as a string, adding two leading zeros if it contains a period\n",
    "        if curr_row.count('.') == 0:\n",
    "            return str(curr_row)\n",
    "        else:\n",
    "            return str(2 * '0' + curr_row)\n",
    "    # Case where the length of the current row ID is greater than the maximum allowed length\n",
    "    else: \n",
    "        # Check if the period is in the third position\n",
    "        if curr_row.find('.') == 2: \n",
    "            return str('00' + curr_row)\n",
    "        # Check if the period is in the fourth position\n",
    "        elif curr_row.find('.') == 3: \n",
    "            return str('0' + curr_row)\n",
    "        # Return the current row ID as is\n",
    "        else: \n",
    "            return str(curr_row)\n",
    "        \n",
    "def formatNumber(num):\n",
    "    # Check if the number is an integer (i.e., no fractional part)\n",
    "    if num % 1 == 0:\n",
    "        # If it is an integer, convert it to an integer type and return it\n",
    "        return int(num)\n",
    "    else:\n",
    "        # If it is not an integer, return the number as it is\n",
    "        return num\n",
    "    \n",
    "def vertical_mean_line_survived(x, **kwargs):\n",
    "    \"\"\"\n",
    "    This function draws a vertical line on a plot at the mean value of the input data 'x'.\n",
    "    The style and color of the line are determined by additional keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): The data for which the mean will be calculated.\n",
    "    **kwargs: Additional keyword arguments to customize the line.\n",
    "        'label' (str): A label that can be 'male' or 'female' to determine the line style.\n",
    "        'color' (str): The color of the line, default is green ('g').\n",
    "\n",
    "    Example usage:\n",
    "    vertical_mean_line_survived(data, label='male', color='b')\n",
    "    \"\"\"\n",
    "\n",
    "    # Define line styles for male and female\n",
    "    ls = {\"male\": \"-\", \"female\": \"-\"}\n",
    "    \n",
    "    # Draw a vertical line at the mean of 'x'\n",
    "    plt.axvline(x.mean(), \n",
    "                linestyle=ls[kwargs.get(\"label\")],  # Set line style based on 'label' keyword argument\n",
    "                color=kwargs.get(\"color\", \"g\"))     # Set line color based on 'color' keyword argument, default is green\n",
    "\n",
    "# Example of how the function might be called:\n",
    "# vertical_mean_line_survived(data, label='male', color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1 datasets\n",
    "\n",
    "## HH identification data: Reading household identification data from a Stata file\n",
    "df_r1 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\001_mod_a_male.dta') \n",
    "df_r1_f = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\002_mod_a_female.dta') \n",
    "\n",
    "## FTF: Feed the Futur (FTF) census data from a Stata file\n",
    "df_r1_ftf = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\001_census_ftf.dta') \n",
    "\n",
    "\n",
    "### Sociaux economics characteristics\n",
    "df_r1_SE = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\003_mod_b1_male.dta') \n",
    "\n",
    "## Anthro data: Reading anthropometric data from Stata files\n",
    "### 1 for all household members\n",
    "df_r1_anthr1 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\045_mod_w1_female.dta') \n",
    "\n",
    "### 2 for under five children\n",
    "df_r1_anthr2 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\046_mod_w2_female.dta') \n",
    "\n",
    "# Importing the Bangladesh raw map: Reading a shapefile containing administrative boundaries of Bangladesh\n",
    "#bgd_adm = gpd.read_file(os.getcwd() + '\\\\input\\\\shapefile_data\\\\shapefile_zip\\\\BGD_adm\\\\BGD_adm3.shp')\n",
    "\n",
    "\n",
    "# Crosswalk to merge sociaux economic data to geographic informations\n",
    "crosswalk = pd.read_excel(os.getcwd() + '\\\\input\\\\cross_walk_data_final.xlsx', dtype={'upazila_code':'category','code adm':'category','affected_upazila':'category'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1 = df_r1.loc[:, ['div', 'dcode', 'uzcode', 'uncode','vcode_n', 'div_name', 'District', \n",
    "                    'District_Name', 'Upazila', 'Upazila_Name', 'Union', 'Union_Name','a01', \n",
    "                    'a02', 'a13', 'a15','a16_dd','a16_mm','a16_yy','Sample_type']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1 = df_r1.rename(columns={'div':'division_code', 'District':'district_code', 'District_Name':'district_name','Upazila':'upazila_code','Upazila_Name':'upazila_name',  'Union':'union_code','Union_Name':'union_name','vcode_n':'village_code','a01': \"hh_id\",\n",
    "                    \"a02\": \"census_nbr\", \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\",'a16_dd':'first_v_day','a16_mm':'first_v_month','a16_yy':'first_v_year', 'Sample_type':'sample_type'})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "### Identifiant\n",
    "df_r1['division_code'] = df_r1['division_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1['district_code'] = df_r1['district_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1['upazila_code'] = df_r1['upazila_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1['village_code'] = df_r1['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1['hh_id'] = df_r1['hh_id'].astype(\"int64\").astype(\"string\")\n",
    "\n",
    "df_r1['union_code'] = df_r1['union_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1['census_nbr'] = df_r1['census_nbr'].astype(\"category\")\n",
    "df_r1['sample_type'] = df_r1['sample_type'].astype(\"category\")\n",
    "\n",
    "\n",
    "\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type!='FTF additional')]\n",
    "#df_r1.sample_type.value_counts()\n",
    "\n",
    "df_r1['first_v_day'] = df_r1['first_v_day'].astype(\"int64\")\n",
    "df_r1['first_v_month'] = df_r1['first_v_month'].astype(\"int64\")\n",
    "df_r1['first_v_year'] = df_r1['first_v_year'].astype(\"int64\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type=='FTF Original') | (df_r1.sample_type=='National Representative')]\n",
    "\n",
    "# Creating the interview date\n",
    "df_r1['date_int'] = pd.to_datetime(dict(year=df_r1.first_v_year, month=df_r1.first_v_month, day=df_r1.first_v_day))\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r1.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r1[\"hh_id\"] = df_r1.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r1.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r1[\"upazila_code\"] = df_r1.upazila_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r1.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r1[\"village_code\"] = df_r1.village_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r1.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union_code' column with the corrected codes\n",
    "df_r1[\"union_code\"] = df_r1.union_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh\n",
    "df_r1[\"hh_id_cmplt\"] = df_r1.apply(lambda row : str(row.loc[\"union_code\"]) + str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r1[\"survey_round\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1_f = df_r1_f.loc[:, ['a01', 'div', 'dcode', 'uzcode', 'uncode','vcode_n', 'div_name', 'district', \n",
    "                    'district_name', 'upazila', 'upazila_name', 'union', 'union_name',\n",
    "                    'a02', 'a13', 'a15','a16_dd','a16_mm','a16_yy']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1_f = df_r1_f.rename(columns={'a01': \"hh_id\",'div':'division', \"a02\": \"census_nbr\", \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\", 'district':'district_code', 'district_name':'district_name',\n",
    "                    'upazila':'upazila_code','vcode_n':'village_code', 'upazila_name':'upazila_name', 'union':'union_code', 'union_name':'union_name','a16_dd':'first_v_day','a16_mm':'first_v_month','a16_yy':'first_v_year'})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r1_f['hh_id'] = df_r1_f['hh_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_f['division'] = df_r1_f['division'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_f['village_code'] = df_r1_f['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_f['census_nbr'] = df_r1_f['census_nbr'].astype(\"category\")\n",
    "#df_r1_f['sample_type'] = df_r1_f['sample_type'].astype(\"category\")\n",
    "\n",
    "df_r1_f['first_v_day'] = df_r1_f['first_v_day'].astype(\"int64\")\n",
    "df_r1_f['first_v_month'] = df_r1_f['first_v_month'].astype(\"int64\")\n",
    "df_r1_f['first_v_year'] = df_r1_f['first_v_year'].astype(\"int64\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1_f = df_r1_f.loc[(df_r1_f.sample_type=='ftf original') | (df_r1_f.sample_type=='national representative')]\n",
    "# creating the interview date\n",
    "#df_r1_f['date_int'] = pd.to_datetime(dict(year=df_r1_f.first_v_year, month=df_r1_f.first_v_month, day=df_r1_f.first_v_day))\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r1_f.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r1_f[\"hh_id\"] = df_r1_f.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r1_f.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r1_f[\"upazila_code\"] = df_r1_f.upazila_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r1_f.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r1_f[\"village_code\"] = df_r1_f.village_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r1_f.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union' column with the corrected codes\n",
    "df_r1_f[\"union_code\"] = df_r1_f.union_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh\n",
    "df_r1_f[\"hh_id_cmplt\"] = df_r1_f.apply(lambda row : str(row.loc[\"union_code\"]) + str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r1_f[\"survey_round\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1_ftf = df_r1_ftf.loc[:, ['a01', 'division', 'dcode', 'uzcode', 'uncode', 'vcode', 'district', 'village','distri_a', 'upazila', 'upazil_a', 'union', 'union_na','a1_01', 'a1_02', 'a1_03',\"a1_04\",\"a1_05\",\"a1_06\"]]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1_ftf = df_r1_ftf.rename(columns={'a01': \"hh_id\", 'vcode':'village_name', 'village':'village_code', 'division':'div_name', 'district':'district_code','distri_a':'district_name',\n",
    "                    'upazila':'upazila_code', 'upazil_a':'upazila_name','union':'union_code', 'union_na':'union_name', \"a1_01\": \"sexe_hhm\", \"a1_02\": \"age_hhm\",\n",
    "                    \"a1_03\":\"relation_head_hh\",\"a1_04\":\"marital_status_hhm\",\"a1_05\":\"literacy_hhm\",\"a1_06\":\"education_high\"})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r1_ftf['hh_id'] = df_r1_ftf['hh_id'].astype(\"int64\", errors='ignore').astype(\"string\")\n",
    "df_r1_ftf['div_name'] = df_r1_ftf['div_name'].astype(\"string\")\n",
    "df_r1_ftf['village_code'] = df_r1_ftf['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_ftf['sexe_hhm'] = df_r1_ftf['sexe_hhm'].astype(\"category\")\n",
    "df_r1_ftf['age_hhm'] = df_r1_ftf['age_hhm'].astype(\"int64\")\n",
    "df_r1_ftf['relation_head_hh'] = df_r1_ftf['relation_head_hh'].astype(\"category\")\n",
    "df_r1_ftf['marital_status_hhm'] = df_r1_ftf['marital_status_hhm'].astype(\"category\")\n",
    "df_r1_ftf['literacy_hhm'] = df_r1_ftf['literacy_hhm'].astype(\"category\")\n",
    "df_r1_ftf['education_high'] = df_r1_ftf['education_high'].astype(\"category\")\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "#nbr_max = df_r1_ftf.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "###df_r1_ftf[\"hh_id\"] = df_r1_ftf.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "###nbr_max = df_r1_ftf.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "###df_r1_ftf[\"village_code\"] = df_r1_ftf.village_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r1_ftf[\"survey_round\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\1704833532.py:41: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df_r1_anthr1['if_not_measured_why'] = df_r1_anthr1['if_not_measured_why'].replace({\"have measured\":\"have measured\",\"absent\":\"absent\",\"sick\":\"sick\",\"others\":\"other\",\"refused to  give measurement\":\"refused\"})\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1_anthr1 = df_r1_anthr1.loc[:, ['a01', 'mid', 'w1_01', 'w1_02', 'w1_03','w1_04', 'w1_05']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1_anthr1 = df_r1_anthr1.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"w1_01\": \"are_you_pregnant\", \"w1_02\": \"are_you_lactating\", \n",
    "                            \"w1_03\": \"weight_kg\", 'w1_04':'height_cm', 'w1_05':'if_not_measured_why'})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r1_anthr1['hh_id'] = df_r1_anthr1['hh_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_anthr1['hhm_id'] = df_r1_anthr1['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_anthr1['are_you_pregnant'] = df_r1_anthr1['are_you_pregnant'].astype(\"category\")\n",
    "df_r1_anthr1['are_you_lactating'] = df_r1_anthr1['are_you_lactating'].astype(\"category\")\n",
    "df_r1_anthr1['weight_kg'] = df_r1_anthr1['weight_kg'].astype(\"float64\")\n",
    "df_r1_anthr1['height_cm'] = df_r1_anthr1['height_cm'].astype(\"float64\")\n",
    "# computing the BMI\n",
    "df_r1_anthr1['bmi'] = df_r1_anthr1['weight_kg']*10000/(df_r1_anthr1['height_cm']**2)\n",
    "df_r1_anthr1['if_not_measured_why'] = df_r1_anthr1['if_not_measured_why'].astype(\"category\")\n",
    "#df_r1_anthr1['sample_type'] = df_r1_anthr1['sample_type'].astype(\"category\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1_anthr1 = df_r1_anthr1.loc[(df_r1_anthr1.sample_type=='ftf original') | (df_r1_anthr1.sample_type=='national representative')].drop(columns=['sample_type'], axis=1)\n",
    "df_r1_anthr1.reset_index(inplace=True)\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r1_anthr1.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r1_anthr1[\"hh_id\"] = df_r1_anthr1.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r1_anthr1.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r1_anthr1[\"hhm_id\"] = df_r1_anthr1.hhm_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r1_anthr1[\"member_id\"] = df_r1_anthr1.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "# Correcting the no measured output\n",
    "df_r1_anthr1['if_not_measured_why'] = df_r1_anthr1['if_not_measured_why'].replace({\"have measured\":\"have measured\",\"absent\":\"absent\",\"sick\":\"sick\",\"others\":\"other\",\"refused to  give measurement\":\"refused\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2932269341.py:57: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df_r1_anthr2['if_not_measured_why'] = df_r1_anthr2['if_not_measured_why'].replace({\"have measured\":\"have measured\",\"absent\":\"absent\",\"sick\":\"sick\",\"others\":\"other\",\"refused to  give measurement\":\"refused\"})\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1_anthr2 = df_r1_anthr2.loc[:, ['a01', 'mid', 'w2_01', 'w2_02', 'w2_03', 'w2_04', 'w2_05','w2_07','w2_08', 'w2_10']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1_anthr2 = df_r1_anthr2.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"w2_01\": \"mother_id\", \"w2_02\": \"childbirth_day\", 'w2_03':'if_not_week_month', 'w2_04':'month_birth', 'w2_05':'year_birth',\n",
    "                            \"w2_07\": \"weight_kg\", 'w2_08':'height_cm', 'w2_10':'if_not_measured_why'})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r1_anthr2['hh_id'] = df_r1_anthr2['hh_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_anthr2['hhm_id'] = df_r1_anthr2['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_anthr2['mother_id'] = df_r1_anthr2['mother_id'].astype(\"string\")\n",
    "df_r1_anthr2['childbirth_day'] = df_r1_anthr2['childbirth_day'].astype(\"int64\", errors='ignore')\n",
    "df_r1_anthr2['month_birth'] = df_r1_anthr2['month_birth'].astype(\"int64\", errors='ignore')\n",
    "df_r1_anthr2['year_birth'] = df_r1_anthr2['year_birth'].astype(\"int64\", errors='ignore')\n",
    "df_r1_anthr2['if_not_week_month'] = df_r1_anthr2['if_not_week_month'].astype(\"category\")\n",
    "df_r1_anthr2['weight_kg'] = df_r1_anthr2['weight_kg'].astype(\"float64\")\n",
    "df_r1_anthr2['height_cm'] = df_r1_anthr2['height_cm'].astype(\"float64\")\n",
    "# computing the BMI\n",
    "df_r1_anthr2['bmi'] = df_r1_anthr2['weight_kg']*10000/(df_r1_anthr2['height_cm']**2)\n",
    "df_r1_anthr2['if_not_measured_why'] = df_r1_anthr2['if_not_measured_why'].astype(\"category\")\n",
    "#df_r1_anthr2['sample_type'] = df_r1_anthr2['sample_type'].astype(\"category\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1_anthr2 = df_r1_anthr2.loc[(df_r1_anthr2.sample_type=='ftf original') | (df_r1_anthr2.sample_type=='national representative')].drop(columns=['sample_type'], axis=1)\n",
    "df_r1_anthr2.reset_index(inplace=True)\n",
    "\n",
    "# Fill nan in child birth day \n",
    "df_r1_anthr2.childbirth_day = df_r1_anthr2.childbirth_day.fillna(1)\n",
    "\n",
    "# some correction\n",
    "## there is a child who have as day 31 and month september, I correct it to 30 to make sure that the information is well defined\n",
    "df_r1_anthr2.loc[(df_r1_anthr2.childbirth_day==31) & (df_r1_anthr2.month_birth==9),\"childbirth_day\"]=30\n",
    "## there is only 28 days in february 2007\n",
    "df_r1_anthr2.loc[(df_r1_anthr2.childbirth_day==29) & (df_r1_anthr2.month_birth==2) & (df_r1_anthr2.year_birth==2007),\"childbirth_day\"]=28\n",
    "\n",
    "## A child have 0.5 as month of birth, this code replace it by 1\n",
    "df_r1_anthr2.loc[(df_r1_anthr2.month_birth==0.5),\"month_birth\"]=1\n",
    "df_r1_anthr2['date_birth'] = pd.to_datetime(dict(year=df_r1_anthr2.year_birth, month=df_r1_anthr2.month_birth, day=df_r1_anthr2.childbirth_day))\n",
    "\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r1_anthr2.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r1_anthr2[\"hh_id\"] = df_r1_anthr2.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r1_anthr2.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r1_anthr2[\"hhm_id\"] = df_r1_anthr2.hhm_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r1_anthr2[\"member_id\"] = df_r1_anthr2.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "# Correcting the no measured output\n",
    "df_r1_anthr2['if_not_measured_why'] = df_r1_anthr2['if_not_measured_why'].replace({\"have measured\":\"have measured\",\"absent\":\"absent\",\"sick\":\"sick\",\"others\":\"other\",\"refused to  give measurement\":\"refused\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sociaux economics caracteristiques of the household members\n",
    "\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1_SE = df_r1_SE.loc[:, ['a01', 'mid', 'b1_01', 'b1_02', 'b1_03', 'b1_04', 'b1_07','b1_08','b1_09']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1_SE = df_r1_SE.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"b1_01\": \"hhm_sex\", \"b1_02\": \"hhm_age\", 'b1_03':'relation_hhh', 'b1_04':'marital_status_hhm', 'b1_07':'literacy_hhm',\n",
    "                            \"b1_08\": \"education_high\", 'b1_09':'curr_att_school'})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r1_SE['hh_id'] = df_r1_SE['hh_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_SE['hhm_id'] = df_r1_SE['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_SE['hhm_sex'] = df_r1_SE['hhm_sex'].astype(\"category\")\n",
    "df_r1_SE['hhm_age'] = df_r1_SE['hhm_age'].astype(\"int64\")\n",
    "\n",
    "df_r1_SE['relation_hhh'] = df_r1_SE['relation_hhh'].astype(\"category\")\n",
    "df_r1_SE['marital_status_hhm'] = df_r1_SE['marital_status_hhm'].astype(\"category\")\n",
    "df_r1_SE['literacy_hhm'] = df_r1_SE['literacy_hhm'].astype(\"category\")\n",
    "\n",
    "df_r1_SE['education_high'] = df_r1_SE['education_high'].astype(\"category\")\n",
    "df_r1_SE['curr_att_school'] = df_r1_SE['curr_att_school'].astype(\"category\")\n",
    "#df_r1_SE['sample_type'] = df_r1_SE['sample_type'].astype(\"category\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1_SE = df_r1_SE.loc[(df_r1_SE.sample_type=='ftf original') | (df_r1_SE.sample_type=='national representative')].drop(columns=['sample_type'], axis=1)\n",
    "df_r1_SE.reset_index(inplace=True)\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r1_SE.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r1_SE[\"hh_id\"] = df_r1_SE.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r1_SE.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r1_SE[\"hhm_id\"] = df_r1_SE.hhm_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r1_SE[\"member_id\"] = df_r1_SE.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "df_r1_SE[\"hhm_status\"]=\"Permanent (r1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the crosswalk\n",
    "crosswalk = crosswalk[['upazila_code','code adm','affected_upazila_all','affected_komen','affected_roanu','affected_mora']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1 datasets\n",
    "\n",
    "## HH identification data: Reading household identification data from a Stata file\n",
    "df_r2 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\001_r2_mod_a_male.dta', convert_categoricals=False) \n",
    "df_r2_f = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\002_r2_mod_a_female.dta', convert_categoricals=False) \n",
    "\n",
    "\n",
    "## FTF: Feed the Futur (FTF) census data from a Stata file\n",
    "#df_r2_ftf = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\001_census_ftf.dta') \n",
    "\n",
    "\n",
    "### Sociaux economics characteristics\n",
    "df_r2_SE = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\003_r2_male_mod_b1.dta', convert_categoricals=False) \n",
    "\n",
    "## Anthro data: Reading anthropometric data from Stata files\n",
    "### 1 for all household members\n",
    "df_r2_anthr1 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\059_r2_mod_w1_female.dta') \n",
    "\n",
    "### 2 for under five children\n",
    "df_r2_anthr2 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\060_r2_mod_w2_female.dta') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\3588970700.py:13: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2['hh_id'] = df_r2['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r2\n",
    "df_r2 = df_r2.loc[:, ['a01', 'div', 'dcode', 'uzcode', 'uncode','vcode', 'div_name', 'District', \n",
    "                    'District_Name', 'Upazila', 'Upazila_Name', 'Union', 'Union_Name','mouzacode','mouza_name','Village','village_name',\n",
    "                    'a02', 'a13', 'a15','hh_type','flag_a']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r2 = df_r2.rename(columns={'a01': \"hh_id\",'div':'division', \"a02\": \"census_nbr\", \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\", 'District':'district_code', 'District_Name':'district_name',\n",
    "                    'Upazila':'upazila_code', 'Upazila_Name':'upazila_name', 'Union':'union_code', 'Union_Name':'union_name','Village':'village_code','village_name':'village_name', \n",
    "                    'mouzacode':'mauza code','mouza_name':'mauza name','hh_type':'sample_type', 'flag_a':'interview_status'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r2' to correct the 'hh_id\n",
    "df_r2['hh_id'] = df_r2['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r2['hh_id'] = df_r2['hh_id'].astype(\"string\")\n",
    "df_r2['division'] = df_r2['division'].astype(\"int64\").astype(\"string\")\n",
    "df_r2['village_code'] = df_r2['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r2['union_code'] = df_r2['union_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r2['interview_status'] = df_r2['interview_status'].astype(\"int64\").astype(\"string\")\n",
    "df_r2['census_nbr'] = df_r2['census_nbr'].astype(\"category\")\n",
    "df_r2['sample_type'] = df_r2['sample_type'].astype(\"category\")\n",
    "\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r2['interview_status'] = df_r2['interview_status'].replace({'1':'Complete','2':'Partial','3':'Refused','4':'Not at home','5':'Migrated'})\n",
    "df_r2['hh_head_religion'] = df_r2['hh_head_religion'].replace({1:'Muslim',2:'Hindu',3:'Christian',4:'Buddhist',5:'Other(specify)'})\n",
    "df_r2['hh_ethnic_group'] = df_r2['hh_ethnic_group'].replace({1:'Bangali',2:'Bihari',3:'Sawtal',4:'Khasia',5:'Rakhain',6:'Bown',\n",
    "                                                            7:'Chak',8:'Chakma',9:'Khumi',10:'Kheyanf',11:'Lusai/Pankho',12:'Marma',13:'Mru(Murong)',\n",
    "                                                            14:'Tonchonga',15:'Tripura',16:'Bonojogi',17:'Others'})\n",
    "\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type!='FTF additional')]\n",
    "#df_r1.sample_type.value_counts()\n",
    "\n",
    "#df_r2['first_v_day'] = df_r2['first_v_day'].astype(\"float64\")\n",
    "#df_r2['first_v_month'] = df_r2['first_v_month'].astype(\"float64\")\n",
    "#df_r2['first_v_year'] = df_r2['first_v_year'].astype(\"float64\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type=='FTF Original') | (df_r1.sample_type=='National Representative')]\n",
    "\n",
    "# creating the interview date\n",
    "#df_r2['date_int'] = pd.to_datetime(dict(year=df_r2.first_v_year, month=df_r2.first_v_month, day=df_r2.first_v_day))\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r2.hh_id.str.len().max() - 2\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r2[\"hh_id\"] = df_r2.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r2.village_code.str.len().max()\n",
    "\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r2[\"village_code\"] = df_r2.village_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r2.upazila_code.str.len().max()\n",
    "\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r2[\"upazila_code\"] = df_r2.upazila_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r2.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union' column with the corrected codes\n",
    "df_r2[\"union_code\"] = df_r2.union_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Household initial id to get the parent household\n",
    "df_r2[\"hh_id_parent\"] = df_r2.hh_id.apply(lambda row: row[:row.find('.')] if row.find('.')!=-1 else row)\n",
    "\n",
    "## hh_split capture hh who didn't split at least once during the study period (1 if don't split and 0 otherwise)\n",
    "df_r2[\"hh_split\"] = df_r2.apply(lambda row: '1' if row[\"hh_id_parent\"] != row[\"hh_id\"] else '0', axis=1).astype('category')\n",
    "\n",
    "## hh_split_bis capture infant hh (1 if the hh is an infant hh and 0 otherwise)\n",
    "df_r2[\"hh_split_bis\"] = df_r2.apply(lambda row: '1' if (row[\"hh_id_parent\"]+\".1\" != row[\"hh_id\"]) & (row[\"hh_id_parent\"] != row[\"hh_id\"]) else '0', axis=1).astype('category')\n",
    "\n",
    "\n",
    "# Build an unique IDs for each hh\n",
    "df_r2[\"hh_id_cmplt\"] = df_r2.apply(lambda row : str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r2[\"survey_round\"] = '2'\n",
    "\n",
    "# List of all households surveyed in the second round (This is needed to know which household split between R2 and R3)\n",
    "lst_hh_r2 = np.unique(df_r2.hh_id.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2577530553.py:12: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_f['hh_id'] = df_r2_f['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2577530553.py:31: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_f[\"hh_id\"] = df_r2_f.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2577530553.py:36: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_f[\"upazila_code\"] = df_r2_f.upazila_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2577530553.py:41: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_f[\"village_code\"] = df_r2_f.village_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r2_f = df_r2_f.loc[:, ['a01', 'div', 'dcode', 'uzcode', 'uncode','Village', 'div_name', 'District', \n",
    "                    'District_Name', 'Upazila', 'Upazila_Name', 'Union', 'Union_Name',\n",
    "                    'a02', 'a13', 'a15','hh_type', 'flag_fem_a']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r2_f = df_r2_f.rename(columns={'a01': \"hh_id\",'div':'division', \"a02\": \"census_nbr\", \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\", 'District':'district_code', 'District_Name':'district_name',\n",
    "                    'Upazila':'upazila_code','Village':'village_code', 'Upazila_Name':'upazila_name', 'Union':'union_code', 'Union_Name':'union_name','hh_type':'sample_type','flag_fem_a':'interview_status'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r2_f' to correct the 'hh_id\n",
    "df_r2_f['hh_id'] = df_r2_f['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r2_f['hh_id'] = df_r2_f['hh_id'].astype(\"string\")\n",
    "df_r2_f['division'] = df_r2_f['division'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_f['village_code'] = df_r2_f['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_f['upazila_code'] = df_r2_f['upazila_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_f['union_code'] = df_r2_f['union_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_f['census_nbr'] = df_r2_f['census_nbr'].astype(\"category\")\n",
    "df_r2_f['sample_type'] = df_r2_f['sample_type'].astype(\"category\")\n",
    "#df_r2_f['sample_type'] = df_r2_f['sample_type'].astype(\"category\")\n",
    "\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r2_f['interview_status'] = df_r2_f['interview_status'].replace({'1':'Complete','2':'Partial','3':'Refused','4':'Not at home','5':'Migrated'})\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r2_f.hh_id.str.len().max() - 2\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r2_f[\"hh_id\"] = df_r2_f.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r2_f.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r2_f[\"upazila_code\"] = df_r2_f.upazila_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r2_f.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r2_f[\"village_code\"] = df_r2_f.village_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r2_f.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union' column with the corrected codes\n",
    "df_r2_f[\"union_code\"] = df_r2_f.union_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Household initial id to get the parent household\n",
    "df_r2_f[\"hh_id_parent\"] = df_r2_f.hh_id.apply(lambda row: row[:row.find('.')] if row.find('.')!=-1 else row)\n",
    "df_r2_f[\"hh_split\"] = df_r2_f.apply(lambda row: '1' if row[\"hh_id_parent\"] != row[\"hh_id\"] else '0', axis=1).astype('category')\n",
    "# Build an unique IDs for each hh\n",
    "df_r2_f[\"hh_id_cmplt\"] = df_r2_f.apply(lambda row : str(row.loc[\"union_code\"]) + str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r2_f[\"survey_round\"] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\3228784964.py:10: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_anthr1['hh_id'] = df_r2_anthr1['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\3228784964.py:36: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_anthr1[\"hh_id\"] = df_r2_anthr1.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\3228784964.py:40: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_anthr1[\"hhm_id\"] = df_r2_anthr1.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r2_anthr1 = df_r2_anthr1.loc[:, ['a01', 'mid','w1_01', 'w1_02', 'w1_03','w1_04', 'w1_05']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r2_anthr1 = df_r2_anthr1.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"w1_01\": \"are_you_pregnant\", \"w1_02\": \"are_you_lactating\",\n",
    "                            \"w1_03\": \"weight_kg\", 'w1_04':'height_cm', 'w1_05':'if_not_measured_why'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r2_anthr1' to correct the 'hh_id\n",
    "df_r2_anthr1['hh_id'] = df_r2_anthr1['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r2_anthr1['hh_id'] = df_r2_anthr1['hh_id'].astype(\"string\")\n",
    "df_r2_anthr1['hhm_id'] = df_r2_anthr1['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_anthr1['are_you_pregnant'] = df_r2_anthr1['are_you_pregnant'].astype(\"category\")\n",
    "df_r2_anthr1['are_you_lactating'] = df_r2_anthr1['are_you_lactating'].astype(\"category\")\n",
    "df_r2_anthr1['weight_kg'] = df_r2_anthr1['weight_kg'].astype(\"float64\")\n",
    "df_r2_anthr1['height_cm'] = df_r2_anthr1['height_cm'].astype(\"float64\")\n",
    "\n",
    "# computing the BMI\n",
    "df_r2_anthr1['bmi'] = df_r2_anthr1['weight_kg']*10000/(df_r2_anthr1['height_cm']**2)\n",
    "\n",
    "df_r2_anthr1['if_not_measured_why'] = df_r2_anthr1['if_not_measured_why'].astype(\"category\")\n",
    "df_r2_anthr1['if_not_measured_why'] = df_r2_anthr1['if_not_measured_why'].cat.rename_categories({\"Have measured\":\"have measured\",\"Absent\":\"absent\",\"Sick\":\"sick\",\"Other\":\"other\",\"Refused to  give Measure ment\":\"refused\"})\n",
    "df_r2_anthr1['if_not_measured_why'] = df_r2_anthr1['if_not_measured_why'].cat.set_categories([\"have measured\",\"absent\",\"sick\",\"other\",\"refused\",\"missing\"])\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "##df_r1_anthr2 = df_r1_anthr2.loc[(df_r1_anthr2.sample_type=='ftf original') | (df_r1_anthr2.sample_type=='national representative')]\n",
    "## df_r1_anthr2.reset_index(inplace=True)\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r2_anthr1.hh_id.str.len().max() - 2\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r2_anthr1[\"hh_id\"] = df_r2_anthr1.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r2_anthr1.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r2_anthr1[\"hhm_id\"] = df_r2_anthr1.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r2_anthr1[\"member_id\"] = df_r2_anthr1.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "# Correcting the no measured output\n",
    "\n",
    "df_r2_anthr1['if_not_measured_why'] = df_r2_anthr1['if_not_measured_why'].fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\3839828607.py:13: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_anthr2['hh_id'] = df_r2_anthr2['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r2_anthr2 = df_r2_anthr2.loc[:, ['a01', 'mid', 'w2_01', 'w2_02', 'w2_03', 'w2_04', 'w2_05','w2_07','w2_08', 'w2_10', 'w2_14']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r2_anthr2 = df_r2_anthr2.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"w2_01\": \"mother_id\", \"w2_02\": \"childbirth_day\", 'w2_03':'if_not_week_month', 'w2_04':'month_birth', 'w2_05':'year_birth',\n",
    "                            \"w2_07\": \"weight_kg\", 'w2_08':'height_cm', 'w2_14':'age_month', 'w2_10':'if_not_measured_why'})\n",
    "\n",
    "# Filtering to get only informations on underfive children only\n",
    "df_r2_anthr2 = df_r2_anthr2[df_r2_anthr2.loc[:,'hhm_id'] !='Not under five age child in this household']\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r2_anthr2' to correct the 'hh_id\n",
    "df_r2_anthr2['hh_id'] = df_r2_anthr2['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "df_r2_anthr2.reset_index(inplace=True)\n",
    "# Apply encoding to correct label\n",
    "label_to_int = {label: label for int, label in enumerate(df_r2_anthr2[\"hhm_id\"].unique())}\n",
    "df_r2_anthr2[\"hhm_id\"] = df_r2_anthr2[\"hhm_id\"].map(label_to_int)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r2_anthr2['hh_id'] = df_r2_anthr2['hh_id'].astype(\"string\")\n",
    "df_r2_anthr2['hhm_id'] = df_r2_anthr2['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "#df_r2_anthr2['mother_id'] = df_r2_anthr2['mother_id'].astype(\"int64\").astype(\"string\")\n",
    "#df_r2_anthr2['childbirth_day'] = df_r2_anthr2['childbirth_day'].astype(\"float64\")\n",
    "#df_r2_anthr2['month_birth'] = df_r2_anthr2['month_birth'].astype(\"float64\")\n",
    "#df_r2_anthr2['year_birth'] = df_r2_anthr2['year_birth'].astype(\"float64\")\n",
    "df_r2_anthr2['age_month'] = df_r2_anthr2['age_month'].astype(\"int64\", errors='ignore')\n",
    "df_r2_anthr2['if_not_week_month'] = df_r2_anthr2['if_not_week_month'].astype(\"category\")\n",
    "df_r2_anthr2['weight_kg'] = df_r2_anthr2['weight_kg'].astype(\"float64\")\n",
    "df_r2_anthr2['height_cm'] = df_r2_anthr2['height_cm'].astype(\"float64\")\n",
    "\n",
    "# computing the BMI\n",
    "df_r2_anthr2['bmi'] = df_r2_anthr2['weight_kg']*10000/(df_r2_anthr2['height_cm']**2)\n",
    "\n",
    "df_r2_anthr2['if_not_measured_why'] = df_r2_anthr2['if_not_measured_why'].astype(\"category\")\n",
    "# Correcting the no measured output\n",
    "df_r2_anthr2['if_not_measured_why'] = df_r2_anthr2['if_not_measured_why'].cat.rename_categories({\"Have measured\":\"have measured\",\"Absent\":\"absent\",\"Sick\":\"sick\",\"Other\":\"other\",\"Refused to  give Measure ment\":\"refused\"})\n",
    "df_r2_anthr2['if_not_measured_why'] = df_r2_anthr2['if_not_measured_why'].cat.set_categories([\"have measured\",\"absent\",\"sick\",\"other\",\"refused\",\"missing\"])\n",
    "\n",
    "df_r2_anthr2.loc[df_r2_anthr2['age_month']==0,'age_month'] = 1\n",
    "df_r2_anthr2['age_days'] = df_r2_anthr2['age_month'].apply(lambda curr_age:  curr_age*30)\n",
    "\n",
    "# Fill Nan in the child birth day\n",
    "df_r2_anthr2.childbirth_day = df_r2_anthr2.childbirth_day.fillna(1)\n",
    "\n",
    "# some correction\n",
    "## there is a child who have as day 31 and month september, I correct it to 30 to make sure that the information is well defined\n",
    "df_r2_anthr2.loc[(df_r2_anthr2.childbirth_day==31) & (df_r2_anthr2.month_birth==9),\"childbirth_day\"]=30\n",
    "## there is only 28 days in february 2007\n",
    "df_r2_anthr2.loc[(df_r2_anthr2.childbirth_day==29) & (df_r2_anthr2.month_birth==2) & (df_r2_anthr2.year_birth==2007),\"childbirth_day\"]=28\n",
    "\n",
    "## A child have 0.5 as month of birth\n",
    "df_r2_anthr2.loc[(df_r2_anthr2.month_birth==0.5),\"month_birth\"]=1\n",
    "df_r2_anthr2['date_birth'] = pd.to_datetime(dict(year=df_r2_anthr2.year_birth, month=df_r2_anthr2.month_birth, day=df_r2_anthr2.childbirth_day))\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r2_anthr2.hh_id.str.len().max() - 2\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r2_anthr2[\"hh_id\"] = df_r2_anthr2.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r2_anthr2.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r2_anthr2[\"hhm_id\"] = df_r2_anthr2.hhm_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r2_anthr2[\"member_id\"] = df_r2_anthr2.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "\n",
    "# Replace NaN in the reason for not measured by a give value:\n",
    "#df_r2_anthr2['if_not_measured_why'] = df_r2_anthr2['if_not_measured_why'].replace({\"Have measured\":\"have measured\",\"Absent\":\"absent\",\"Sick\":\"sick\",\"Other\":\"other\",\"Refused to  give Measure ment\":\"refused\"})\n",
    "\n",
    "df_r2_anthr2['if_not_measured_why'] = df_r2_anthr2['if_not_measured_why'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\3322896342.py:11: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_SE['hh_id'] = df_r2_SE['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\3322896342.py:54: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_SE[\"hh_id\"] = df_r2_SE.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\3322896342.py:58: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_SE[\"hhm_id\"] = df_r2_SE.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Sociaux economics caracteristiques of the household members\n",
    "\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r2_SE = df_r2_SE.loc[:, ['a01', 'mid','mem_stat', 'b1_01', 'b1_02', 'b1_03', 'b1_04', 'b1_07','b1_08','b1_09']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r2_SE = df_r2_SE.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"b1_01\": \"hhm_sex\", \"b1_02\": \"hhm_age\", 'b1_03':'relation_hhh', 'b1_04':'marital_status_hhm', 'b1_07':'literacy_hhm',\n",
    "                            \"b1_08\": \"education_high\", 'b1_09':'curr_att_school','mem_stat':'hhm_status'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r2_SE' to correct the 'hh_id\n",
    "df_r2_SE['hh_id'] = df_r2_SE['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "#df_r2_SE['hhm_age'] = df_r2_SE['hhm_age'].fillna(0)\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r2_SE['hh_id'] = df_r2_SE['hh_id'].astype(\"string\")\n",
    "df_r2_SE['hhm_id'] = df_r2_SE['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_SE['hhm_sex'] = df_r2_SE['hhm_sex'].astype(\"category\")\n",
    "df_r2_SE['hhm_age'] = df_r2_SE['hhm_age'].astype(\"int64\", errors='ignore')\n",
    "\n",
    "df_r2_SE['relation_hhh'] = df_r2_SE['relation_hhh'].astype(\"category\")\n",
    "df_r2_SE['marital_status_hhm'] = df_r2_SE['marital_status_hhm'].astype(\"category\")\n",
    "df_r2_SE['literacy_hhm'] = df_r2_SE['literacy_hhm'].astype(\"category\")\n",
    "#df_r2_SE['hhm_status'] = df_r2_SE['hhm_status'].astype(\"category\")\n",
    "\n",
    "df_r2_SE['education_high'] = df_r2_SE['education_high'].astype(\"category\")\n",
    "df_r2_SE['curr_att_school'] = df_r2_SE['curr_att_school'].astype(\"category\")\n",
    "\n",
    "\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r2_SE['relation_hhh'] = df_r2_SE['relation_hhh'].cat.rename_categories({1:'primary respondent',2:'primary respondent husband/wife',3:'son/daughter',4:'daughter/son-in-law',5:'grandson/daughter',6:'father/mother',\n",
    "                                                            7:'brother/sister',8:'niece/nephew',9:'primary respondent?s cousin',10:'father-in-law/mother-in-law',11:'brother/sister-in-law',\n",
    "                                                            12:'husband/wife?s  niece/nephew',13:'primary respondent?s husband/wife\\'s cousin',14:'other relative',15:'permanent servant',16:'other non relative/friends'})\n",
    "\n",
    "df_r2_SE['marital_status_hhm'] = df_r2_SE['marital_status_hhm'].cat.rename_categories({1:'unmarried (never married)',2:'married',3:'widow/widower',4:'divorced',5:'separated/deserted'})\n",
    "\n",
    "df_r2_SE['literacy_hhm'] = df_r2_SE['literacy_hhm'].cat.rename_categories({1:'cannot read and write',2:'can sign only',3:'can read  only',4:'can read and write'})\n",
    "\n",
    "df_r2_SE['education_high'] = df_r2_SE['education_high'].cat.rename_categories({0:'reads in class i',1:'completed class i',2:'completed class 2',3:'completed class 3',4:'completed class 4',5:'completed class 5',\n",
    "                                                                6:'completed class 6',7:'completed class 7',8:'completed class 8',9:'completed class 9',10:'completed ssc/dakhil',12:'completed hsc/alim',14:'ba/bsc pass/fazil',\n",
    "                                                                15:'ba/bsc honors/fazil',16:'ma/msc and above/kamil',22:'ssc candidate',33:'hsc candidate',66:'preschool class (general)',67:'preschool (mosque based)',\n",
    "                                                                71:'medical/mbbs',72:'nursing',73:'diploma engineer',74:'diploma Enginee',75:'vocational (scolarship based /technical Education',76:'others(specify)',99:'never attended school'})\n",
    "\n",
    "df_r2_SE['curr_att_school'] = df_r2_SE['curr_att_school'].cat.rename_categories({1:'yes',2:'no'})\n",
    "\n",
    "df_r2_SE['hhm_status'] = df_r2_SE['hhm_status'].replace({0:'Previous and current round member',1:'New member (new born)',2:'New member through marriage',3:'New member upon return from divorce or seperation',4:'Household merged/combined',5:'Other reasons (permanent)',\n",
    "                                                                6:'Residing elsewhere for the pursuit of studies',7:'Death',8:'Married and left household',9:'Divorced and left household',10:'Household split',11:'Left household for employment',12:'Other reasns for leaving the household',\n",
    "                                                                66:'New sample household and current round member'})\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r2_SE.hh_id.str.len().max() - 2\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r2_SE[\"hh_id\"] = df_r2_SE.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r2_SE.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each hh member code to ensure they all have the same length and update the 'hhm_id' column with the corrected codes\n",
    "df_r2_SE[\"hhm_id\"] = df_r2_SE.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r2_SE[\"member_id\"] = df_r2_SE.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "Previous and current round member                    23210\n",
       "New sample household and current round member         2387\n",
       "New member (new born)                                 1676\n",
       "Household split                                       1410\n",
       "Other reasons (permanent)                             1358\n",
       "Left household for employment                          880\n",
       "Married and left household                             837\n",
       "New member through marriage                            630\n",
       "Other reasns for leaving the household                 574\n",
       "Death                                                  494\n",
       "Household merged/combined                              442\n",
       "Residing elsewhere for the pursuit of studies          302\n",
       "Divorced and left household                             62\n",
       "New member upon return from divorce or seperation       35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r2_SE.hhm_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6943"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_r2_SE.shape\n",
    "#34297\n",
    "#df_r2_anthr1.shape\n",
    "#24591\n",
    "#df_r2_anthr2.shape\n",
    "#2763\n",
    "#24591+2763\n",
    "\n",
    "#df_r2_SE[df_r2_SE.hhm_status.isna()]\n",
    "24591+2763 - 34297"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 3 datasets\n",
    "\n",
    "## HH identification data: Reading household identification data from a Stata file\n",
    "df_r3 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 3 (2018)\\\\BIHSRound3\\\\male\\\\009_bihs_r3_male_mod_a.dta', convert_categoricals=False) \n",
    "df_r3_f = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 3 (2018)\\\\BIHSRound3\\\\female\\\\092_bihs_r3_female_mod_a.dta', convert_categoricals=False) \n",
    "\n",
    "\n",
    "## FTF: Feed the Futur (FTF) census data from a Stata file\n",
    "#df_r2_ftf = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\001_census_ftf.dta') \n",
    "\n",
    "\n",
    "### Sociaux economics characteristics\n",
    "df_r3_SE = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 3 (2018)\\\\BIHSRound3\\\\male\\\\010_bihs_r3_male_mod_b1.dta', convert_categoricals=False) \n",
    "\n",
    "## Anthro data: Reading anthropometric data from Stata files\n",
    "### 1 for all household members\n",
    "df_r3_anthr1 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 3 (2018)\\\\BIHSRound3\\\\female\\\\099_bihs_r3_female_mod_w1.dta') \n",
    "\n",
    "### 2 for under five children\n",
    "df_r3_anthr2 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 3 (2018)\\\\BIHSRound3\\\\female\\\\100_bihs_r3_female_mod_w2.dta') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Male enumerators data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\1472767799.py:10: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3['hh_id'] = df_r3['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\1472767799.py:24: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df_r3['interview_status'] = df_r3['interview_status'].replace({1:'Complete',2:'Refused',3:'Not at home',4:'Migrated',5:'Partial',6:'Refused'})\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\1472767799.py:50: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3[\"hh_id\"] = df_r3.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\1472767799.py:55: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3[\"upazila_code\"] = df_r3.upazila_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\1472767799.py:60: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3[\"village_code\"] = df_r3.village_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\1472767799.py:65: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3[\"union_code\"] = df_r3.union_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r3\n",
    "df_r3 = df_r3.loc[:, ['a01', 'div', 'village', 'union', 'div_name', 'district', 'upazila', 'a13', 'a15','a16_1_dd','a16_1_mm','a16_1_yy','a27', 'hh_type']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r3 = df_r3.rename(columns={'a01': \"hh_id\",'div':'division', \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\", 'district':'district_code', \n",
    "                    'upazila':'upazila_code','village':'village_code', 'union':'union_code', 'a16_1_dd': 'first_v_day' ,'a16_1_mm': 'first_v_month','a16_1_yy': 'first_v_year','a27':'interview_status','hh_type':'sample_type'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r3' to correct the 'hh_id\n",
    "df_r3['hh_id'] = df_r3['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r3['hh_id'] = df_r3['hh_id'].astype(\"string\")\n",
    "df_r3['division'] = df_r3['division'].astype(\"int64\").astype(\"string\")\n",
    "df_r3['village_code'] = df_r3['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3['upazila_code'] = df_r3['upazila_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3['union_code'] = df_r3['union_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3['interview_status'] = df_r3['interview_status'].astype(\"category\")\n",
    "df_r3['sample_type'] = df_r3['sample_type'].astype(\"category\")\n",
    "\n",
    "df_r3['hh_head_religion'] = df_r3['hh_head_religion'].astype(\"category\")\n",
    "df_r3['hh_ethnic_group'] = df_r3['hh_ethnic_group'].astype(\"category\")\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r3['interview_status'] = df_r3['interview_status'].replace({1:'Complete',2:'Refused',3:'Not at home',4:'Migrated',5:'Partial',6:'Refused'})\n",
    "df_r3['hh_head_religion'] = df_r3['hh_head_religion'].cat.rename_categories({1:'Muslim',2:'Hindu',3:'Christian',4:'Buddhist',5:'Other(specify)'})\n",
    "df_r3['hh_ethnic_group'] = df_r3['hh_ethnic_group'].cat.rename_categories({1:'Bangali',2:'Bihari',3:'Sawtal',4:'Khasia',5:'Rakhain',6:'Bown',\n",
    "                                                            7:'Chak',8:'Chakma',9:'Khumi',10:'Kheyanf',11:'Lusai/Pankho',12:'Marma',13:'Mru(Murong)',\n",
    "                                                            14:'Tonchonga',15:'Tripura',16:'Bonojogi',17:'Others'})\n",
    "\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type!='FTF additional')]\n",
    "#df_r1.sample_type.value_counts()\n",
    "\n",
    "#df_r2['first_v_day'] = df_r2['first_v_day'].astype(\"float64\")\n",
    "#df_r2['first_v_month'] = df_r2['first_v_month'].astype(\"float64\")\n",
    "#df_r2['first_v_year'] = df_r2['first_v_year'].astype(\"float64\")\n",
    "df_r3['first_v_day'] = df_r3['first_v_day'].astype(\"int64\")\n",
    "df_r3['first_v_month'] = df_r3['first_v_month'].astype(\"int64\")\n",
    "df_r3['first_v_year'] = df_r3['first_v_year'].astype(\"int64\")\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type=='FTF Original') | (df_r1.sample_type=='National Representative')]\n",
    "\n",
    "df_r3['first_v_day'] = df_r3.first_v_day.replace({99999:'01'})\n",
    "# creating the interview date\n",
    "df_r3['date_int'] = pd.to_datetime(dict(year=df_r3.first_v_year, month=df_r3.first_v_month, day=df_r3.first_v_day))\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r3.hh_id.str.len().max() - 3\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r3[\"hh_id\"] = df_r3.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r3.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r3[\"upazila_code\"] = df_r3.upazila_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r3.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r3[\"village_code\"] = df_r3.village_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r3.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union' column with the corrected codes\n",
    "df_r3[\"union_code\"] = df_r3.union_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Household initial id to get the parent household\n",
    "df_r3[\"hh_id_parent\"] = df_r3.hh_id.apply(lambda row: row[:row.find('.')] if row.find('.')!=-1 else row)\n",
    "df_r3[\"hh_split\"] = df_r3.apply(lambda row: '1' if row[\"hh_id_parent\"] != row[\"hh_id\"] else '0', axis=1).astype('category')\n",
    "\n",
    "## hh_split_bis capture infant hh (1 if the hh is an infant hh and 0 otherwise)\n",
    "df_r3[\"hh_split_bis\"] = df_r3.apply(lambda row: '1' if (row[\"hh_id_parent\"]+\".1\" != row[\"hh_id\"]) & (row[\"hh_id_parent\"] != row[\"hh_id\"]) else '0', axis=1).astype('category')\n",
    "\n",
    "# Build an unique IDs for each hh\n",
    "df_r3[\"hh_id_cmplt\"] = df_r3.apply(lambda row : str(row.loc[\"union_code\"]) + str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "## hh_split between r2 and r3\n",
    "df_r3[\"hh_split_btwn_r2_r3\"] = df_r3.apply(lambda row: '0' if (row[\"hh_id\"] in lst_hh_r2) else '1', axis=1).astype('category')\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r3[\"survey_round\"] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Female enumerators data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\1445982298.py:27: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df_r3_f['interview_status'] = df_r3_f['interview_status'].replace({1:'Complete',2:'Refused',3:'Not at home',4:'Migrated',5:'Partial',6:'Refused'})\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r3_f = df_r3_f.loc[:, ['a01', 'div', 'village', 'union', 'div_name', 'district', 'upazila','a02', 'a13', 'a15', 'a27','hh_type']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r3_f = df_r3_f.rename(columns={'a01': \"hh_id\",'div':'division', \"a02\": \"census_nbr\", \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\", 'district':'district_code', \n",
    "                    'upazila':'upazila_code','village':'village_code', 'union':'union_code', 'a27':'interview_status', 'hh_type':'sample_type'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r3_f' to correct the 'hh_id\n",
    "df_r3_f['hh_id'] = df_r3_f['hh_id'].astype(\"category\").apply( lambda n: n if n % 1 else int(n))\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r3_f['hh_id'] = df_r3_f['hh_id'].astype(\"string\")\n",
    "df_r3_f['division'] = df_r3_f['division'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_f['village_code'] = df_r3_f['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_f['upazila_code'] = df_r3_f['upazila_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_f['union_code'] = df_r3_f['union_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_f['census_nbr'] = df_r3_f['census_nbr'].astype(\"category\")\n",
    "df_r3_f['sample_type'] = df_r3_f['sample_type'].astype(\"category\")\n",
    "df_r3_f['interview_status'] = df_r3_f['interview_status'].astype(\"category\")\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r3_f['interview_status'] = df_r3_f['interview_status'].cat.rename_categories({1:'Complete',2:'Partial',3:'Refused',4:'Not at home',5:'Migrated'})\n",
    "\n",
    "df_r3_f['hh_head_religion'] = df_r3_f['hh_head_religion'].astype(\"category\")\n",
    "df_r3_f['hh_ethnic_group'] = df_r3_f['hh_ethnic_group'].astype(\"category\")\n",
    "# Replace the numeric value by there label\n",
    "df_r3_f['interview_status'] = df_r3_f['interview_status'].replace({1:'Complete',2:'Refused',3:'Not at home',4:'Migrated',5:'Partial',6:'Refused'})\n",
    "df_r3_f['hh_head_religion'] = df_r3_f['hh_head_religion'].cat.rename_categories({1:'Muslim',2:'Hindu',3:'Christian',4:'Buddhist',5:'Other(specify)'})\n",
    "df_r3_f['hh_ethnic_group'] = df_r3_f['hh_ethnic_group'].cat.rename_categories({1:'Bengali',2:'Bihari',3:'Sawtal',4:'Khasia',5:'Rakhain',6:'Bown',\n",
    "                                                            7:'Chak',8:'Chakma',9:'Khumi',10:'Kheyanf',11:'Lusai/Pankho',12:'Marma',13:'Mru(Murong)',\n",
    "                                                            14:'Tonchonga',15:'Tripura',16:'Bonojogi',17:'Others'})\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r3_f.hh_id.str.len().max() - 3\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r3_f[\"hh_id\"] = df_r3_f.hh_id.astype(\"category\").apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r3_f.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r3_f[\"upazila_code\"] = df_r3_f.upazila_code.astype(\"category\").apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r3_f.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r3_f[\"village_code\"] = df_r3_f.village_code.astype(\"category\").apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r3_f.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union' column with the corrected codes\n",
    "df_r3_f[\"union_code\"] = df_r3_f.union_code.astype(\"category\").apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Household initial id to get the parent household\n",
    "df_r3_f[\"hh_id_parent\"] = df_r3_f.hh_id.apply(lambda row: row[:row.find('.')] if row.find('.')!=-1 else row)\n",
    "df_r3_f[\"hh_split\"] = df_r3_f.apply(lambda row: '1' if row[\"hh_id_parent\"] != row[\"hh_id\"] else '0', axis=1).astype('category')\n",
    "\n",
    "# Build an unique IDs for each hh\n",
    "df_r3_f[\"hh_id_cmplt\"] = df_r3_f.apply(lambda row : str(row.loc[\"union_code\"]) + str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r3_f[\"survey_round\"] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anthropometric data (more than 5 years old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2738657749.py:10: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr1['hh_id'] = df_r3_anthr1['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2738657749.py:33: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr1[\"hh_id\"] = df_r3_anthr1.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2738657749.py:37: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr1[\"hhm_id\"] = df_r3_anthr1.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r3_anthr1 = df_r3_anthr1.loc[:, ['a01', 'mid_w1' ,'w1_01', 'w1_02', 'w1_03','w1_04', 'w1_05']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r3_anthr1 = df_r3_anthr1.rename(columns={'a01': \"hh_id\",'mid_w1':'hhm_id', \"w1_01\": \"are_you_pregnant\", \"w1_02\": \"are_you_lactating\",'mem_stat':'mem_stat',\n",
    "                            \"w1_03\": \"weight_kg\", 'w1_04':'height_cm', 'w1_05':'if_not_measured_why'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r3_anthr1' to correct the 'hh_id\n",
    "df_r3_anthr1['hh_id'] = df_r3_anthr1['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "#df_r3_anthr1['if_not_measured_why'] = df_r3_anthr1['if_not_measured_why'].cat.set_categories({\"have measured\",\"absent\",\"sick\",\"other\",\"refused\",\"missing\"})\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r3_anthr1['hh_id'] = df_r3_anthr1['hh_id'].astype(\"string\")\n",
    "df_r3_anthr1['hhm_id'] = df_r3_anthr1['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_anthr1['are_you_pregnant'] = df_r3_anthr1['are_you_pregnant'].astype(\"category\")\n",
    "df_r3_anthr1['are_you_lactating'] = df_r3_anthr1['are_you_lactating'].astype(\"category\")\n",
    "df_r3_anthr1['weight_kg'] = df_r3_anthr1['weight_kg'].astype(\"float64\")\n",
    "df_r3_anthr1['height_cm'] = df_r3_anthr1['height_cm'].astype(\"float64\")\n",
    "\n",
    "# computing the BMI\n",
    "df_r3_anthr1['bmi'] = df_r3_anthr1['weight_kg']*10000/(df_r3_anthr1['height_cm']**2)\n",
    "\n",
    "df_r3_anthr1['if_not_measured_why'] = df_r3_anthr1['if_not_measured_why'].astype(\"category\")\n",
    "# Correcting the no measured output\n",
    "df_r3_anthr1['if_not_measured_why'] = df_r3_anthr1['if_not_measured_why'].cat.rename_categories({\"Have measured\":\"have measured\",\"Absent\":\"absent\",\"Sick\":\"sick\",\"Other (specify)\":\"other\",\"Refused to  give measurement\":\"refused\"})\n",
    "df_r3_anthr1['if_not_measured_why'] = df_r3_anthr1['if_not_measured_why'].cat.set_categories({\"have measured\",\"absent\",\"sick\",\"other\",\"refused\",\"missing\"})\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r3_anthr1.hh_id.str.len().max() - 3\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r3_anthr1[\"hh_id\"] = df_r3_anthr1.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r3_anthr1.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each hh member code to ensure they all have the same length and update the 'hhm_id' column with the corrected codes\n",
    "df_r3_anthr1[\"hhm_id\"] = df_r3_anthr1.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r3_anthr1[\"member_id\"] = df_r3_anthr1.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "df_r3_anthr1['if_not_measured_why'] = df_r3_anthr1['if_not_measured_why'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anthropometric data (more less than 5 years old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\4161378374.py:10: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr2['hh_id'] = df_r3_anthr2['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\4161378374.py:60: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr2[\"hh_id\"] = df_r3_anthr2.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\4161378374.py:64: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr2[\"hhm_id\"] = df_r3_anthr2.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r3\n",
    "df_r3_anthr2 = df_r3_anthr2.loc[:, ['a01', 'mid_w2', 'w2_01', 'w2_05','w2_07','w2_08', 'w2_10','w2_02','w2_03','w2_04','w2_05','w2_13','w2_15' ,'haz06', 'waz06', 'bmiz06','flag_zs']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r3_anthr2 = df_r3_anthr2.rename(columns={'a01': \"hh_id\",'mid_w2':'hhm_id', \"w2_01\": \"mother_id\",'w2_02':'child',\"w2_02\": \"childbirth_day\",\"w2_13\":\"birth_order\", 'w2_03':'if_not_week_month', 'w2_04':'month_birth', 'w2_05':'year_birth',\n",
    "                                            'haz06':'HAZ', 'waz06':'WAZ', 'bmiz06':'BMIZ',\"w2_07\": \"weight_kg\",\"w2_15\":\"age_month\", 'w2_08':'height_cm', 'w2_10':'if_not_measured_why','flag_zs':'flag'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r3_anthr2' to correct the 'hh_id\n",
    "df_r3_anthr2['hh_id'] = df_r3_anthr2['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "## Filtering to get surveyed household with an underfive child\n",
    "df_r3_anthr2 = df_r3_anthr2[df_r3_anthr2.hhm_id !='There are no <5 child in this household']\n",
    "df_r3_anthr2.reset_index(inplace=True)\n",
    "\n",
    "# Apply encoding to correct label\n",
    "label_to_int = {label: label for int, label in enumerate(df_r3_anthr2[\"hhm_id\"].unique())}\n",
    "df_r3_anthr2[\"hhm_id\"] = df_r3_anthr2[\"hhm_id\"].map(label_to_int)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r3_anthr2['hh_id'] = df_r3_anthr2['hh_id'].astype(\"string\")\n",
    "df_r3_anthr2['hhm_id'] = df_r3_anthr2['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "#df_r3_anthr2['mother_id'] = df_r3_anthr2['mother_id'].astype(\"int64\").astype(\"string\")\n",
    "#df_r3_anthr2['childbirth_day'] = df_r3_anthr2['childbirth_day'].astype(\"int64\")\n",
    "#df_r3_anthr2['month_birth'] = df_r3_anthr2['month_birth'].astype(\"int64\")\n",
    "#df_r3_anthr2['year_birth'] = df_r3_anthr2['year_birth'].astype(\"int64\")\n",
    "#df_r3_anthr2['if_not_week_month'] = df_r3_anthr2['if_not_week_month'].astype(\"category\")\n",
    "df_r3_anthr2['weight_kg'] = df_r3_anthr2['weight_kg'].astype(\"float64\")\n",
    "df_r3_anthr2['height_cm'] = df_r3_anthr2['height_cm'].astype(\"float64\")\n",
    "\n",
    "# computing the BMI\n",
    "df_r3_anthr2['bmi'] = df_r3_anthr2['weight_kg']*10000/(df_r3_anthr2['height_cm']**2)\n",
    "df_r3_anthr2['if_not_measured_why'] = df_r3_anthr2['if_not_measured_why'].astype(\"category\")\n",
    "# Correcting the no measured output\n",
    "df_r3_anthr2['if_not_measured_why'] = df_r3_anthr2['if_not_measured_why'].cat.rename_categories({\"Have measured\":\"have measured\",\"Absent\":\"absent\",\"Sick\":\"sick\",\"Other (specify)\":\"other\",\"Refused to  give measurement\":\"refused\"})\n",
    "df_r3_anthr2['if_not_measured_why'] = df_r3_anthr2['if_not_measured_why'].cat.set_categories({\"have measured\",\"absent\",\"sick\",\"other\",\"refused\",\"missing\"})\n",
    "\n",
    "#df_r3_anthr2.loc[df_r3_anthr2['age_month']==0,'age_month'] = 1\n",
    "df_r3_anthr2['age_days'] = df_r3_anthr2['age_month'].apply(lambda curr_age:  curr_age*30)\n",
    "\n",
    "# Fill Nan in the child birth day\n",
    "#df_r3_anthr2.childbirth_day = df_r3_anthr2.childbirth_day.fillna(1)\n",
    "\n",
    "# some correction\n",
    "## there is a child who have as day 31 and month september, I correct it to 30 to make sure that the information is well defined\n",
    "### df_r3_anthr2.loc[(df_r3_anthr2.childbirth_day==31) & (df_r3_anthr2.month_birth==9),\"childbirth_day\"]=30\n",
    "## there is only 28 days in february 2007\n",
    "### df_r3_anthr2.loc[(df_r3_anthr2.childbirth_day==29) & (df_r3_anthr2.month_birth==2) & (df_r3_anthr2.year_birth==2007),\"childbirth_day\"]=28\n",
    "\n",
    "## A child have 0.5 as month of birth\n",
    "###df_r3_anthr2.loc[(df_r3_anthr2.month_birth==0.5),\"month_birth\"]=1\n",
    "###df_r3_anthr2['date_birth'] = pd.to_datetime(dict(year=df_r3_anthr2.year_birth, month=df_r3_anthr2.month_birth, day=df_r3_anthr2.childbirth_day))\n",
    "\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r3_anthr2.hh_id.str.len().max() - 3\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r3_anthr2[\"hh_id\"] = df_r3_anthr2.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r3_anthr2.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each hh member code to ensure they all have the same length and update the 'hhm_id' column with the corrected codes\n",
    "df_r3_anthr2[\"hhm_id\"] = df_r3_anthr2.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r3_anthr2[\"member_id\"] = df_r3_anthr2.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "df_r3_anthr2['if_not_measured_why'] = df_r3_anthr2['if_not_measured_why'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other household data (more than 5 years old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2909285177.py:11: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_SE['hh_id'] = df_r3_SE['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2909285177.py:55: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_SE[\"hh_id\"] = df_r3_SE.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21392\\2909285177.py:59: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_SE[\"hhm_id\"] = df_r3_SE.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Sociaux economics caracteristiques of the household members\n",
    "\n",
    "## Select columns of interest from the dataframe df_r3\n",
    "df_r3_SE = df_r3_SE.loc[:, ['a01', 'mid', 'mem_stat' ,'b1_01', 'b1_02', 'b1_03', 'b1_04', 'b1_07','b1_08','b1_09']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r3_SE = df_r3_SE.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"b1_01\": \"hhm_sex\", \"b1_02\": \"hhm_age\", 'b1_03':'relation_hhh', 'b1_04':'marital_status_hhm', 'b1_07':'literacy_hhm',\n",
    "                            \"b1_08\": \"education_high\", 'b1_09':'curr_att_school','mem_stat':'hhm_status'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r3_SE' to correct the 'hh_id\n",
    "df_r3_SE['hh_id'] = df_r3_SE['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r3_SE['hh_id'] = df_r3_SE['hh_id'].astype(\"string\")\n",
    "df_r3_SE['hhm_id'] = df_r3_SE['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_SE['hhm_sex'] = df_r3_SE['hhm_sex'].astype(\"category\")\n",
    "df_r3_SE['hhm_age'] = df_r3_SE['hhm_age'].astype(\"int64\", errors='ignore')\n",
    "\n",
    "df_r3_SE['relation_hhh'] = df_r3_SE['relation_hhh'].astype(\"category\")\n",
    "df_r3_SE['marital_status_hhm'] = df_r3_SE['marital_status_hhm'].astype(\"category\")\n",
    "df_r3_SE['literacy_hhm'] = df_r3_SE['literacy_hhm'].astype(\"category\")\n",
    "\n",
    "df_r3_SE['education_high'] = df_r3_SE['education_high'].astype(\"category\")\n",
    "df_r3_SE['curr_att_school'] = df_r3_SE['curr_att_school'].astype(\"category\")\n",
    "\n",
    "\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r3_SE['relation_hhh'] = df_r3_SE['relation_hhh'].cat.rename_categories({1:'primary respondent',2:'primary respondent husband/wife',3:'son/daughter',4:'daughter/son-in-law',5:'grandson/daughter',6:'father/mother',\n",
    "                                                            7:'brother/sister',8:'niece/nephew',9:'primary respondent?s cousin',10:'father-in-law/mother-in-law',11:'brother/sister-in-law',\n",
    "                                                            12:'husband/wife?s  niece/nephew',13:'primary respondent?s husband/wife\\'s cousin',14:'other relative',15:'permanent servant',16:'other non relative/friends'})\n",
    "\n",
    "df_r3_SE['marital_status_hhm'] = df_r3_SE['marital_status_hhm'].cat.rename_categories({1:'unmarried (never married)',2:'married',3:'widow/widower',4:'divorced',5:'separated/deserted'})\n",
    "\n",
    "df_r3_SE['literacy_hhm'] = df_r3_SE['literacy_hhm'].cat.rename_categories({1:'cannot read and write',2:'can sign only',3:'can read  only',4:'can read and write'})\n",
    "\n",
    "df_r3_SE['education_high'] = df_r3_SE['education_high'].cat.rename_categories({0:'reads in class i',1:'completed class i',2:'completed class 2',3:'completed class 3',4:'completed class 4',5:'completed class 5',\n",
    "                                                                6:'completed class 6',7:'completed class 7',8:'completed class 8',9:'completed class 9',10:'completed ssc/dakhil',12:'completed hsc/alim',14:'ba/bsc pass/fazil',\n",
    "                                                                15:'ba/bsc honors/fazil',16:'ma/msc and above/kamil',22:'ssc candidate',33:'hsc candidate',66:'preschool class (general)',67:'preschool (mosque based)',\n",
    "                                                                71:'medical/mbbs',72:'nursing',73:'diploma engineer',74:'diploma Enginee',75:'vocational (scolarship based /technical Education',76:'others(specify)',99:'never attended school'})\n",
    "\n",
    "df_r3_SE['curr_att_school'] = df_r3_SE['curr_att_school'].cat.rename_categories({1:'yes',2:'no'})\n",
    "\n",
    "\n",
    "df_r3_SE['hhm_status'] = df_r3_SE['hhm_status'].replace({0:'Previous and current round member',1:'New member (new born)',2:'New member through marriage',3:'New member upon return from divorce or seperation',4:'Household merged/combined',5:'Other reasons (permanent)',\n",
    "                                                                6:'Residing elsewhere for the pursuit of studies',7:'Death',8:'Married and left household',9:'Divorced and left household',10:'Household split',11:'Left household for employment',12:'Other reasns for leaving the household',\n",
    "                                                                66:'New sample household and current round member'})\n",
    "\n",
    "#{0:'Prvs and crrnt rnd Mmbr',1:'Nw mmbr (Nw Brn)',2:'Nw mmbr thrgh mrrg',3:'Nw mmbr upon rtrn frm dvrc or sprtn',4:'Hshld mrgd/cmbnd',5:'Other reasons (Permanent)',\n",
    "#                                                                6:'Residing elsewhere for the pursuit of studies',7:'Death',8:'Married and left household',9:'Divorced and left household',10:'Household split',11:'Left household for employment',12:'Other reasns for leaving the household',\n",
    "#                                                                66:'New sample Household and current round member'}\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r3_SE.hh_id.str.len().max() - 3\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r3_SE[\"hh_id\"] = df_r3_SE.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r3_SE.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each hh member code to ensure they all have the same length and update the 'hhm_id' column with the corrected codes\n",
    "df_r3_SE[\"hhm_id\"] = df_r3_SE.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r3_SE[\"member_id\"] = df_r3_SE.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "Previous and current round member                    19284\n",
       "Household split                                       3310\n",
       "New member (new born)                                 1738\n",
       "Left household for employment                         1603\n",
       "Married and left household                            1552\n",
       "Other reasons (permanent)                             1372\n",
       "Other reasns for leaving the household                1091\n",
       "Death                                                  993\n",
       "New member through marriage                            609\n",
       "Residing elsewhere for the pursuit of studies          552\n",
       "Household merged/combined                              120\n",
       "Divorced and left household                             94\n",
       "New member upon return from divorce or seperation       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_SE['hhm_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building some variales characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is in the last round of the survey around 6011 households for a total of 5503 distinct household. Which represent an attrition rate of 15.37 percent (not data on the reason for not being surveyed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interview_status\n",
       "Complete       6436\n",
       "Migrated        237\n",
       "Not at home      30\n",
       "Refused          12\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r2.interview_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the round 2, there is a total of 6715 household survey. Among these, 396 household splitted from the round 1 (around 6 percent of the nbr in the first round): \n",
    "* Complete       6436 (99 percent); 6040 (92.88 percent) among the hh who doesn't split ; 6224 distinct hh in r1\n",
    "* Migrated        237 (3.64 percent); 237 (3.64 percent) ;\n",
    "* Not at home      30 (0.5 percent); 30 (0.5 percent);\n",
    "* Refused          12 (0.2 percent); 12 (0.2 percent);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying hh where present in the first round but migrate or leave the house, refused in r2\n",
    "\n",
    "lst_complete = df_r2[df_r2.interview_status==\"Complete\"].hh_id_parent.drop_duplicates().values\n",
    "lst_migrate = df_r2[df_r2.interview_status==\"Migrated\"].hh_id_parent.drop_duplicates().values\n",
    "lst_nthome = df_r2[df_r2.interview_status==\"Not at home\"].hh_id_parent.drop_duplicates().values\n",
    "lst_refused = df_r2[df_r2.interview_status==\"Refused\"].hh_id_parent.drop_duplicates().values\n",
    "lst_split = df_r2[df_r2.hh_split==\"1\"].hh_id_parent.drop_duplicates().values\n",
    "\n",
    "df_r1[\"interview_status_r2\"] = df_r1.hh_id.apply(lambda row: \"Complete\" if row in lst_complete else \"Migrated\" if row in lst_migrate else \"Not at home\" if row in lst_nthome else \"Refused\" if row in lst_refused else \"Not found\")\n",
    "df_r1[\"hh_split_r2\"] = df_r1.hh_id.apply(lambda row: \"1\" if row in lst_split else \"0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hh_split_r2\n",
       "0    6319\n",
       "1     184\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r1[\"hh_split_r2\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lst_r3 = np.unique(df_r3[df_r3.hh_split_btwn_r2_r3==\"1\"].hh_id_parent)\n",
    "#lst_r2 = np.unique(df_r2[df_r2.hh_split==\"1\"].hh_id_parent)\n",
    "# There is 285 household who split between R2 and R3 giving birth to 606 households in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the round 3, there is a total of 6011 household survey. Among these, 935 households splitted from the round 2 (around 14.37 percent of the nbr in the first round): \n",
    "* Complete       5604 (86.17 percent); 4690 (72.12 percent) among the hh who doesn't split \n",
    "* Migrated        360 (5.53 percent); 346 (5.32 percent) \n",
    "* Not at home      24 (0.36 percent); 24 (0.36 percent)\n",
    "* Refused          22 (0.34 percent); 15 (0.34 percent)\n",
    "* Partial           1 (0.01 percent); 1 (0.01 percent)\n",
    "* **Not found**: 1421 (22 percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying hh who were present in the first round but migrate or leave the house, refused or partial refused\n",
    "\n",
    "lst_complete = df_r3[df_r3.interview_status==\"Complete\"].hh_id_parent.drop_duplicates().values\n",
    "lst_migrate = df_r3[df_r3.interview_status==\"Migrated\"].hh_id_parent.drop_duplicates().values\n",
    "lst_nthome = df_r3[df_r3.interview_status==\"Not at home\"].hh_id_parent.drop_duplicates().values\n",
    "lst_refused = df_r3[df_r3.interview_status==\"Refused\"].hh_id_parent.drop_duplicates().values\n",
    "lst_split = df_r3[df_r3.hh_split==\"1\"].hh_id_parent.drop_duplicates().values\n",
    "\n",
    "df_r1[\"interview_status_r3\"] = df_r1.hh_id.apply(lambda row: \"Complete\" if row in lst_complete else \"Migrated\" if row in lst_migrate else \"Not at home\" if row in lst_nthome else \"Refused\" if row in lst_refused else \"Not found\")\n",
    "df_r1[\"hh_split_r3\"] = df_r1.hh_id.apply(lambda row: \"1\" if row in lst_split else \"0\")\n",
    "\n",
    "df_r1[\"hh_split_btwn_r2_r3\"] = df_r1.hh_id.apply(lambda row: \"1\" if row in lst_r3 else \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    358\n",
       "3     58\n",
       "4     10\n",
       "5      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3[df_r3.hh_split==\"1\"].groupby(\"hh_id_parent\").size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary for hh of interest\n",
    "df_r1[\"hh_interest\"] = df_r1.apply(lambda row: \"1\" if (row.loc[\"interview_status_r3\"]==\"Complete\") & (row.loc[\"hh_split_r2\"]==\"0\") & (row.loc[\"hh_split_r3\"]==\"0\") else \"0\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hh_split_r2_r3\n",
       "0    6054\n",
       "1     449\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary for the household who split at least one time\n",
    "df_r1[\"hh_split_r2_r3\"] = df_r1.apply(lambda row: \"1\" if row[\"hh_split_r2\"]==\"1\" or row[\"hh_split_r3\"]==\"1\" else \"0\", axis=1)\n",
    "df_r1[\"hh_split_r2_r3\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_r1[(df_r1[\"hh_id\"].isin(lst_r3)) & (df_r1[\"hh_split_r2_r3\"]==\"1\")]\n",
    "#df_r1[(df_r1[\"hh_id\"].isin(lst_r3)) & (df_r1[\"hh_id\"].isin(lst_r2))].hh_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1\n",
    "\n",
    "## Group the DataFrame 'df_r1_SE' by 'hh_id'\n",
    "df_r1_SE_gr = df_r1_SE.groupby('hh_id')\n",
    "\n",
    "## Aggregate the grouped data\n",
    "# aggregated_result_r1 = df_r1_SE_gr.agg(\n",
    "#     ## Count the number of females in each household\n",
    "#     nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),\n",
    "\n",
    "#     ## Count the number of household members under 5 years old\n",
    "#     nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 5 and 10 years old\n",
    "#     nbr_yngchldrn_5_10 = ('hhm_age', lambda x: ((x >= 5)  & (x < 10)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 10 and 19 years old\n",
    "#     nbr_teenager_10_19 =('hhm_age', lambda x: ((x >= 10)  & (x < 19)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 20 and 65 years old\n",
    "#     nbr_adults_20_65 = ('hhm_age', lambda x: ((x >= 20)  & (x < 65)).sum()),\n",
    "\n",
    "#     ## Count the number of household members 65 years old and over\n",
    "#     nbr_elderly_65_over = ('hhm_age', lambda x: (x >= 65).sum()),\n",
    "# ).reset_index()\n",
    "\n",
    "\n",
    "# Round 2\n",
    "\n",
    "## Group the DataFrame 'df_r1_SE' by 'hh_id'\n",
    "df_r2_SE_gr = df_r2_SE.groupby('hh_id')\n",
    "\n",
    "# ## Aggregate the grouped data\n",
    "# aggregated_result_r2 = df_r2_SE_gr.agg(\n",
    "#     ## Count the number of females in each household\n",
    "#     nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),\n",
    "\n",
    "#     ## Count the number of household members under 5 years old\n",
    "#     nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 5 and 10 years old\n",
    "#     nbr_yngchldrn_5_10 = ('hhm_age', lambda x: ((x >= 5)  & (x < 10)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 10 and 19 years old\n",
    "#     nbr_teenager_10_19 =('hhm_age', lambda x: ((x >= 10)  & (x < 19)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 20 and 65 years old\n",
    "#     nbr_adults_20_65 = ('hhm_age', lambda x: ((x >= 20)  & (x < 65)).sum()),\n",
    "\n",
    "#     ## Count the number of household members 65 years old and over\n",
    "#     nbr_elderly_65_over = ('hhm_age', lambda x: (x >= 65).sum()),\n",
    "# ).reset_index()\n",
    "\n",
    "\n",
    "# Round 3\n",
    "\n",
    "## Group the DataFrame 'df_r1_SE' by 'hh_id'\n",
    "df_r3_SE_gr = df_r3_SE.groupby('hh_id')\n",
    "\n",
    "## Aggregate the grouped data\n",
    "# aggregated_result_r3 = df_r3_SE_gr.agg(\n",
    "#     ## Count the number of females in each household\n",
    "#     nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),\n",
    "\n",
    "#     ## Count the number of household members under 5 years old\n",
    "#     nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 5 and 10 years old\n",
    "#     nbr_yngchldrn_5_10 = ('hhm_age', lambda x: ((x >= 5)  & (x < 10)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 10 and 19 years old\n",
    "#     nbr_teenager_10_19 =('hhm_age', lambda x: ((x >= 10)  & (x < 19)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 20 and 65 years old\n",
    "#     nbr_adults_20_65 = ('hhm_age', lambda x: ((x >= 20)  & (x < 65)).sum()),\n",
    "\n",
    "#     ## Count the number of household members 65 years old and over\n",
    "#     nbr_elderly_65_over = ('hhm_age', lambda x: (x >= 65).sum()),\n",
    "# ).reset_index()\n",
    "\n",
    "# Display the aggregated result\n",
    "#aggregated_result_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1\n",
    "df_r1_SE = (\n",
    "    df_r1_SE\n",
    "        # Merge the original dataframe with the aggregated counts of household members by age and sex\n",
    "        .merge(\n",
    "                df_r1_SE_gr\n",
    "                .agg(\n",
    "                    nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),  # Count of females in the household\n",
    "                    nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),  # Count of children under 5 years old\n",
    "                    nbr_yngchldrn_5_10=('hhm_age', lambda x: ((x >= 5) & (x < 10)).sum()),  # Count of children aged 5-10\n",
    "                    nbr_teenager_10_20=('hhm_age', lambda x: ((x >= 10) & (x < 20)).sum()),  # Count of teenagers aged 10-20\n",
    "                    nbr_adults_20_65=('hhm_age', lambda x: ((x >= 20) & (x < 65)).sum()),  # Count of adults aged 20-65\n",
    "                    nbr_elderly_65_over=('hhm_age', lambda x: (x >= 65).sum())  # Count of elderly aged 65 and over\n",
    "                )\n",
    "                .reset_index()  # Reset index to prepare for merging\n",
    "        )\n",
    "        # Merge with the count of females under 5 years old\n",
    "        .merge(\n",
    "            df_r1_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]<5)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_underfive'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 5-10\n",
    "        .merge(\n",
    "            df_r1_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=5) & (row[\"hhm_age\"] < 10)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_yngchldrn_5_10'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 10-20\n",
    "        .merge(\n",
    "            df_r1_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=10) & (row[\"hhm_age\"] < 20)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_teenager_10_20'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 20-65\n",
    "        .merge(\n",
    "            df_r1_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=20) & (row[\"hhm_age\"] < 65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_adults_20_65'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 65 and over\n",
    "        .merge(\n",
    "            df_r1_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_elderly_65_over'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    ")\n",
    "\n",
    "\n",
    "# Round 2\n",
    "\n",
    "df_r2_SE = (\n",
    "    df_r2_SE\n",
    "        # Merge the original dataframe with the aggregated counts of household members by age and sex\n",
    "        .merge(\n",
    "                df_r2_SE_gr\n",
    "                .agg(\n",
    "                    nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),  # Count of females in the household\n",
    "                    nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),  # Count of children under 5 years old\n",
    "                    nbr_yngchldrn_5_10=('hhm_age', lambda x: ((x >= 5) & (x < 10)).sum()),  # Count of children aged 5-10\n",
    "                    nbr_teenager_10_20=('hhm_age', lambda x: ((x >= 10) & (x < 20)).sum()),  # Count of teenagers aged 10-20\n",
    "                    nbr_adults_20_65=('hhm_age', lambda x: ((x >= 20) & (x < 65)).sum()),  # Count of adults aged 20-65\n",
    "                    nbr_elderly_65_over=('hhm_age', lambda x: (x >= 65).sum())  # Count of elderly aged 65 and over\n",
    "                )\n",
    "                .reset_index()  # Reset index to prepare for merging\n",
    "        )\n",
    "        # Merge with the count of females under 5 years old\n",
    "        .merge(\n",
    "            df_r2_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]<5)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_underfive'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 5-10\n",
    "        .merge(\n",
    "            df_r2_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=5) & (row[\"hhm_age\"] < 10)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_yngchldrn_5_10'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 10-20\n",
    "        .merge(\n",
    "            df_r2_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=10) & (row[\"hhm_age\"] < 20)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_teenager_10_20'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 20-65\n",
    "        .merge(\n",
    "            df_r2_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=20) & (row[\"hhm_age\"] < 65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_adults_20_65'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 65 and over\n",
    "        .merge(\n",
    "            df_r2_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_elderly_65_over'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    ")\n",
    "\n",
    "# Round 3\n",
    "df_r3_SE = (\n",
    "    df_r3_SE\n",
    "        # Merge the original dataframe with the aggregated counts of household members by age and sex\n",
    "        .merge(\n",
    "                df_r3_SE_gr\n",
    "                .agg(\n",
    "                    nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),  # Count of females in the household\n",
    "                    nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),  # Count of children under 5 years old\n",
    "                    nbr_yngchldrn_5_10=('hhm_age', lambda x: ((x >= 5) & (x < 10)).sum()),  # Count of children aged 5-10\n",
    "                    nbr_teenager_10_20=('hhm_age', lambda x: ((x >= 10) & (x < 20)).sum()),  # Count of teenagers aged 10-20\n",
    "                    nbr_adults_20_65=('hhm_age', lambda x: ((x >= 20) & (x < 65)).sum()),  # Count of adults aged 20-65\n",
    "                    nbr_elderly_65_over=('hhm_age', lambda x: (x >= 65).sum())  # Count of elderly aged 65 and over\n",
    "                )\n",
    "                .reset_index()  # Reset index to prepare for merging\n",
    "        )\n",
    "        # Merge with the count of females under 5 years old\n",
    "        .merge(\n",
    "            df_r3_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]<5)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_underfive'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 5-10\n",
    "        .merge(\n",
    "            df_r3_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=5) & (row[\"hhm_age\"] < 10)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_yngchldrn_5_10'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 10-20\n",
    "        .merge(\n",
    "            df_r3_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=10) & (row[\"hhm_age\"] < 20)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_teenager_10_20'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 20-65\n",
    "        .merge(\n",
    "            df_r3_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=20) & (row[\"hhm_age\"] < 65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_adults_20_65'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 65 and over\n",
    "        .merge(\n",
    "            df_r3_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_elderly_65_over'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hh_id', 'hhm_id', 'hhm_status', 'hhm_sex', 'hhm_age', 'relation_hhh',\n",
       "       'marital_status_hhm', 'literacy_hhm', 'education_high',\n",
       "       'curr_att_school', 'member_id', 'nbr_female', 'nbr_underfive',\n",
       "       'nbr_yngchldrn_5_10', 'nbr_teenager_10_20', 'nbr_adults_20_65',\n",
       "       'nbr_elderly_65_over', 'nbr_female_underfive',\n",
       "       'nbr_female_yngchldrn_5_10', 'nbr_female_teenager_10_20',\n",
       "       'nbr_female_adults_20_65', 'nbr_female_elderly_65_over'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_SE.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For all household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Interview status R3</th>\n",
       "      <th>Complete</th>\n",
       "      <th>Migrated</th>\n",
       "      <th>Not at home</th>\n",
       "      <th>Not found</th>\n",
       "      <th>Refused</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interview status R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Complete</th>\n",
       "      <td>5025</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "      <td>965</td>\n",
       "      <td>11</td>\n",
       "      <td>6224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Migrated</th>\n",
       "      <td>68</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not at home</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Refused</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>5117</td>\n",
       "      <td>346</td>\n",
       "      <td>24</td>\n",
       "      <td>1001</td>\n",
       "      <td>15</td>\n",
       "      <td>6503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Interview status R3  Complete  Migrated  Not at home  Not found  Refused   All\n",
       "Interview status R2                                                           \n",
       "Complete                 5025       200           23        965       11  6224\n",
       "Migrated                   68       137            1         31        0   237\n",
       "Not at home                17         8            0          4        1    30\n",
       "Refused                     7         1            0          1        3    12\n",
       "All                      5117       346           24       1001       15  6503"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_r1[\"interview_status_r2\"], df_r1[\"interview_status_r3\"], margins=True, rownames=[\"Interview status R2\"], colnames=[\"Interview status R3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the household in the nationally representative sample (for comparaison purpose with the sample the table in the repport of the BIHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some discrepancies between the report and the dataset that I can't explain:\n",
    "\n",
    "* In the dataset, the FTF additional sample size is 1,080, whereas the report states it as 1,040.\n",
    "* The sum of National Representative (4,423) and FTF Original (1,000) samples equals 5,423. However, the report indicates a total of 5,503, meaning 80 households are missing from the dataset compared to the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Interview status R3</th>\n",
       "      <th>Complete</th>\n",
       "      <th>Migrated</th>\n",
       "      <th>Not at home</th>\n",
       "      <th>Not found</th>\n",
       "      <th>Refused</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interview status R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Complete</th>\n",
       "      <td>5025</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Migrated</th>\n",
       "      <td>68</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not at home</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Refused</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>5117</td>\n",
       "      <td>346</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Interview status R3  Complete  Migrated  Not at home  Not found  Refused   All\n",
       "Interview status R2                                                           \n",
       "Complete                 5025       200           23          1       11  5260\n",
       "Migrated                   68       137            1          0        0   206\n",
       "Not at home                17         8            0          0        1    26\n",
       "Refused                     7         1            0          0        3    11\n",
       "All                      5117       346           24          1       15  5503"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_r1[(df_r1.sample_type == \"National Representative\") | (df_r1.sample_type ==\"FTF additional\")][\"interview_status_r2\"], df_r1[(df_r1.sample_type == \"National Representative\") | (df_r1.sample_type ==\"FTF additional\")][\"interview_status_r3\"], margins=True, rownames=[\"Interview status R2\"], colnames=[\"Interview status R3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merging with the crosswalk\n",
    "df_r1 = df_r1.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "df_r2 = df_r2.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "\n",
    "## merging to count the nbr of hhm\n",
    "df_r1 = df_r1.merge(df_r1_SE.groupby(['hh_id'], as_index=False).size(), how=\"left\", on=[\"hh_id\"]).rename(columns={'size':'hh_size'}) \n",
    "df_r2 = df_r2.merge(df_r2_SE.groupby(['hh_id'], as_index=False).size(), how=\"left\", on=[\"hh_id\"]).rename(columns={'size':'hh_size'}) \n",
    "df_r3 = df_r3.merge(df_r3_SE.groupby(['hh_id'], as_index=False).size(), how=\"left\", on=[\"hh_id\"]).rename(columns={'size':'hh_size'}) \n",
    "\n",
    "df_r1[\"hh_size\"] = df_r1[\"hh_size\"].astype(\"int64\", errors='ignore')\n",
    "df_r2[\"hh_size\"] = df_r2[\"hh_size\"].astype(\"int64\", errors='ignore')\n",
    "df_r3[\"hh_size\"] = df_r3[\"hh_size\"].astype(\"int64\", errors='ignore')\n",
    "\n",
    "\n",
    "# merging to collect the same informations in r1 and r3\n",
    "df_r3 = df_r3.merge(df_r1[['hh_id','district_name','upazila_name','union_name','union_code','code adm','affected_upazila_all','affected_komen', 'affected_roanu', 'affected_mora']], \n",
    "                    right_on='hh_id', left_on='hh_id_parent', how='left', suffixes=('_left', '_right')).rename(columns={'hh_id_left':'hh_id', 'union_code_right':'union_code'}).drop(columns=['hh_id_right', 'union_code_left'], axis=1)\n",
    "\n",
    "\n",
    "df_r1[\"treat_group_all\"] = 0\n",
    "df_r2[\"treat_group_all\"] = df_r2.apply(lambda x: 1 if (x[\"affected_upazila_all\"]==1) & (x[\"interview_status\"]==\"Complete\") else 0 if (x[\"affected_upazila_all\"]==0) & (x[\"interview_status\"]==\"Complete\") else np.nan, axis=1)\n",
    "df_r3[\"treat_group_all\"] = df_r3.apply(lambda x: 1 if (x[\"affected_upazila_all\"]==1) & (x[\"interview_status\"]==\"Complete\") else 0 if (x[\"affected_upazila_all\"]==0) & (x[\"interview_status\"]==\"Complete\") else np.nan, axis=1)\n",
    "\n",
    "df_r1[\"treat_group_komen\"] = 0\n",
    "df_r2[\"treat_group_komen\"] = df_r2.apply(lambda x: 1 if (x[\"affected_komen\"]==1) & (x[\"interview_status\"]==\"Complete\") else 0 if (x[\"affected_komen\"]==0) & (x[\"interview_status\"]==\"Complete\") else np.nan, axis=1)\n",
    "df_r3[\"treat_group_komen\"] = df_r3.apply(lambda x: 1 if (x[\"affected_komen\"]==1) & (x[\"interview_status\"]==\"Complete\") else 0 if (x[\"affected_komen\"]==0) & (x[\"interview_status\"]==\"Complete\") else np.nan, axis=1)\n",
    "\n",
    "df_r1[\"treat_group_roanu\"] = 0\n",
    "df_r2[\"treat_group_roanu\"] = df_r2.apply(lambda x: 1 if (x[\"affected_roanu\"]==1) & (x[\"interview_status\"]==\"Complete\") else 0 if (x[\"affected_roanu\"]==0) & (x[\"interview_status\"]==\"Complete\") else np.nan, axis=1)\n",
    "df_r3[\"treat_group_roanu\"] = df_r3.apply(lambda x: 1 if (x[\"affected_roanu\"]==1) & (x[\"interview_status\"]==\"Complete\") else 0 if (x[\"affected_roanu\"]==0) & (x[\"interview_status\"]==\"Complete\") else np.nan, axis=1)\n",
    "\n",
    "df_r1[\"treat_group_mora\"] = 0\n",
    "df_r2[\"treat_group_mora\"] = df_r2.apply(lambda x: 1 if (x[\"affected_mora\"]==1) & (x[\"interview_status\"]==\"Complete\") else 0 if (x[\"affected_mora\"]==0) & (x[\"interview_status\"]==\"Complete\") else np.nan, axis=1)\n",
    "df_r3[\"treat_group_mora\"] = df_r3.apply(lambda x: 1 if (x[\"affected_mora\"]==1) & (x[\"interview_status\"]==\"Complete\") else 0 if (x[\"affected_mora\"]==0) & (x[\"interview_status\"]==\"Complete\") else np.nan, axis=1)\n",
    "\n",
    "\n",
    "df_r1[\"survey_year\"] = 2011\n",
    "df_r2[\"survey_year\"] = 2015\n",
    "df_r3[\"survey_year\"] = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First merge to obtain the dataset for adults\n",
    "## Merging to obtains the anthro and the informations on age, sex and id\n",
    "df_r1_anthr1 = df_r1_anthr1.merge(df_r1_SE, on=['hh_id','hhm_id'], how='left')\n",
    "df_r2_anthr1 = df_r2_anthr1.merge(df_r2_SE, on=['hh_id','hhm_id'], how='left')\n",
    "df_r3_anthr1 = df_r3_anthr1.merge(df_r3_SE, on=['hh_id','hhm_id'], how='left')\n",
    "\n",
    "## Merging to obtains the information on the household localisation (division, district, upazilla, union, moza, village)\n",
    "df_r1_anthr1 = df_r1_anthr1.merge(df_r1, on=['hh_id'], how='left').drop([\"index_x\",\"index_y\",\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})\n",
    "df_r2_anthr1 = df_r2_anthr1.merge(df_r2, on=['hh_id'], how='left').drop([\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})\n",
    "df_r3_anthr1 = df_r3_anthr1.merge(df_r3, on=['hh_id'], how='left').drop([\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})\n",
    "\n",
    "\n",
    "# Second merge to obtain the dataset for children\n",
    "## Merging to obtains the anthro and the informations on age, sex and id\n",
    "df_r1_anthr2 = df_r1_anthr2.merge(df_r1_SE, on=['hh_id','hhm_id'], how='left')\n",
    "df_r2_anthr2 = df_r2_anthr2.merge(df_r2_SE, on=['hh_id','hhm_id'], how='left')\n",
    "df_r3_anthr2 = df_r3_anthr2.merge(df_r3_SE, on=['hh_id','hhm_id'], how='left')\n",
    "\n",
    "## Merging to obtains the information on the household localisation (division, district, upazilla, union, moza, village)\n",
    "df_r1_anthr2 = df_r1_anthr2.merge(df_r1, on=['hh_id'], how='left').drop([\"index_x\",\"index_y\",\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})\n",
    "df_r2_anthr2 = df_r2_anthr2.merge(df_r2, on=['hh_id'], how='left').drop([\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})\n",
    "df_r3_anthr2 = df_r3_anthr2.merge(df_r3, on=['hh_id'], how='left').drop([\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing age in days by subtracting the birth date from the interview date\n",
    "df_r1_anthr2[\"age_days\"] = (df_r1_anthr2.date_int - df_r1_anthr2.date_birth).dt.days\n",
    "\n",
    "# Mapping household member sex to strings \"male\" and \"female\"\n",
    "df_r1_anthr2[\"hhm_sex\"] = df_r1_anthr2[\"hhm_sex\"].map({\"male\": \"male\", \"female\": \"female\"})\n",
    "\n",
    "df_r2_anthr1[\"hhm_sex\"] = df_r2_anthr1[\"hhm_sex\"].map({1: \"male\", 2: \"female\"})\n",
    "df_r2_anthr2[\"hhm_sex\"] = df_r2_anthr2[\"hhm_sex\"].map({1: \"male\", 2: \"female\"})\n",
    "\n",
    "df_r3_anthr1[\"hhm_sex\"] = df_r3_anthr1[\"hhm_sex\"].map({1: \"male\", 2: \"female\"})\n",
    "df_r3_anthr2[\"hhm_sex\"] = df_r3_anthr2[\"hhm_sex\"].map({1: \"male\", 2: \"female\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "New member (new born)                1674\n",
       "Previous and current round member     914\n",
       "Other reasons (permanent)             128\n",
       "Household merged/combined              47\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r2_anthr2.hhm_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "Previous and current round member                    18782\n",
       "Other reasons (permanent)                             1242\n",
       "New member through marriage                            609\n",
       "Household merged/combined                              107\n",
       "New member (new born)                                   18\n",
       "New member upon return from divorce or seperation       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_anthr1.hhm_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unborn children status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "if_not_measured_why\n",
       "have measured    2277\n",
       "absent             64\n",
       "refused             9\n",
       "sick                3\n",
       "other               1\n",
       "missing             0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_anthr2[\"if_not_measured_why\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_1 = df_r1.hh_id.values\n",
    "\n",
    "lst_2 = df_r2.hh_id_parent.values\n",
    "\n",
    "diff = list(set(lst_1) - set(lst_2))\n",
    "\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merging with the crosswalk\n",
    "#df_r1_anthr1 = df_r1_anthr1.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "#df_r2_anthr1 = df_r2_anthr1.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "#df_r3_anthr1 = df_r3_anthr1.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "\n",
    "## merging with the crosswalk\n",
    "#df_r1_anthr2 = df_r1_anthr2.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "#df_r2_anthr2 = df_r2_anthr2.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "#df_r3_anthr2 = df_r3_anthr2.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       count\n",
      "affected_upazila_all interview_status       \n",
      "0                    Complete           3877\n",
      "                     Migrated            146\n",
      "                     Not at home          16\n",
      "                     Refused               4\n",
      "1                    Complete           1350\n",
      "                     Migrated             54\n",
      "                     Not at home          10\n",
      "                     Refused               6\n"
     ]
    }
   ],
   "source": [
    "#### Status of households surveyed (migrated)\n",
    "print(df_r2[df_r2.sample_type!=2].drop_duplicates(subset=['hh_id_parent'], keep='first').groupby(['affected_upazila_all', 'interview_status'], observed=False)['hh_id_parent'].agg(['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Status of households surveyed (migrated)\n",
    "# df_r2[df_r2.interview_status==\"Completed\"].groupby(['affected_upazila','hh_split']).agg(nbr_splited=(\"hh_split\",\"count\"))\n",
    "#print(df_r2.groupby(['affected_upazila', 'interview_status'], observed=False)['hh_split'].agg(['count']))\n",
    "# pd.crosstab(df_r2.affected_upazila, df_r2.hh_split, margins=True, normalize=\"index\", rownames=['Affected upazila'], colnames=['Hh split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       count\n",
      "affected_upazila_all interview_status       \n",
      "0                    Complete           4129\n",
      "                     Refused               9\n",
      "                     Not at home          14\n",
      "                     Migrated            271\n",
      "                     Partial               1\n",
      "1                    Complete           1475\n",
      "                     Refused              13\n",
      "                     Not at home          10\n",
      "                     Migrated             89\n",
      "                     Partial               0\n"
     ]
    }
   ],
   "source": [
    "#### Status of households surveyed (migrated)\n",
    "print(df_r3.groupby(['affected_upazila_all', 'interview_status'], observed=False)['hh_id'].agg(['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the global dataframe (for the 3 survey round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "Previous and current round member                    18782\n",
       "Other reasons (permanent)                             1242\n",
       "New member through marriage                            609\n",
       "Household merged/combined                              107\n",
       "New member (new born)                                   18\n",
       "New member upon return from divorce or seperation       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_anthr1.hhm_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i keep only the hh with completed the survey and don't split during the study. Set remove the duplicates in the list\n",
    "lst_hhm_id_r3 = np.unique(np.concatenate((df_r3_anthr1[(df_r3_anthr1.interview_status==\"Complete\") & (df_r3_anthr1.hh_split==\"0\") & (df_r3_anthr1.hhm_age < 20) & (df_r3_anthr1.hhm_status==\"Previous and current round member\")][\"member_id\"].values, \n",
    "                                        df_r3_anthr2[(df_r3_anthr2.interview_status==\"Complete\") & (df_r3_anthr2.hh_split==\"0\") & (df_r3_anthr2.hhm_age >= 3) & (df_r3_anthr2.hhm_status==\"Previous and current round member\")][\"member_id\"].values)))\n",
    "\n",
    "\n",
    "# lst of underfive during the first or secind round\n",
    "## fist (underfive first round); second (under 12 first round) and underfive in round 2\n",
    "# 9238 children not splitted and before the natural disaster\n",
    "#lst_underfive_int =  np.unique(np.concatenate((df_r1_anthr2.member_id.values, \n",
    "#                                            df_r1_anthr1[df_r1_anthr1.hhm_age <= 12].member_id.values, \n",
    "#                                            df_r2_anthr2[(df_r2_anthr2.interview_status==\"Complete\") &  (df_r2_anthr2.hh_split==\"0\")].member_id.values)))\n",
    "lst_intvar = ['union_code','union_name','hh_id', 'hhm_id','weight_kg','height_cm','bmi' ,'if_not_measured_why', 'member_id','hhm_sex','hhm_status', 'hhm_age', 'relation_hhh', 'marital_status_hhm','hh_size',\n",
    "    'literacy_hhm', 'education_high', 'curr_att_school', 'hh_head_religion','hh_ethnic_group','nbr_female', 'nbr_underfive',\n",
    "    'nbr_yngchldrn_5_10', 'nbr_teenager_10_20', 'nbr_adults_20_65','nbr_elderly_65_over', 'nbr_female_underfive','nbr_female_yngchldrn_5_10', \n",
    "    'nbr_female_teenager_10_20','nbr_female_adults_20_65', 'nbr_female_elderly_65_over','sample_type', 'survey_round', 'code adm','affected_upazila_all','affected_komen', 'affected_roanu', 'affected_mora',]\n",
    "\n",
    "#df_r1_global = pd.concat([df_r1_anthr1[df_r1_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True), \n",
    "#                    df_r1_anthr2[df_r1_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True)\n",
    "\n",
    "#df_r2_global = pd.concat([df_r2_anthr1[df_r2_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True),\n",
    "#                    df_r2_anthr2[df_r2_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True)\n",
    "\n",
    "#df_r3_global = pd.concat([df_r3_anthr1[df_r3_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True),\n",
    "#                    df_r3_anthr2[df_r3_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True)\n",
    "\n",
    "df_global_corrected = pd.concat([pd.concat([df_r1_anthr1[df_r1_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True), \n",
    "                    df_r1_anthr2[df_r1_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True), \n",
    "\n",
    "                    pd.concat([df_r2_anthr1[df_r2_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True),\n",
    "                    df_r2_anthr2[df_r2_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True), \n",
    "                    \n",
    "                    pd.concat([df_r3_anthr1[df_r3_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True),\n",
    "                    df_r3_anthr2[df_r3_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True)], ignore_index = True, verify_integrity=True)\n",
    "\n",
    "\n",
    "df_global = pd.concat([pd.concat([df_r1_anthr1[lst_intvar].reset_index(drop=True), \n",
    "                    df_r1_anthr2[lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True), \n",
    "\n",
    "                    pd.concat([df_r2_anthr1[lst_intvar].reset_index(drop=True),\n",
    "                    df_r2_anthr2[lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True), \n",
    "                    \n",
    "                    pd.concat([df_r3_anthr1[lst_intvar].reset_index(drop=True),\n",
    "                    df_r3_anthr2[lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True)], ignore_index = True, verify_integrity=True)\n",
    "\n",
    "# treatment binaries\n",
    "df_global[\"treat_group_all\"] = df_global.apply(lambda x: 1 if (x[\"survey_round\"]==\"3\") & (x[\"affected_upazila_all\"]==1) else 0, axis=1)\n",
    "df_global[\"treat_komen\"] = df_global.apply(lambda x: 1 if (x[\"survey_round\"]==\"3\") & (x[\"affected_komen\"]==1) else 0, axis=1)\n",
    "df_global[\"treat_roanu\"] = df_global.apply(lambda x: 1 if (x[\"survey_round\"]==\"3\") & (x[\"affected_roanu\"]==1) else 0, axis=1)\n",
    "df_global[\"treat_mora\"] = df_global.apply(lambda x: 1 if (x[\"survey_round\"]==\"3\") & (x[\"affected_mora\"]==1) else 0, axis=1)\n",
    "\n",
    "\n",
    "df_global_corrected[\"treat_group_all\"] = df_global_corrected.apply(lambda x: 1 if (x[\"survey_round\"]==\"3\") & (x[\"affected_upazila_all\"]==1) else 0, axis=1)\n",
    "df_global_corrected[\"treat_komen\"] = df_global_corrected.apply(lambda x: 1 if (x[\"survey_round\"]==\"3\") & (x[\"affected_komen\"]==1) else 0, axis=1)\n",
    "df_global_corrected[\"treat_roanu\"] = df_global_corrected.apply(lambda x: 1 if (x[\"survey_round\"]==\"3\") & (x[\"affected_roanu\"]==1) else 0, axis=1)\n",
    "df_global_corrected[\"treat_mora\"] = df_global_corrected.apply(lambda x: 1 if (x[\"survey_round\"]==\"3\") & (x[\"affected_mora\"]==1) else 0, axis=1)\n",
    "\n",
    "\n",
    "# survey year\n",
    "df_global[\"survey_year\"] = df_global.apply(lambda x: 2018 if x[\"survey_round\"]==\"3\" else 2015 if x[\"survey_round\"]==\"2\" else 2011, axis=1)\n",
    "df_global_corrected[\"survey_year\"] = df_global_corrected.apply(lambda x: 2018 if x[\"survey_round\"]==\"3\" else 2015 if x[\"survey_round\"]==\"2\" else 2011, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data on FTF additional and National Representative data was collected during the third round of the BIHS. Data on FTF original was not collected, so i remove then from the previous rounds of the BIHS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "Previous and current round member                    18782\n",
       "Other reasons (permanent)                             1242\n",
       "New member through marriage                            609\n",
       "Household merged/combined                              107\n",
       "New member (new born)                                   18\n",
       "New member upon return from divorce or seperation       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_anthr1.hhm_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=df_global.groupby([\"member_id\"], observed=False)[\"survey_round\"].agg([\"sum\"])\n",
    "#test[].shape()\n",
    "test[\"sum\"] = test[\"sum\"].astype(\"int64\")\n",
    "testview= test[ test[\"sum\"] >= 4]\n",
    "#print(df_r3.groupby(['affected_upazila', 'interview_status'], observed=False)['hh_id'].agg(['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survey_round\n",
      "1    27285\n",
      "2    27354\n",
      "3    23126\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>union_code</th>\n",
       "      <th>union_name</th>\n",
       "      <th>hh_id</th>\n",
       "      <th>hhm_id</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>bmi</th>\n",
       "      <th>if_not_measured_why</th>\n",
       "      <th>member_id</th>\n",
       "      <th>hhm_sex</th>\n",
       "      <th>...</th>\n",
       "      <th>nbr_female_teenager_10_20</th>\n",
       "      <th>nbr_female_adults_20_65</th>\n",
       "      <th>nbr_female_elderly_65_over</th>\n",
       "      <th>sample_type</th>\n",
       "      <th>survey_round</th>\n",
       "      <th>code adm</th>\n",
       "      <th>affected_upazila_all</th>\n",
       "      <th>affected_komen</th>\n",
       "      <th>affected_roanu</th>\n",
       "      <th>affected_mora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010817</td>\n",
       "      <td>Barai Para</td>\n",
       "      <td>0001</td>\n",
       "      <td>01</td>\n",
       "      <td>56.700001</td>\n",
       "      <td>164.800003</td>\n",
       "      <td>20.877026</td>\n",
       "      <td>have measured</td>\n",
       "      <td>000101</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FTF additional</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010817</td>\n",
       "      <td>Barai Para</td>\n",
       "      <td>0001</td>\n",
       "      <td>02</td>\n",
       "      <td>45.900002</td>\n",
       "      <td>143.300003</td>\n",
       "      <td>22.352201</td>\n",
       "      <td>have measured</td>\n",
       "      <td>000102</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FTF additional</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010817</td>\n",
       "      <td>Barai Para</td>\n",
       "      <td>0001</td>\n",
       "      <td>03</td>\n",
       "      <td>58.599998</td>\n",
       "      <td>169.300003</td>\n",
       "      <td>20.444838</td>\n",
       "      <td>have measured</td>\n",
       "      <td>000103</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FTF additional</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010817</td>\n",
       "      <td>Barai Para</td>\n",
       "      <td>0001</td>\n",
       "      <td>04</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>170.300003</td>\n",
       "      <td>18.619347</td>\n",
       "      <td>have measured</td>\n",
       "      <td>000104</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FTF additional</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010817</td>\n",
       "      <td>Barai Para</td>\n",
       "      <td>0002</td>\n",
       "      <td>01</td>\n",
       "      <td>63.200001</td>\n",
       "      <td>168.399994</td>\n",
       "      <td>22.286042</td>\n",
       "      <td>have measured</td>\n",
       "      <td>000201</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FTF additional</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24369</th>\n",
       "      <td>857309</td>\n",
       "      <td>Annadanagar</td>\n",
       "      <td>6501</td>\n",
       "      <td>03</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>have measured</td>\n",
       "      <td>650103</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>National Representative</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24370</th>\n",
       "      <td>857309</td>\n",
       "      <td>Annadanagar</td>\n",
       "      <td>6502</td>\n",
       "      <td>01</td>\n",
       "      <td>48.599998</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>19.223923</td>\n",
       "      <td>have measured</td>\n",
       "      <td>650201</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>National Representative</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24371</th>\n",
       "      <td>857309</td>\n",
       "      <td>Annadanagar</td>\n",
       "      <td>6502</td>\n",
       "      <td>02</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>148.600006</td>\n",
       "      <td>22.642916</td>\n",
       "      <td>have measured</td>\n",
       "      <td>650202</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>National Representative</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24372</th>\n",
       "      <td>857309</td>\n",
       "      <td>Annadanagar</td>\n",
       "      <td>6503</td>\n",
       "      <td>01</td>\n",
       "      <td>43.799999</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>17.002940</td>\n",
       "      <td>have measured</td>\n",
       "      <td>650301</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>National Representative</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24373</th>\n",
       "      <td>857309</td>\n",
       "      <td>Annadanagar</td>\n",
       "      <td>6503</td>\n",
       "      <td>02</td>\n",
       "      <td>36.400002</td>\n",
       "      <td>139.600006</td>\n",
       "      <td>18.678007</td>\n",
       "      <td>have measured</td>\n",
       "      <td>650302</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>National Representative</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24374 rows  38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      union_code   union_name hh_id hhm_id  weight_kg   height_cm        bmi  \\\n",
       "0         010817   Barai Para  0001     01  56.700001  164.800003  20.877026   \n",
       "1         010817   Barai Para  0001     02  45.900002  143.300003  22.352201   \n",
       "2         010817   Barai Para  0001     03  58.599998  169.300003  20.444838   \n",
       "3         010817   Barai Para  0001     04  54.000000  170.300003  18.619347   \n",
       "4         010817   Barai Para  0002     01  63.200001  168.399994  22.286042   \n",
       "...          ...          ...   ...    ...        ...         ...        ...   \n",
       "24369     857309  Annadanagar  6501     03  15.500000  100.000000  15.500000   \n",
       "24370     857309  Annadanagar  6502     01  48.599998  159.000000  19.223923   \n",
       "24371     857309  Annadanagar  6502     02  50.000000  148.600006  22.642916   \n",
       "24372     857309  Annadanagar  6503     01  43.799999  160.500000  17.002940   \n",
       "24373     857309  Annadanagar  6503     02  36.400002  139.600006  18.678007   \n",
       "\n",
       "      if_not_measured_why member_id hhm_sex  ... nbr_female_teenager_10_20  \\\n",
       "0           have measured    000101    male  ...                         0   \n",
       "1           have measured    000102  female  ...                         0   \n",
       "2           have measured    000103    male  ...                         0   \n",
       "3           have measured    000104    male  ...                         0   \n",
       "4           have measured    000201    male  ...                         0   \n",
       "...                   ...       ...     ...  ...                       ...   \n",
       "24369       have measured    650103  female  ...                         0   \n",
       "24370       have measured    650201    male  ...                         0   \n",
       "24371       have measured    650202  female  ...                         0   \n",
       "24372       have measured    650301    male  ...                         0   \n",
       "24373       have measured    650302  female  ...                         0   \n",
       "\n",
       "       nbr_female_adults_20_65 nbr_female_elderly_65_over  \\\n",
       "0                            1                          0   \n",
       "1                            1                          0   \n",
       "2                            1                          0   \n",
       "3                            1                          0   \n",
       "4                            1                          0   \n",
       "...                        ...                        ...   \n",
       "24369                        1                          0   \n",
       "24370                        1                          0   \n",
       "24371                        1                          0   \n",
       "24372                        1                          0   \n",
       "24373                        1                          0   \n",
       "\n",
       "                   sample_type  survey_round code adm affected_upazila_all  \\\n",
       "0               FTF additional             1      246                    0   \n",
       "1               FTF additional             1      246                    0   \n",
       "2               FTF additional             1      246                    0   \n",
       "3               FTF additional             1      246                    0   \n",
       "4               FTF additional             1      246                    0   \n",
       "...                        ...           ...      ...                  ...   \n",
       "24369  National Representative             1      420                    0   \n",
       "24370  National Representative             1      420                    0   \n",
       "24371  National Representative             1      420                    0   \n",
       "24372  National Representative             1      420                    0   \n",
       "24373  National Representative             1      420                    0   \n",
       "\n",
       "      affected_komen affected_roanu affected_mora  \n",
       "0                  0              0             0  \n",
       "1                  0              0             0  \n",
       "2                  0              0             0  \n",
       "3                  0              0             0  \n",
       "4                  0              0             0  \n",
       "...              ...            ...           ...  \n",
       "24369              0              0             0  \n",
       "24370              0              0             0  \n",
       "24371              0              0             0  \n",
       "24372              0              0             0  \n",
       "24373              0              0             0  \n",
       "\n",
       "[24374 rows x 38 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_global.groupby([\"survey_round\"], observed=False).size())\n",
    "\n",
    "df_r1_anthr1[lst_intvar]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing to obtain exposure information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Importing other shapefiles\n",
    "\n",
    "# crosswalk\n",
    "\n",
    "crosswalk = pd.read_excel(os.getcwd() + '\\\\input\\\\crosswalk\\\\crosswalk_union.xlsx', usecols=\"A:G\", dtype={'union_name':'str','union_code_r1':'str','union_code_r2':'str','union_code_r3':'str',\n",
    "                                                                                        'code_union_adm':'str','union_name_adm':'str','surveyed':'str'}) \n",
    "\n",
    "\n",
    "# Importing the Bangladesh raw map: Reading a shapefile containing administrative boundaries of Bangladesh\n",
    "bgd_adm = gpd.read_file(os.getcwd() + '\\\\input\\\\shapefile_data\\\\shapefile_zip\\\\gadm41_BGD_shp\\\\gadm41_BGD_4.shp')\n",
    "\n",
    "\n",
    "## Raining data (4 days before and after landfall)\n",
    "raining_dta_komen = pd.read_stata(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\BGD4_mswep_2010_2019.dta', \n",
    "                        columns=[\"GID_4\",\"M2015205\",\"M2015206\",\"M2015207\",\"M2015208\",\"M2015209\",\"M2015210\",\"M2015211\",\"M2015212\",\"M2015213\"])\n",
    "\n",
    "raining_dta_roanu = pd.read_stata(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\BGD4_mswep_2010_2019.dta', \n",
    "                        columns=[\"GID_4\",\"M2016137\",\"M2016138\",\"M2016139\",\"M2016140\",\"M2016141\",\"M2016142\",\"M2016143\",\"M2016144\",\"M2016145\"])\n",
    "\n",
    "raining_dta_mora = pd.read_stata(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\BGD4_mswep_2010_2019.dta', \n",
    "                        columns=[\"GID_4\",\"M2017145\",\"M2017146\",\"M2017147\",\"M2017148\",\"M2017149\",\"M2017150\",\"M2017151\",\"M2017152\",\"M2017153\"])\n",
    "\n",
    "## NTL data (4 days before and after landfall)\n",
    "ntl_dta_komen = pd.read_stata(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\BGD4_ntl_bm_2013_2019.dta', \n",
    "                        columns=[\"gid_4\",\"n2015_07_25\",\"n2015_07_26\",\"n2015_07_27\",\"n2015_07_28\",\"n2015_07_29\",\"n2015_07_30\",\"n2015_07_31\",\"n2015_08_01\",\"n2015_08_02\"])\n",
    "\n",
    "ntl_dta_roanu = pd.read_stata(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\BGD4_ntl_bm_2013_2019.dta', \n",
    "                        columns=[\"gid_4\",\"n2016_05_17\",\"n2016_05_18\",\"n2016_05_19\",\"n2016_05_20\",\"n2016_05_21\",\"n2016_05_22\",\"n2016_05_23\",\"n2016_05_24\",\"n2016_05_25\"])\n",
    "\n",
    "ntl_dta_mora = pd.read_stata(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\BGD4_ntl_bm_2013_2019.dta', \n",
    "                        columns=[\"gid_4\",\"n2017_05_26\",\"n2017_05_27\",\"n2017_05_28\",\"n2017_05_29\",\"n2017_05_30\",\"n2017_05_31\",\"n2017_06_01\",\"n2017_06_02\",\"n2017_06_03\"])\n",
    "\n",
    "## Pop density data\n",
    "popdens_dta = pd.read_csv(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\bgd4_pop_landscan_2000_2020.csv',sep=\";\", usecols=[\"GID_4\",\"landscan_global_2017\"])\n",
    "\n",
    "## Storm data\n",
    "exposure_time_komen_W_N = DBF(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\stats_stormR\\\\exposure_msw_komen_W_N.dbf', load=True)\n",
    "exposure_time_roanu_W_N = DBF(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\stats_stormR\\\\exposure_msw_roanu_W_N.dbf', load=True)\n",
    "exposure_time_mora_W_N = DBF(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\stats_stormR\\\\exposure_msw_mora_W_N.dbf', load=True)\n",
    "\n",
    "speed_komen_W_N = DBF(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\stats_stormR\\\\msw_komen_W_N.dbf', load=True)\n",
    "speed_roanu_W_N = DBF(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\stats_stormR\\\\msw_roanu_W_N.dbf', load=True)\n",
    "speed_mora_W_N = DBF(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\stats_stormR\\\\msw_mora_W_N.dbf', load=True)\n",
    "\n",
    "# Convert to Dataframe\n",
    "exposure_time_komen_W_N = pd.DataFrame(iter(exposure_time_komen_W_N))\n",
    "exposure_time_roanu_W_N = pd.DataFrame(iter(exposure_time_roanu_W_N))\n",
    "exposure_time_mora_W_N = pd.DataFrame(iter(exposure_time_mora_W_N))\n",
    "\n",
    "speed_komen_W_N = pd.DataFrame(iter(speed_komen_W_N))\n",
    "speed_roanu_W_N = pd.DataFrame(iter(speed_roanu_W_N))\n",
    "speed_mora_W_N = pd.DataFrame(iter(speed_mora_W_N))\n",
    "\n",
    "#Dropping and renaming columns\n",
    "exposure_time_komen_W_N = exposure_time_komen_W_N[[\"GID_4\",\"COUNT\",\"AREA\",\"MIN\",\"MAX\",\"MEAN\"]]\n",
    "exposure_time_roanu_W_N = exposure_time_roanu_W_N[[\"GID_4\",\"COUNT\",\"AREA\",\"MIN\",\"MAX\",\"MEAN\"]]\n",
    "exposure_time_mora_W_N = exposure_time_mora_W_N[[\"GID_4\",\"COUNT\",\"AREA\",\"MIN\",\"MAX\",\"MEAN\"]]\n",
    "\n",
    "speed_komen_W_N = speed_komen_W_N[[\"GID_4\",\"COUNT\",\"AREA\",\"MIN\",\"MAX\",\"MEAN\"]]\n",
    "speed_roanu_W_N = speed_roanu_W_N[[\"GID_4\",\"COUNT\",\"AREA\",\"MIN\",\"MAX\",\"MEAN\"]]\n",
    "speed_mora_W_N = speed_mora_W_N[[\"GID_4\",\"COUNT\",\"AREA\",\"MIN\",\"MAX\",\"MEAN\"]]\n",
    "\n",
    "exposure_time_komen_W_N.rename(columns={\"COUNT\":\"count_time_komen\",\"AREA\":\"area_time_komen\",\"MIN\":\"min_time_komen\",\"MAX\":\"max_time_komen\",\"MEAN\":\"mean_time_komen\"}, inplace=True)\n",
    "exposure_time_roanu_W_N.rename(columns={\"COUNT\":\"count_time_roanu\",\"AREA\":\"area_time_roanu\",\"MIN\":\"min_time_roanu\",\"MAX\":\"max_time_roanu\",\"MEAN\":\"mean_time_roanu\"}, inplace=True)\n",
    "exposure_time_mora_W_N.rename(columns={\"COUNT\":\"count_time_mora\",\"AREA\":\"area_time_mora\",\"MIN\":\"min_time_mora\",\"MAX\":\"max_time_mora\",\"MEAN\":\"mean_time_mora\"}, inplace=True)\n",
    "\n",
    "speed_komen_W_N.rename(columns={\"COUNT\":\"count_speed_komen\",\"AREA\":\"area_speed_komen\",\"MIN\":\"min_speed_komen\",\"MAX\":\"max_speed_komen\",\"MEAN\":\"mean_speed_komen\"}, inplace=True)\n",
    "speed_roanu_W_N.rename(columns={\"COUNT\":\"count_speed_roanu\",\"AREA\":\"area_speed_roanu\",\"MIN\":\"min_speed_roanu\",\"MAX\":\"max_speed_roanu\",\"MEAN\":\"mean_speed_roanu\"}, inplace=True)\n",
    "speed_mora_W_N.rename(columns={\"COUNT\":\"count_speed_mora\",\"AREA\":\"area_speed_mora\",\"MIN\":\"min_speed_mora\",\"MAX\":\"max_speed_mora\",\"MEAN\":\"mean_speed_mora\"}, inplace=True)\n",
    "\n",
    "\n",
    "## Merging with the crosswalk to get surveyed unions\n",
    "\n",
    "bgd_adm =(\n",
    "    crosswalk\n",
    "    \n",
    "    # Komen\n",
    "    .merge(bgd_adm, how='right', left_on=\"code_union_adm\", right_on=\"GID_4\")\n",
    "    .merge(raining_dta_komen, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    .merge(ntl_dta_komen, how='left', left_on=\"GID_4\", right_on=\"gid_4\")\n",
    "    .merge(popdens_dta, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    .merge(exposure_time_komen_W_N, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    .merge(speed_komen_W_N, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "\n",
    "    # Roanu\n",
    "    .merge(raining_dta_roanu, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    .merge(ntl_dta_roanu, how='left', left_on=\"GID_4\", right_on=\"gid_4\")\n",
    "    .merge(exposure_time_roanu_W_N, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    .merge(speed_roanu_W_N, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "\n",
    "    # Mora\n",
    "    .merge(raining_dta_mora, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    .merge(ntl_dta_mora, how='left', left_on=\"GID_4\", right_on=\"gid_4\")\n",
    "    .merge(exposure_time_mora_W_N, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    .merge(speed_mora_W_N, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    )\n",
    "\n",
    "\n",
    "bgd_adm[\"surveyed\"] = bgd_adm[\"surveyed\"].fillna(\"0\").replace({'1':'Surveyed union','0':'Unsurveyed union'}).astype(str)\n",
    "bgd_adm[\"union_code_r1\"] = bgd_adm[\"union_code_r1\"].astype(str)\n",
    "bgd_adm[\"union_name\"] = bgd_adm[\"union_name\"].astype(str)\n",
    "\n",
    "bgd_adm[\"average_rain_komen\"] = bgd_adm[[\"M2015208\",\"M2015209\",\"M2015210\"]].aggregate(\"mean\",axis=\"columns\")\n",
    "bgd_adm[\"average_rain_roanu\"] = bgd_adm[[\"M2016140\",\"M2016141\",\"M2016142\"]].aggregate(\"mean\",axis=\"columns\")\n",
    "bgd_adm[\"average_rain_mora\"] = bgd_adm[[\"M2017148\",\"M2017149\",\"M2017150\"]].aggregate(\"mean\",axis=\"columns\")\n",
    "\n",
    "bgd_adm[\"average_ntl_komen\"] = bgd_adm[[\"n2015_07_28\",\"n2015_07_29\",\"n2015_07_30\"]].aggregate(\"mean\",axis=\"columns\")\n",
    "bgd_adm[\"average_ntl_roanu\"] = bgd_adm[[\"n2016_05_20\",\"n2016_05_21\",\"n2016_05_22\"]].aggregate(\"mean\",axis=\"columns\")\n",
    "bgd_adm[\"average_ntl_mora\"] = bgd_adm[[\"n2017_05_29\",\"n2017_05_30\",\"n2017_05_31\"]].aggregate(\"mean\",axis=\"columns\")\n",
    "\n",
    "\n",
    "bgd_adm = gpd.GeoDataFrame(bgd_adm, geometry='geometry')\n",
    "\n",
    "# Building quantiles for highly exposed and unexposed regions\n",
    "quantiles_high_komen = np.nanquantile(bgd_adm[bgd_adm['mean_speed_komen']>0].mean_speed_komen, [i * 0.25 + 0.25 for i in range(4)]) \n",
    "bgd_adm[\"high_exposed_komen\"] = bgd_adm[\"mean_speed_komen\"].apply(lambda row: \"1\" if row >= int(quantiles_high_komen[2]) else \"0\")\n",
    "bgd_adm[\"exposed_at_least_zero_komen\"] = bgd_adm[\"mean_speed_komen\"].apply(lambda row: \"1\" if row > 0 else \"0\")\n",
    "\n",
    "quantiles_high_roanu = np.nanquantile(bgd_adm[bgd_adm['mean_speed_roanu']>0].mean_speed_roanu, [i * 0.25 + 0.25 for i in range(4)]) \n",
    "bgd_adm[\"high_exposed_roanu\"] = bgd_adm[\"mean_speed_roanu\"].apply(lambda row: \"1\" if row >= int(quantiles_high_roanu[2]) else \"0\")\n",
    "bgd_adm[\"exposed_at_least_zero_roanu\"] = bgd_adm[\"mean_speed_roanu\"].apply(lambda row: \"1\" if row > 0 else \"0\")\n",
    "\n",
    "quantiles_high_mora = np.nanquantile(bgd_adm[bgd_adm['mean_speed_mora']>0].mean_speed_mora, [i * 0.25 + 0.25 for i in range(4)]) \n",
    "bgd_adm[\"high_exposed_mora\"] = bgd_adm[\"mean_speed_mora\"].apply(lambda row: \"1\" if row >= int(quantiles_high_mora[2]) else \"0\")\n",
    "bgd_adm[\"exposed_at_least_zero_mora\"] = bgd_adm[\"mean_speed_mora\"].apply(lambda row: \"1\" if row > 0 else \"0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing already existing object for an efficient usage of the computer memory\n",
    "\n",
    "del raining_dta_komen\n",
    "del ntl_dta_komen\n",
    "del raining_dta_roanu\n",
    "del ntl_dta_roanu\n",
    "del raining_dta_mora\n",
    "del ntl_dta_mora\n",
    "del popdens_dta\n",
    "del exposure_time_komen_W_N\n",
    "del speed_komen_W_N\n",
    "del exposure_time_roanu_W_N\n",
    "del speed_roanu_W_N\n",
    "del exposure_time_mora_W_N\n",
    "del speed_mora_W_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging adm data with survey data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1 data\n",
    "df_r1 = df_r1.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r1_anthr1 = df_r1_anthr1.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r1_anthr2 = df_r1_anthr2.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "# Round 2 data\n",
    "df_r2 = df_r2.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r2_anthr1 = df_r2_anthr1.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r2_anthr2 = df_r2_anthr2.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "\n",
    "# Round 3 data\n",
    "df_r3 = df_r3.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r3_anthr1 = df_r3_anthr1.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r3_anthr2 = df_r3_anthr2.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "\n",
    "# Correct a mistake in the name\n",
    "#df_global_corrected['union_name'] = df_global_corrected['union_name'].replace({\"Bhayna  (Sujanagar)\":\"Bhayna\"})\n",
    "\n",
    "df_global_corrected = df_global_corrected.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1','code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "\n",
    "df_global = df_global.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1','code_union_adm', 'union_name_adm',\n",
    "        'high_exposed_komen', 'exposed_at_least_zero_komen','mean_speed_komen','mean_time_komen', \n",
    "        'high_exposed_roanu', 'exposed_at_least_zero_roanu','mean_speed_roanu','mean_time_roanu',\n",
    "        'high_exposed_mora', 'exposed_at_least_zero_mora','mean_speed_mora','mean_time_mora']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "# bgd adm information cleaning\n",
    "bgd_adm[\"union_name_adm\"] = bgd_adm[\"union_name_adm\"].astype(str)\n",
    "bgd_adm[\"union_name\"] = bgd_adm[\"union_name\"].astype(str)\n",
    "bgd_adm[\"union_code_r1\"] = bgd_adm[\"union_code_r1\"].astype(str)\n",
    "bgd_adm[\"union_code_r2\"] = bgd_adm[\"union_code_r2\"].astype(str)\n",
    "bgd_adm[\"union_code_r3\"] = bgd_adm[\"union_code_r3\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcting union_name before exporting\n",
    "\n",
    "df_global_corrected['union_name'] = df_global_corrected['union_name'].astype(str)\n",
    "#df_global_corrected['union_name_r1'] = df_global_corrected['union_name_r1'].astype(str)\n",
    "\n",
    "\n",
    "df_global['union_name'] = df_global['union_name'].astype(str)\n",
    "#df_global['union_name_r1'] = df_global['union_name_r1'].astype(str)\n",
    "\n",
    "\n",
    "df_global['union_name'] = df_global['union_name'].astype(str)\n",
    "#df_global['union_name_r1'] = df_global['union_name_r1'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Saving the processed dataframes to Excel files\n",
    "\n",
    "## Anthro for > 5 hhm\n",
    "df_r1_anthr1.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r1_overfive.xlsx\", index=False)\n",
    "df_r2_anthr1.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r2_overfive.xlsx\", index=False)\n",
    "df_r3_anthr1.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r3_overfive.xlsx\", index=False)\n",
    "\n",
    "## Anthro for <= 5 hhm\n",
    "df_r1_anthr2.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r1_underfive.xlsx\", index=False)\n",
    "df_r2_anthr2.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r2_underfive.xlsx\", index=False)\n",
    "df_r3_anthr2.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r3_underfive.xlsx\", index=False)\n",
    "\n",
    "## Hh level data  \n",
    "df_r1.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r1_hh.xlsx\", index=False)\n",
    "df_r2.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r2_hh.xlsx\", index=False)\n",
    "df_r3.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r3_hh.xlsx\", index=False)\n",
    "\n",
    "## Concatenated data\n",
    "\n",
    "df_global_corrected.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\df_global_corrected.xlsx\", index=False)\n",
    "df_global.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\df_global.xlsx\", index=False)\n",
    "bgd_adm.to_csv(os.getcwd() + \"\\\\output\\\\data\\\\bgd_data.csv\", sep=\";\", index=False)\n",
    "#bgd_adm.to_file(os.getcwd() + \"\\\\output\\\\data\\\\bgd_data.shp\")\n",
    "bgd_adm.to_file(os.getcwd() + \"\\\\output\\\\data\\\\bgd_data.geojson\", driver=\"GeoJSON\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os \n",
    "import pandas as pd\n",
    "# Importing the Bangladesh raw map: Reading a shapefile containing administrative boundaries of Bangladesh\n",
    "bgd_adm = gpd.read_file(os.getcwd() + '\\\\input\\\\shapefile_data\\\\shapefile_zip\\\\gadm41_BGD_shp\\\\gadm41_BGD_4.shp')\n",
    "raining_dta_komen = pd.read_stata(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\BGD4_mswep_2010_2019.dta', \n",
    "                        columns=[\"GID_4\",\"M2015205\",\"M2015206\",\"M2015207\",\"M2015208\",\"M2015209\",\"M2015210\",\"M2015211\",\"M2015212\",\"M2015213\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release memory using del\n",
    "del crosswalk\n",
    "del df_r1\n",
    "del df_r1_f\n",
    "del df_r1_ftf\n",
    "del df_r1_SE\n",
    "del df_r2\n",
    "del df_r2_f\n",
    "del df_r2_SE\n",
    "del df_r3\n",
    "del df_r3_f\n",
    "del df_r3_SE\n",
    "del df_global\n",
    "del df_global_corrected\n",
    "\n",
    "del df_r1_anthr1\n",
    "del df_r1_anthr2\n",
    "del df_r2_anthr1\n",
    "del df_r2_anthr2\n",
    "del df_r3_anthr1\n",
    "del df_r3_anthr2\n",
    "\n",
    "del nbr_max\n",
    "del label_to_int\n",
    "del lst_1\n",
    "del lst_2\n",
    "del diff\n",
    "del lst_complete\n",
    "del lst_migrate\n",
    "del lst_refused\n",
    "del lst_nthome\n",
    "del lst_hhm_id_r3\n",
    "del lst_intvar\n",
    "del lst_split\n",
    "del test\n",
    "del testview\n",
    "del lst_r3\n",
    "del df_r1_SE_gr\n",
    "del df_r2_SE_gr\n",
    "del df_r3_SE_gr\n",
    "# Only non nan values\n",
    "#df_r1_anthr2=df_r1_anthr2[(df_r1_anthr2.weight_kg.notnull()) & (df_r1_anthr2.height_cm.notnull()) & (df_r1_anthr2.age_days.notnull())]\n",
    "# df_r2_anthr2=df_r2_anthr2[(df_r2_anthr2.weight_kg.notnull()) & (df_r2_anthr2.height_cm.notnull()) & (df_r2_anthr2.age_days.notnull())]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
