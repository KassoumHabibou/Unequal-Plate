{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Explore and visualize the dataset.\n",
    "- Build clean the data set and missing values.\n",
    "- Generate a set of insights from the dataset.\n",
    "\n",
    "### The key question\n",
    "\n",
    "Do natural disasters worsen the gender-based nutritional disparity among children in Bangladesh?\n",
    "\n",
    "### Problem Formulation:\n",
    "\n",
    "We have a regression problem at hand where we will try to run a diff in diff to see if we can find evidence for a gender bais as.\n",
    "\n",
    "### Proposed approach\n",
    "\n",
    "Since it is a regression problem we will first start with the parametric model linear regression with a diff-in-diff approache.\n",
    "\n",
    "### Overall solution design :\n",
    "\n",
    "The potential solution design would look like this:\n",
    "\n",
    "- Checking the data description to get the idea of basic statistics or summary of data.\n",
    "- Univariate analysis to see how data is spread out, getting to know about the outliers.\n",
    "- Bivariate analysis to see how different attributes vary with the dependent variable.\n",
    "- Outlier treatment if needed.\n",
    "- Missing value treatment using appropriate techniques.\n",
    "- Feature engineering - transforming features, creating new features if possible.\n",
    "- Choosing the model evaluation technique - 1) R Squared 2) RMSE can be any other metrics related to regression analysis.\n",
    "- Splitting the data and proceeding with modeling.\n",
    "\n",
    "\n",
    "### Measures of success :\n",
    "\n",
    "R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries and overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3277542"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the basic libraries we will require for the project\n",
    "\n",
    "# Import libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# Import libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Slightly advanced library for data visualization            \n",
    "import seaborn as sns      \n",
    "\n",
    "# Import necessary modules\n",
    "import geopandas as gpd\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "\n",
    "# import module for geoencoding\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# add sleep time\n",
    "from time import sleep\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up the color sheme:\n",
    "import mapclassify as mc\n",
    "\n",
    "# to compute zscores: https://pypi.org/project/cgmzscore/\n",
    "# Resource R: https://rdrr.io/github/WorldHealthOrganization/anthroplus/man/anthroplus_zscores.html\n",
    "#from cgmzscore.src.main import z_score_lhfa\n",
    "#from cgmzscore.src.main import z_score_wfa\n",
    "#import ast\n",
    "#https://github.com/ewheeler/pygrowup\n",
    "#from pygrowup import Observation\n",
    "#from decimal import Decimal\n",
    "from dbfread import DBF\n",
    "import datetime\n",
    "# Release memory using gc : The gc module to manually trigger garbage collection. \n",
    "# Garbage collection is the process of freeing memory that is no longer being used by the program. \n",
    "# By manually triggering garbage collection, you can release memory that is no longer needed.\n",
    "import gc\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def return_non_na(curr_data,col):\n",
    "#     return curr_data[col].replace(['HWHAZWHO', 'HWWAZWHO', 'HWWHZWHO','HWBMIZWHO','Missing'], np.NaN, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def reverse_geocode(geolocator, latlon, sleep_sec):\n",
    "    \"\"\"\n",
    "    This function attempts to perform reverse geocoding using the provided geolocator\n",
    "    and latitude-longitude coordinates. It handles potential errors and retries on timeouts.\n",
    "\n",
    "    Args:\n",
    "        geolocator (object): A geocoding library object used for reverse geocoding.\n",
    "        latlon (str): A string in the format \"latitude,longitude\" representing the location.\n",
    "        sleep_sec (int): The maximum number of seconds to sleep between retries on timeouts.\n",
    "\n",
    "    Returns:\n",
    "        object: The result of the reverse geocoding request (may vary depending on the geocoder library).\n",
    "                - On success, returns the geocoded information.\n",
    "                - On timeout, retries up to sleep_sec seconds and returns the result.\n",
    "                - On service error or other exceptions, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Attempt reverse geocoding using the geolocator in English language\n",
    "        return geolocator.reverse(latlon, language='en')\n",
    "    except GeocoderTimedOut:\n",
    "        # Handle timeout error\n",
    "        logging.info('TIMED OUT: GeocoderTimedOut: Retrying...')\n",
    "        # Introduce a random sleep between 1 and sleep_sec seconds to avoid overwhelming the service\n",
    "        sleep(randint(1 * 100, sleep_sec * 100) / 100)\n",
    "        # Retry the reverse geocoding with the same parameters\n",
    "        return reverse_geocode(geolocator, latlon, sleep_sec)\n",
    "    except GeocoderServiceError as e:\n",
    "        # Handle service error (e.g., connection refused)\n",
    "        logging.info('CONNECTION REFUSED: GeocoderServiceError encountered.')\n",
    "        logging.error(e)  # Log the detailed error for debugging\n",
    "        return None  # Indicate failure\n",
    "    except Exception as e:\n",
    "        # Handle unexpected exceptions\n",
    "        logging.info('ERROR: Terminating due to exception {}'.format(e))\n",
    "        return None  # Indicate failure\n",
    "    \n",
    "\n",
    "def get_local_information(curr_data, sleep_sec):\n",
    "    \"\"\"\n",
    "    This function enriches a DataFrame with local address information based on latitude and longitude data.\n",
    "\n",
    "    Args:\n",
    "        curr_data (pandas.DataFrame): A DataFrame with columns 'LATNUM' and 'LONGNUM' containing latitude and longitude values.\n",
    "        sleep_sec (int): Number of seconds to sleep between retries for error handling.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with additional columns for city, village, county, state_district, state, and country.\n",
    "        \"\"\"\n",
    "\n",
    "    # Initialize the Nominatim geocoder with a user agent string (important for API usage guidelines)\n",
    "    user_agent = 'bibouPaultest_{}'.format(randint(10000, 99999)) + '@gmail.com'\n",
    "    geolocator = Nominatim(user_agent=user_agent)\n",
    "\n",
    "    # Iterate through each row of the DataFrame\n",
    "    for curr_line in range(curr_data.index.max() + 1):\n",
    "\n",
    "        # Extract latitude and longitude values for the current row\n",
    "        Latitude = str(curr_data.loc[curr_line, \"LATNUM\"])\n",
    "        Longitude = str(curr_data.loc[curr_line, \"LONGNUM\"])\n",
    "\n",
    "        # Perform reverse geocoding to get address information\n",
    "        location = reverse_geocode(geolocator, Latitude + \",\" + Longitude, sleep_sec)  # Uses a custom function for retries\n",
    "\n",
    "        # Extract relevant address components from the geocoding result\n",
    "        if location:\n",
    "            address = location.raw['address']\n",
    "            city = address.get('city')\n",
    "            village = address.get('village')\n",
    "            county = address.get('county')\n",
    "            state_district = address.get('state_district')\n",
    "            state = address.get('state')\n",
    "            country = address.get('country')\n",
    "\n",
    "        # Update the DataFrame with the extracted address information\n",
    "            curr_data.loc[curr_line, 'city'] = city\n",
    "            curr_data.loc[curr_line, 'village'] = village\n",
    "            curr_data.loc[curr_line, 'county'] = county\n",
    "            curr_data.loc[curr_line, 'state_district'] = state_district\n",
    "            curr_data.loc[curr_line, 'state'] = state\n",
    "            curr_data.loc[curr_line, 'country'] = country\n",
    "\n",
    "        else:\n",
    "            # Update the DataFrame with the extracted address information\n",
    "            curr_data.loc[curr_line, 'city'] = np.NAN\n",
    "            curr_data.loc[curr_line, 'village'] = np.NAN\n",
    "            curr_data.loc[curr_line, 'county'] = np.NAN\n",
    "            curr_data.loc[curr_line, 'state_district'] = np.NAN\n",
    "            curr_data.loc[curr_line, 'state'] = np.NAN\n",
    "            curr_data.loc[curr_line, 'country'] = np.NAN\n",
    "    \n",
    "    # Return the DataFrame with the added address information\n",
    "    return curr_data\n",
    "\n",
    "\n",
    "def correct_id(curr_row, nbr_max):\n",
    "    \"\"\"\n",
    "    This function ensures that the input string `curr_row` is formatted to a specified length `nbr_max` \n",
    "    by adding leading zeros if necessary.\n",
    "\n",
    "    Parameters:\n",
    "    curr_row (str): The current row ID that needs to be corrected.\n",
    "    nbr_max (int): The maximum length that the corrected ID should have.\n",
    "    \"\"\"\n",
    "    # Check if the length of the current row ID is less than the maximum allowed length\n",
    "    if len(curr_row) < nbr_max:\n",
    "        # Calculate the number of leading zeros needed to reach the maximum length\n",
    "        nbr_resid = nbr_max - len(curr_row) \n",
    "        # Add the leading zeros to the beginning of the current row ID and return as a string\n",
    "        if curr_row.count('.') == 0:\n",
    "            return str(nbr_resid * '0' + curr_row)\n",
    "        else:\n",
    "            # Add extra leading zeros if the current row ID contains a period (.)\n",
    "            return str((nbr_resid + 2) * '0' + curr_row)\n",
    "    # Check if the length of the current row ID is exactly equal to the maximum allowed length\n",
    "    elif len(curr_row) == nbr_max:\n",
    "        # Return the current row ID as a string, adding two leading zeros if it contains a period\n",
    "        if curr_row.count('.') == 0:\n",
    "            return str(curr_row)\n",
    "        else:\n",
    "            return str(2 * '0' + curr_row)\n",
    "    # Case where the length of the current row ID is greater than the maximum allowed length\n",
    "    else: \n",
    "        # Check if the period is in the third position\n",
    "        if curr_row.find('.') == 2: \n",
    "            return str('00' + curr_row)\n",
    "        # Check if the period is in the fourth position\n",
    "        elif curr_row.find('.') == 3: \n",
    "            return str('0' + curr_row)\n",
    "        # Return the current row ID as is\n",
    "        else: \n",
    "            return str(curr_row)\n",
    "        \n",
    "def formatNumber(num):\n",
    "    # Check if the number is an integer (i.e., no fractional part)\n",
    "    if num % 1 == 0:\n",
    "        # If it is an integer, convert it to an integer type and return it\n",
    "        return int(num)\n",
    "    else:\n",
    "        # If it is not an integer, return the number as it is\n",
    "        return num\n",
    "    \n",
    "def vertical_mean_line_survived(x, **kwargs):\n",
    "    \"\"\"\n",
    "    This function draws a vertical line on a plot at the mean value of the input data 'x'.\n",
    "    The style and color of the line are determined by additional keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): The data for which the mean will be calculated.\n",
    "    **kwargs: Additional keyword arguments to customize the line.\n",
    "        'label' (str): A label that can be 'male' or 'female' to determine the line style.\n",
    "        'color' (str): The color of the line, default is green ('g').\n",
    "\n",
    "    Example usage:\n",
    "    vertical_mean_line_survived(data, label='male', color='b')\n",
    "    \"\"\"\n",
    "\n",
    "    # Define line styles for male and female\n",
    "    ls = {\"male\": \"-\", \"female\": \"-\"}\n",
    "    \n",
    "    # Draw a vertical line at the mean of 'x'\n",
    "    plt.axvline(x.mean(), \n",
    "                linestyle=ls[kwargs.get(\"label\")],  # Set line style based on 'label' keyword argument\n",
    "                color=kwargs.get(\"color\", \"g\"))     # Set line color based on 'color' keyword argument, default is green\n",
    "\n",
    "# Example of how the function might be called:\n",
    "# vertical_mean_line_survived(data, label='male', color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1 datasets\n",
    "\n",
    "## HH identification data: Reading household identification data from a Stata file\n",
    "df_r1 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\001_mod_a_male.dta') \n",
    "df_r1_f = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\002_mod_a_female.dta') \n",
    "\n",
    "## FTF: Feed the Futur (FTF) census data from a Stata file\n",
    "df_r1_ftf = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\001_census_ftf.dta') \n",
    "\n",
    "\n",
    "### Sociaux economics characteristics\n",
    "df_r1_SE = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\003_mod_b1_male.dta') \n",
    "\n",
    "## Anthro data: Reading anthropometric data from Stata files\n",
    "### 1 for all household members\n",
    "df_r1_anthr1 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\045_mod_w1_female.dta') \n",
    "\n",
    "### 2 for under five children\n",
    "df_r1_anthr2 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 1 (2011-2012)\\\\046_mod_w2_female.dta') \n",
    "\n",
    "# Importing the Bangladesh raw map: Reading a shapefile containing administrative boundaries of Bangladesh\n",
    "#bgd_adm = gpd.read_file(os.getcwd() + '\\\\input\\\\shapefile_data\\\\shapefile_zip\\\\BGD_adm\\\\BGD_adm3.shp')\n",
    "\n",
    "\n",
    "# Crosswalk to merge sociaux economic data to geographic informations\n",
    "crosswalk = pd.read_excel(os.getcwd() + '\\\\input\\\\cross_walk_data_final.xlsx', dtype={'upazila_code':'category','code adm':'category','affected_upazila':'category'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1 = df_r1.loc[:, ['div', 'dcode', 'uzcode', 'uncode','vcode_n', 'div_name', 'District', \n",
    "                    'District_Name', 'Upazila', 'Upazila_Name', 'Union', 'Union_Name','a01', \n",
    "                    'a02', 'a13', 'a15','a16_dd','a16_mm','a16_yy','Sample_type']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1 = df_r1.rename(columns={'div':'division_code', 'District':'district_code', 'District_Name':'district_name','Upazila':'upazila_code','Upazila_Name':'upazila_name',  'Union':'union_code','Union_Name':'union_name','vcode_n':'village_code','a01': \"hh_id\",\n",
    "                    \"a02\": \"census_nbr\", \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\",'a16_dd':'first_v_day','a16_mm':'first_v_month','a16_yy':'first_v_year', 'Sample_type':'sample_type'})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "### Identifiant\n",
    "df_r1['division_code'] = df_r1['division_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1['district_code'] = df_r1['district_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1['upazila_code'] = df_r1['upazila_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1['village_code'] = df_r1['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1['hh_id'] = df_r1['hh_id'].astype(\"int64\").astype(\"string\")\n",
    "\n",
    "df_r1['union_code'] = df_r1['union_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1['census_nbr'] = df_r1['census_nbr'].astype(\"category\")\n",
    "df_r1['sample_type'] = df_r1['sample_type'].astype(\"category\")\n",
    "\n",
    "\n",
    "\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type!='FTF additional')]\n",
    "#df_r1.sample_type.value_counts()\n",
    "\n",
    "df_r1['first_v_day'] = df_r1['first_v_day'].astype(\"int64\")\n",
    "df_r1['first_v_month'] = df_r1['first_v_month'].astype(\"int64\")\n",
    "df_r1['first_v_year'] = df_r1['first_v_year'].astype(\"int64\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type=='FTF Original') | (df_r1.sample_type=='National Representative')]\n",
    "\n",
    "# Creating the interview date\n",
    "df_r1['date_int'] = pd.to_datetime(dict(year=df_r1.first_v_year, month=df_r1.first_v_month, day=df_r1.first_v_day))\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r1.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r1[\"hh_id\"] = df_r1.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r1.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r1[\"upazila_code\"] = df_r1.upazila_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r1.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r1[\"village_code\"] = df_r1.village_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r1.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union_code' column with the corrected codes\n",
    "df_r1[\"union_code\"] = df_r1.union_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh\n",
    "df_r1[\"hh_id_cmplt\"] = df_r1.apply(lambda row : str(row.loc[\"union_code\"]) + str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r1[\"survey_round\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1_f = df_r1_f.loc[:, ['a01', 'div', 'dcode', 'uzcode', 'uncode','vcode_n', 'div_name', 'district', \n",
    "                    'district_name', 'upazila', 'upazila_name', 'union', 'union_name',\n",
    "                    'a02', 'a13', 'a15','a16_dd','a16_mm','a16_yy']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1_f = df_r1_f.rename(columns={'a01': \"hh_id\",'div':'division', \"a02\": \"census_nbr\", \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\", 'district':'district_code', 'district_name':'district_name',\n",
    "                    'upazila':'upazila_code','vcode_n':'village_code', 'upazila_name':'upazila_name', 'union':'union_code', 'union_name':'union_name','a16_dd':'first_v_day','a16_mm':'first_v_month','a16_yy':'first_v_year'})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r1_f['hh_id'] = df_r1_f['hh_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_f['division'] = df_r1_f['division'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_f['village_code'] = df_r1_f['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_f['census_nbr'] = df_r1_f['census_nbr'].astype(\"category\")\n",
    "#df_r1_f['sample_type'] = df_r1_f['sample_type'].astype(\"category\")\n",
    "\n",
    "df_r1_f['first_v_day'] = df_r1_f['first_v_day'].astype(\"int64\")\n",
    "df_r1_f['first_v_month'] = df_r1_f['first_v_month'].astype(\"int64\")\n",
    "df_r1_f['first_v_year'] = df_r1_f['first_v_year'].astype(\"int64\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1_f = df_r1_f.loc[(df_r1_f.sample_type=='ftf original') | (df_r1_f.sample_type=='national representative')]\n",
    "# creating the interview date\n",
    "#df_r1_f['date_int'] = pd.to_datetime(dict(year=df_r1_f.first_v_year, month=df_r1_f.first_v_month, day=df_r1_f.first_v_day))\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r1_f.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r1_f[\"hh_id\"] = df_r1_f.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r1_f.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r1_f[\"upazila_code\"] = df_r1_f.upazila_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r1_f.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r1_f[\"village_code\"] = df_r1_f.village_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r1_f.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union' column with the corrected codes\n",
    "df_r1_f[\"union_code\"] = df_r1_f.union_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh\n",
    "df_r1_f[\"hh_id_cmplt\"] = df_r1_f.apply(lambda row : str(row.loc[\"union_code\"]) + str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r1_f[\"survey_round\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1_ftf = df_r1_ftf.loc[:, ['a01', 'division', 'dcode', 'uzcode', 'uncode', 'vcode', 'district', 'village','distri_a', 'upazila', 'upazil_a', 'union', 'union_na','a1_01', 'a1_02', 'a1_03',\"a1_04\",\"a1_05\",\"a1_06\"]]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1_ftf = df_r1_ftf.rename(columns={'a01': \"hh_id\", 'vcode':'village_name', 'village':'village_code', 'division':'div_name', 'district':'district_code','distri_a':'district_name',\n",
    "                    'upazila':'upazila_code', 'upazil_a':'upazila_name','union':'union_code', 'union_na':'union_name', \"a1_01\": \"sexe_hhm\", \"a1_02\": \"age_hhm\",\n",
    "                    \"a1_03\":\"relation_head_hh\",\"a1_04\":\"marital_status_hhm\",\"a1_05\":\"literacy_hhm\",\"a1_06\":\"education_high\"})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r1_ftf['hh_id'] = df_r1_ftf['hh_id'].astype(\"int64\", errors='ignore').astype(\"string\")\n",
    "df_r1_ftf['div_name'] = df_r1_ftf['div_name'].astype(\"string\")\n",
    "df_r1_ftf['village_code'] = df_r1_ftf['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_ftf['sexe_hhm'] = df_r1_ftf['sexe_hhm'].astype(\"category\")\n",
    "df_r1_ftf['age_hhm'] = df_r1_ftf['age_hhm'].astype(\"int64\")\n",
    "df_r1_ftf['relation_head_hh'] = df_r1_ftf['relation_head_hh'].astype(\"category\")\n",
    "df_r1_ftf['marital_status_hhm'] = df_r1_ftf['marital_status_hhm'].astype(\"category\")\n",
    "df_r1_ftf['literacy_hhm'] = df_r1_ftf['literacy_hhm'].astype(\"category\")\n",
    "df_r1_ftf['education_high'] = df_r1_ftf['education_high'].astype(\"category\")\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "#nbr_max = df_r1_ftf.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "###df_r1_ftf[\"hh_id\"] = df_r1_ftf.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "###nbr_max = df_r1_ftf.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "###df_r1_ftf[\"village_code\"] = df_r1_ftf.village_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r1_ftf[\"survey_round\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\1704833532.py:41: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df_r1_anthr1['if_not_measured_why'] = df_r1_anthr1['if_not_measured_why'].replace({\"have measured\":\"have measured\",\"absent\":\"absent\",\"sick\":\"sick\",\"others\":\"other\",\"refused to  give measurement\":\"refused\"})\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1_anthr1 = df_r1_anthr1.loc[:, ['a01', 'mid', 'w1_01', 'w1_02', 'w1_03','w1_04', 'w1_05']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1_anthr1 = df_r1_anthr1.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"w1_01\": \"are_you_pregnant\", \"w1_02\": \"are_you_lactating\", \n",
    "                            \"w1_03\": \"weight_kg\", 'w1_04':'height_cm', 'w1_05':'if_not_measured_why'})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r1_anthr1['hh_id'] = df_r1_anthr1['hh_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_anthr1['hhm_id'] = df_r1_anthr1['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_anthr1['are_you_pregnant'] = df_r1_anthr1['are_you_pregnant'].astype(\"category\")\n",
    "df_r1_anthr1['are_you_lactating'] = df_r1_anthr1['are_you_lactating'].astype(\"category\")\n",
    "df_r1_anthr1['weight_kg'] = df_r1_anthr1['weight_kg'].astype(\"float64\")\n",
    "df_r1_anthr1['height_cm'] = df_r1_anthr1['height_cm'].astype(\"float64\")\n",
    "# computing the BMI\n",
    "df_r1_anthr1['bmi'] = df_r1_anthr1['weight_kg']*10000/(df_r1_anthr1['height_cm']**2)\n",
    "df_r1_anthr1['if_not_measured_why'] = df_r1_anthr1['if_not_measured_why'].astype(\"category\")\n",
    "#df_r1_anthr1['sample_type'] = df_r1_anthr1['sample_type'].astype(\"category\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1_anthr1 = df_r1_anthr1.loc[(df_r1_anthr1.sample_type=='ftf original') | (df_r1_anthr1.sample_type=='national representative')].drop(columns=['sample_type'], axis=1)\n",
    "df_r1_anthr1.reset_index(inplace=True)\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r1_anthr1.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r1_anthr1[\"hh_id\"] = df_r1_anthr1.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r1_anthr1.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r1_anthr1[\"hhm_id\"] = df_r1_anthr1.hhm_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r1_anthr1[\"member_id\"] = df_r1_anthr1.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "# Correcting the no measured output\n",
    "df_r1_anthr1['if_not_measured_why'] = df_r1_anthr1['if_not_measured_why'].replace({\"have measured\":\"have measured\",\"absent\":\"absent\",\"sick\":\"sick\",\"others\":\"other\",\"refused to  give measurement\":\"refused\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2932269341.py:57: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df_r1_anthr2['if_not_measured_why'] = df_r1_anthr2['if_not_measured_why'].replace({\"have measured\":\"have measured\",\"absent\":\"absent\",\"sick\":\"sick\",\"others\":\"other\",\"refused to  give measurement\":\"refused\"})\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1_anthr2 = df_r1_anthr2.loc[:, ['a01', 'mid', 'w2_01', 'w2_02', 'w2_03', 'w2_04', 'w2_05','w2_07','w2_08', 'w2_10']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1_anthr2 = df_r1_anthr2.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"w2_01\": \"mother_id\", \"w2_02\": \"childbirth_day\", 'w2_03':'if_not_week_month', 'w2_04':'month_birth', 'w2_05':'year_birth',\n",
    "                            \"w2_07\": \"weight_kg\", 'w2_08':'height_cm', 'w2_10':'if_not_measured_why'})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r1_anthr2['hh_id'] = df_r1_anthr2['hh_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_anthr2['hhm_id'] = df_r1_anthr2['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_anthr2['mother_id'] = df_r1_anthr2['mother_id'].astype(\"string\")\n",
    "df_r1_anthr2['childbirth_day'] = df_r1_anthr2['childbirth_day'].astype(\"int64\", errors='ignore')\n",
    "df_r1_anthr2['month_birth'] = df_r1_anthr2['month_birth'].astype(\"int64\", errors='ignore')\n",
    "df_r1_anthr2['year_birth'] = df_r1_anthr2['year_birth'].astype(\"int64\", errors='ignore')\n",
    "df_r1_anthr2['if_not_week_month'] = df_r1_anthr2['if_not_week_month'].astype(\"category\")\n",
    "df_r1_anthr2['weight_kg'] = df_r1_anthr2['weight_kg'].astype(\"float64\")\n",
    "df_r1_anthr2['height_cm'] = df_r1_anthr2['height_cm'].astype(\"float64\")\n",
    "# computing the BMI\n",
    "df_r1_anthr2['bmi'] = df_r1_anthr2['weight_kg']*10000/(df_r1_anthr2['height_cm']**2)\n",
    "df_r1_anthr2['if_not_measured_why'] = df_r1_anthr2['if_not_measured_why'].astype(\"category\")\n",
    "#df_r1_anthr2['sample_type'] = df_r1_anthr2['sample_type'].astype(\"category\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1_anthr2 = df_r1_anthr2.loc[(df_r1_anthr2.sample_type=='ftf original') | (df_r1_anthr2.sample_type=='national representative')].drop(columns=['sample_type'], axis=1)\n",
    "df_r1_anthr2.reset_index(inplace=True)\n",
    "\n",
    "# Fill nan in child birth day \n",
    "df_r1_anthr2.childbirth_day = df_r1_anthr2.childbirth_day.fillna(1)\n",
    "\n",
    "# some correction\n",
    "## there is a child who have as day 31 and month september, I correct it to 30 to make sure that the information is well defined\n",
    "df_r1_anthr2.loc[(df_r1_anthr2.childbirth_day==31) & (df_r1_anthr2.month_birth==9),\"childbirth_day\"]=30\n",
    "## there is only 28 days in february 2007\n",
    "df_r1_anthr2.loc[(df_r1_anthr2.childbirth_day==29) & (df_r1_anthr2.month_birth==2) & (df_r1_anthr2.year_birth==2007),\"childbirth_day\"]=28\n",
    "\n",
    "## A child have 0.5 as month of birth, this code replace it by 1\n",
    "df_r1_anthr2.loc[(df_r1_anthr2.month_birth==0.5),\"month_birth\"]=1\n",
    "df_r1_anthr2['date_birth'] = pd.to_datetime(dict(year=df_r1_anthr2.year_birth, month=df_r1_anthr2.month_birth, day=df_r1_anthr2.childbirth_day))\n",
    "\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r1_anthr2.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r1_anthr2[\"hh_id\"] = df_r1_anthr2.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r1_anthr2.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r1_anthr2[\"hhm_id\"] = df_r1_anthr2.hhm_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r1_anthr2[\"member_id\"] = df_r1_anthr2.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "# Correcting the no measured output\n",
    "df_r1_anthr2['if_not_measured_why'] = df_r1_anthr2['if_not_measured_why'].replace({\"have measured\":\"have measured\",\"absent\":\"absent\",\"sick\":\"sick\",\"others\":\"other\",\"refused to  give measurement\":\"refused\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sociaux economics caracteristiques of the household members\n",
    "\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r1_SE = df_r1_SE.loc[:, ['a01', 'mid', 'b1_01', 'b1_02', 'b1_03', 'b1_04', 'b1_07','b1_08','b1_09']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r1_SE = df_r1_SE.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"b1_01\": \"hhm_sex\", \"b1_02\": \"hhm_age\", 'b1_03':'relation_hhh', 'b1_04':'marital_status_hhm', 'b1_07':'literacy_hhm',\n",
    "                            \"b1_08\": \"education_high\", 'b1_09':'curr_att_school'})\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r1_SE['hh_id'] = df_r1_SE['hh_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_SE['hhm_id'] = df_r1_SE['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r1_SE['hhm_sex'] = df_r1_SE['hhm_sex'].astype(\"category\")\n",
    "df_r1_SE['hhm_age'] = df_r1_SE['hhm_age'].astype(\"int64\")\n",
    "\n",
    "df_r1_SE['relation_hhh'] = df_r1_SE['relation_hhh'].astype(\"category\")\n",
    "df_r1_SE['marital_status_hhm'] = df_r1_SE['marital_status_hhm'].astype(\"category\")\n",
    "df_r1_SE['literacy_hhm'] = df_r1_SE['literacy_hhm'].astype(\"category\")\n",
    "\n",
    "df_r1_SE['education_high'] = df_r1_SE['education_high'].astype(\"category\")\n",
    "df_r1_SE['curr_att_school'] = df_r1_SE['curr_att_school'].astype(\"category\")\n",
    "#df_r1_SE['sample_type'] = df_r1_SE['sample_type'].astype(\"category\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1_SE = df_r1_SE.loc[(df_r1_SE.sample_type=='ftf original') | (df_r1_SE.sample_type=='national representative')].drop(columns=['sample_type'], axis=1)\n",
    "df_r1_SE.reset_index(inplace=True)\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r1_SE.hh_id.str.len().max()\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r1_SE[\"hh_id\"] = df_r1_SE.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r1_SE.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r1_SE[\"hhm_id\"] = df_r1_SE.hhm_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r1_SE[\"member_id\"] = df_r1_SE.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "df_r1_SE[\"hhm_status\"]=\"Permanent (r1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the crosswalk\n",
    "crosswalk = crosswalk[['upazila_code','code adm','affected_upazila']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1 datasets\n",
    "\n",
    "## HH identification data: Reading household identification data from a Stata file\n",
    "df_r2 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\001_r2_mod_a_male.dta', convert_categoricals=False) \n",
    "df_r2_f = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\002_r2_mod_a_female.dta', convert_categoricals=False) \n",
    "\n",
    "\n",
    "## FTF: Feed the Futur (FTF) census data from a Stata file\n",
    "#df_r2_ftf = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\001_census_ftf.dta') \n",
    "\n",
    "\n",
    "### Sociaux economics characteristics\n",
    "df_r2_SE = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\003_r2_male_mod_b1.dta', convert_categoricals=False) \n",
    "\n",
    "## Anthro data: Reading anthropometric data from Stata files\n",
    "### 1 for all household members\n",
    "df_r2_anthr1 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\059_r2_mod_w1_female.dta') \n",
    "\n",
    "### 2 for under five children\n",
    "df_r2_anthr2 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\060_r2_mod_w2_female.dta') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2354176384.py:13: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2['hh_id'] = df_r2['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r2\n",
    "df_r2 = df_r2.loc[:, ['a01', 'div', 'dcode', 'uzcode', 'uncode','vcode', 'div_name', 'District', \n",
    "                    'District_Name', 'Upazila', 'Upazila_Name', 'Union', 'Union_Name','mouzacode','mouza_name','Village','village_name',\n",
    "                    'a02', 'a13', 'a15','hh_type','flag_a']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r2 = df_r2.rename(columns={'a01': \"hh_id\",'div':'division', \"a02\": \"census_nbr\", \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\", 'District':'district_code', 'District_Name':'district_name',\n",
    "                    'Upazila':'upazila_code', 'Upazila_Name':'upazila_name', 'Union':'union_code', 'Union_Name':'union_name','Village':'village_code','village_name':'village_name', \n",
    "                    'mouzacode':'mauza code','mouza_name':'mauza name','hh_type':'sample_type', 'flag_a':'interview_status'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r2' to correct the 'hh_id\n",
    "df_r2['hh_id'] = df_r2['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r2['hh_id'] = df_r2['hh_id'].astype(\"string\")\n",
    "df_r2['division'] = df_r2['division'].astype(\"int64\").astype(\"string\")\n",
    "df_r2['village_code'] = df_r2['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r2['union_code'] = df_r2['union_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r2['interview_status'] = df_r2['interview_status'].astype(\"int64\").astype(\"string\")\n",
    "df_r2['census_nbr'] = df_r2['census_nbr'].astype(\"category\")\n",
    "df_r2['sample_type'] = df_r2['sample_type'].astype(\"category\")\n",
    "\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r2['interview_status'] = df_r2['interview_status'].replace({'1':'Complete','2':'Partial','3':'Refused','4':'Not at home','5':'Migrated'})\n",
    "df_r2['hh_head_religion'] = df_r2['hh_head_religion'].replace({1:'Muslim',2:'Hindu',3:'Christian',4:'Buddhist',5:'Other(specify)'})\n",
    "df_r2['hh_ethnic_group'] = df_r2['hh_ethnic_group'].replace({1:'Bangali',2:'Bihari',3:'Sawtal',4:'Khasia',5:'Rakhain',6:'Bown',\n",
    "                                                            7:'Chak',8:'Chakma',9:'Khumi',10:'Kheyanf',11:'Lusai/Pankho',12:'Marma',13:'Mru(Murong)',\n",
    "                                                            14:'Tonchonga',15:'Tripura',16:'Bonojogi',17:'Others'})\n",
    "\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type!='FTF additional')]\n",
    "#df_r1.sample_type.value_counts()\n",
    "\n",
    "#df_r2['first_v_day'] = df_r2['first_v_day'].astype(\"float64\")\n",
    "#df_r2['first_v_month'] = df_r2['first_v_month'].astype(\"float64\")\n",
    "#df_r2['first_v_year'] = df_r2['first_v_year'].astype(\"float64\")\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type=='FTF Original') | (df_r1.sample_type=='National Representative')]\n",
    "\n",
    "# creating the interview date\n",
    "#df_r2['date_int'] = pd.to_datetime(dict(year=df_r2.first_v_year, month=df_r2.first_v_month, day=df_r2.first_v_day))\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r2.hh_id.str.len().max() - 2\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r2[\"hh_id\"] = df_r2.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r2.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r2[\"village_code\"] = df_r2.village_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r2.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r2[\"upazila_code\"] = df_r2.upazila_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r2.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union' column with the corrected codes\n",
    "df_r2[\"union_code\"] = df_r2.union_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Household initial id to get the parent household\n",
    "df_r2[\"hh_id_parent\"] = df_r2.hh_id.apply(lambda row: row[:row.find('.')] if row.find('.')!=-1 else row)\n",
    "## hh_split capture hh who didn't split at least once during the study period (1 if don't split and 0 otherwise)\n",
    "df_r2[\"hh_split\"] = df_r2.apply(lambda row: '1' if row[\"hh_id_parent\"] != row[\"hh_id\"] else '0', axis=1).astype('category')\n",
    "\n",
    "## hh_split_bis capture infant hh (1 if the hh is an infant hh and 0 otherwise)\n",
    "df_r2[\"hh_split_bis\"] = df_r2.apply(lambda row: '1' if (row[\"hh_id_parent\"]+\".1\" != row[\"hh_id\"]) & (row[\"hh_id_parent\"] != row[\"hh_id\"]) else '0', axis=1).astype('category')\n",
    "\n",
    "\n",
    "# Build an unique IDs for each hh\n",
    "df_r2[\"hh_id_cmplt\"] = df_r2.apply(lambda row : str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r2[\"survey_round\"] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2577530553.py:12: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_f['hh_id'] = df_r2_f['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2577530553.py:31: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_f[\"hh_id\"] = df_r2_f.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2577530553.py:36: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_f[\"upazila_code\"] = df_r2_f.upazila_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2577530553.py:41: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_f[\"village_code\"] = df_r2_f.village_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r2_f = df_r2_f.loc[:, ['a01', 'div', 'dcode', 'uzcode', 'uncode','Village', 'div_name', 'District', \n",
    "                    'District_Name', 'Upazila', 'Upazila_Name', 'Union', 'Union_Name',\n",
    "                    'a02', 'a13', 'a15','hh_type', 'flag_fem_a']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r2_f = df_r2_f.rename(columns={'a01': \"hh_id\",'div':'division', \"a02\": \"census_nbr\", \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\", 'District':'district_code', 'District_Name':'district_name',\n",
    "                    'Upazila':'upazila_code','Village':'village_code', 'Upazila_Name':'upazila_name', 'Union':'union_code', 'Union_Name':'union_name','hh_type':'sample_type','flag_fem_a':'interview_status'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r2_f' to correct the 'hh_id\n",
    "df_r2_f['hh_id'] = df_r2_f['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r2_f['hh_id'] = df_r2_f['hh_id'].astype(\"string\")\n",
    "df_r2_f['division'] = df_r2_f['division'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_f['village_code'] = df_r2_f['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_f['upazila_code'] = df_r2_f['upazila_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_f['union_code'] = df_r2_f['union_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_f['census_nbr'] = df_r2_f['census_nbr'].astype(\"category\")\n",
    "df_r2_f['sample_type'] = df_r2_f['sample_type'].astype(\"category\")\n",
    "#df_r2_f['sample_type'] = df_r2_f['sample_type'].astype(\"category\")\n",
    "\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r2_f['interview_status'] = df_r2_f['interview_status'].replace({'1':'Complete','2':'Partial','3':'Refused','4':'Not at home','5':'Migrated'})\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r2_f.hh_id.str.len().max() - 2\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r2_f[\"hh_id\"] = df_r2_f.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r2_f.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r2_f[\"upazila_code\"] = df_r2_f.upazila_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r2_f.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r2_f[\"village_code\"] = df_r2_f.village_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r2_f.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union' column with the corrected codes\n",
    "df_r2_f[\"union_code\"] = df_r2_f.union_code.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Household initial id to get the parent household\n",
    "df_r2_f[\"hh_id_parent\"] = df_r2_f.hh_id.apply(lambda row: row[:row.find('.')] if row.find('.')!=-1 else row)\n",
    "df_r2_f[\"hh_split\"] = df_r2_f.apply(lambda row: '1' if row[\"hh_id_parent\"] != row[\"hh_id\"] else '0', axis=1).astype('category')\n",
    "# Build an unique IDs for each hh\n",
    "df_r2_f[\"hh_id_cmplt\"] = df_r2_f.apply(lambda row : str(row.loc[\"union_code\"]) + str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r2_f[\"survey_round\"] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\3228784964.py:10: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_anthr1['hh_id'] = df_r2_anthr1['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\3228784964.py:36: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_anthr1[\"hh_id\"] = df_r2_anthr1.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\3228784964.py:40: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_anthr1[\"hhm_id\"] = df_r2_anthr1.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r2_anthr1 = df_r2_anthr1.loc[:, ['a01', 'mid','w1_01', 'w1_02', 'w1_03','w1_04', 'w1_05']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r2_anthr1 = df_r2_anthr1.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"w1_01\": \"are_you_pregnant\", \"w1_02\": \"are_you_lactating\",\n",
    "                            \"w1_03\": \"weight_kg\", 'w1_04':'height_cm', 'w1_05':'if_not_measured_why'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r2_anthr1' to correct the 'hh_id\n",
    "df_r2_anthr1['hh_id'] = df_r2_anthr1['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r2_anthr1['hh_id'] = df_r2_anthr1['hh_id'].astype(\"string\")\n",
    "df_r2_anthr1['hhm_id'] = df_r2_anthr1['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_anthr1['are_you_pregnant'] = df_r2_anthr1['are_you_pregnant'].astype(\"category\")\n",
    "df_r2_anthr1['are_you_lactating'] = df_r2_anthr1['are_you_lactating'].astype(\"category\")\n",
    "df_r2_anthr1['weight_kg'] = df_r2_anthr1['weight_kg'].astype(\"float64\")\n",
    "df_r2_anthr1['height_cm'] = df_r2_anthr1['height_cm'].astype(\"float64\")\n",
    "\n",
    "# computing the BMI\n",
    "df_r2_anthr1['bmi'] = df_r2_anthr1['weight_kg']*10000/(df_r2_anthr1['height_cm']**2)\n",
    "\n",
    "df_r2_anthr1['if_not_measured_why'] = df_r2_anthr1['if_not_measured_why'].astype(\"category\")\n",
    "df_r2_anthr1['if_not_measured_why'] = df_r2_anthr1['if_not_measured_why'].cat.rename_categories({\"Have measured\":\"have measured\",\"Absent\":\"absent\",\"Sick\":\"sick\",\"Other\":\"other\",\"Refused to  give Measure ment\":\"refused\"})\n",
    "df_r2_anthr1['if_not_measured_why'] = df_r2_anthr1['if_not_measured_why'].cat.set_categories([\"have measured\",\"absent\",\"sick\",\"other\",\"refused\",\"missing\"])\n",
    "\n",
    "# Filtering to only consider household in the national representative sample\n",
    "##df_r1_anthr2 = df_r1_anthr2.loc[(df_r1_anthr2.sample_type=='ftf original') | (df_r1_anthr2.sample_type=='national representative')]\n",
    "## df_r1_anthr2.reset_index(inplace=True)\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r2_anthr1.hh_id.str.len().max() - 2\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r2_anthr1[\"hh_id\"] = df_r2_anthr1.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r2_anthr1.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r2_anthr1[\"hhm_id\"] = df_r2_anthr1.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r2_anthr1[\"member_id\"] = df_r2_anthr1.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "# Correcting the no measured output\n",
    "\n",
    "df_r2_anthr1['if_not_measured_why'] = df_r2_anthr1['if_not_measured_why'].fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\3839828607.py:13: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_anthr2['hh_id'] = df_r2_anthr2['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r2_anthr2 = df_r2_anthr2.loc[:, ['a01', 'mid', 'w2_01', 'w2_02', 'w2_03', 'w2_04', 'w2_05','w2_07','w2_08', 'w2_10', 'w2_14']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r2_anthr2 = df_r2_anthr2.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"w2_01\": \"mother_id\", \"w2_02\": \"childbirth_day\", 'w2_03':'if_not_week_month', 'w2_04':'month_birth', 'w2_05':'year_birth',\n",
    "                            \"w2_07\": \"weight_kg\", 'w2_08':'height_cm', 'w2_14':'age_month', 'w2_10':'if_not_measured_why'})\n",
    "\n",
    "# Filtering to get only informations on underfive children only\n",
    "df_r2_anthr2 = df_r2_anthr2[df_r2_anthr2.loc[:,'hhm_id'] !='Not under five age child in this household']\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r2_anthr2' to correct the 'hh_id\n",
    "df_r2_anthr2['hh_id'] = df_r2_anthr2['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "df_r2_anthr2.reset_index(inplace=True)\n",
    "# Apply encoding to correct label\n",
    "label_to_int = {label: label for int, label in enumerate(df_r2_anthr2[\"hhm_id\"].unique())}\n",
    "df_r2_anthr2[\"hhm_id\"] = df_r2_anthr2[\"hhm_id\"].map(label_to_int)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r2_anthr2['hh_id'] = df_r2_anthr2['hh_id'].astype(\"string\")\n",
    "df_r2_anthr2['hhm_id'] = df_r2_anthr2['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "#df_r2_anthr2['mother_id'] = df_r2_anthr2['mother_id'].astype(\"int64\").astype(\"string\")\n",
    "#df_r2_anthr2['childbirth_day'] = df_r2_anthr2['childbirth_day'].astype(\"float64\")\n",
    "#df_r2_anthr2['month_birth'] = df_r2_anthr2['month_birth'].astype(\"float64\")\n",
    "#df_r2_anthr2['year_birth'] = df_r2_anthr2['year_birth'].astype(\"float64\")\n",
    "df_r2_anthr2['age_month'] = df_r2_anthr2['age_month'].astype(\"int64\", errors='ignore')\n",
    "df_r2_anthr2['if_not_week_month'] = df_r2_anthr2['if_not_week_month'].astype(\"category\")\n",
    "df_r2_anthr2['weight_kg'] = df_r2_anthr2['weight_kg'].astype(\"float64\")\n",
    "df_r2_anthr2['height_cm'] = df_r2_anthr2['height_cm'].astype(\"float64\")\n",
    "\n",
    "# computing the BMI\n",
    "df_r2_anthr2['bmi'] = df_r2_anthr2['weight_kg']*10000/(df_r2_anthr2['height_cm']**2)\n",
    "\n",
    "df_r2_anthr2['if_not_measured_why'] = df_r2_anthr2['if_not_measured_why'].astype(\"category\")\n",
    "# Correcting the no measured output\n",
    "df_r2_anthr2['if_not_measured_why'] = df_r2_anthr2['if_not_measured_why'].cat.rename_categories({\"Have measured\":\"have measured\",\"Absent\":\"absent\",\"Sick\":\"sick\",\"Other\":\"other\",\"Refused to  give Measure ment\":\"refused\"})\n",
    "df_r2_anthr2['if_not_measured_why'] = df_r2_anthr2['if_not_measured_why'].cat.set_categories([\"have measured\",\"absent\",\"sick\",\"other\",\"refused\",\"missing\"])\n",
    "\n",
    "df_r2_anthr2.loc[df_r2_anthr2['age_month']==0,'age_month'] = 1\n",
    "df_r2_anthr2['age_days'] = df_r2_anthr2['age_month'].apply(lambda curr_age:  curr_age*30)\n",
    "\n",
    "# Fill Nan in the child birth day\n",
    "df_r2_anthr2.childbirth_day = df_r2_anthr2.childbirth_day.fillna(1)\n",
    "\n",
    "# some correction\n",
    "## there is a child who have as day 31 and month september, I correct it to 30 to make sure that the information is well defined\n",
    "df_r2_anthr2.loc[(df_r2_anthr2.childbirth_day==31) & (df_r2_anthr2.month_birth==9),\"childbirth_day\"]=30\n",
    "## there is only 28 days in february 2007\n",
    "df_r2_anthr2.loc[(df_r2_anthr2.childbirth_day==29) & (df_r2_anthr2.month_birth==2) & (df_r2_anthr2.year_birth==2007),\"childbirth_day\"]=28\n",
    "\n",
    "## A child have 0.5 as month of birth\n",
    "df_r2_anthr2.loc[(df_r2_anthr2.month_birth==0.5),\"month_birth\"]=1\n",
    "df_r2_anthr2['date_birth'] = pd.to_datetime(dict(year=df_r2_anthr2.year_birth, month=df_r2_anthr2.month_birth, day=df_r2_anthr2.childbirth_day))\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r2_anthr2.hh_id.str.len().max() - 2\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r2_anthr2[\"hh_id\"] = df_r2_anthr2.hh_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r2_anthr2.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r2_anthr2[\"hhm_id\"] = df_r2_anthr2.hhm_id.apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r2_anthr2[\"member_id\"] = df_r2_anthr2.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "\n",
    "# Replace NaN in the reason for not measured by a give value:\n",
    "#df_r2_anthr2['if_not_measured_why'] = df_r2_anthr2['if_not_measured_why'].replace({\"Have measured\":\"have measured\",\"Absent\":\"absent\",\"Sick\":\"sick\",\"Other\":\"other\",\"Refused to  give Measure ment\":\"refused\"})\n",
    "\n",
    "df_r2_anthr2['if_not_measured_why'] = df_r2_anthr2['if_not_measured_why'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\3322896342.py:11: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_SE['hh_id'] = df_r2_SE['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\3322896342.py:54: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_SE[\"hh_id\"] = df_r2_SE.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\3322896342.py:58: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r2_SE[\"hhm_id\"] = df_r2_SE.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Sociaux economics caracteristiques of the household members\n",
    "\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r2_SE = df_r2_SE.loc[:, ['a01', 'mid','mem_stat', 'b1_01', 'b1_02', 'b1_03', 'b1_04', 'b1_07','b1_08','b1_09']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r2_SE = df_r2_SE.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"b1_01\": \"hhm_sex\", \"b1_02\": \"hhm_age\", 'b1_03':'relation_hhh', 'b1_04':'marital_status_hhm', 'b1_07':'literacy_hhm',\n",
    "                            \"b1_08\": \"education_high\", 'b1_09':'curr_att_school','mem_stat':'hhm_status'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r2_SE' to correct the 'hh_id\n",
    "df_r2_SE['hh_id'] = df_r2_SE['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "#df_r2_SE['hhm_age'] = df_r2_SE['hhm_age'].fillna(0)\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r2_SE['hh_id'] = df_r2_SE['hh_id'].astype(\"string\")\n",
    "df_r2_SE['hhm_id'] = df_r2_SE['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r2_SE['hhm_sex'] = df_r2_SE['hhm_sex'].astype(\"category\")\n",
    "df_r2_SE['hhm_age'] = df_r2_SE['hhm_age'].astype(\"int64\", errors='ignore')\n",
    "\n",
    "df_r2_SE['relation_hhh'] = df_r2_SE['relation_hhh'].astype(\"category\")\n",
    "df_r2_SE['marital_status_hhm'] = df_r2_SE['marital_status_hhm'].astype(\"category\")\n",
    "df_r2_SE['literacy_hhm'] = df_r2_SE['literacy_hhm'].astype(\"category\")\n",
    "#df_r2_SE['hhm_status'] = df_r2_SE['hhm_status'].astype(\"category\")\n",
    "\n",
    "df_r2_SE['education_high'] = df_r2_SE['education_high'].astype(\"category\")\n",
    "df_r2_SE['curr_att_school'] = df_r2_SE['curr_att_school'].astype(\"category\")\n",
    "\n",
    "\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r2_SE['relation_hhh'] = df_r2_SE['relation_hhh'].cat.rename_categories({1:'primary respondent',2:'primary respondent husband/wife',3:'son/daughter',4:'daughter/son-in-law',5:'grandson/daughter',6:'father/mother',\n",
    "                                                            7:'brother/sister',8:'niece/nephew',9:'primary respondent?s cousin',10:'father-in-law/mother-in-law',11:'brother/sister-in-law',\n",
    "                                                            12:'husband/wife?s  niece/nephew',13:'primary respondent?s husband/wife\\'s cousin',14:'other relative',15:'permanent servant',16:'other non relative/friends'})\n",
    "\n",
    "df_r2_SE['marital_status_hhm'] = df_r2_SE['marital_status_hhm'].cat.rename_categories({1:'unmarried (never married)',2:'married',3:'widow/widower',4:'divorced',5:'separated/deserted'})\n",
    "\n",
    "df_r2_SE['literacy_hhm'] = df_r2_SE['literacy_hhm'].cat.rename_categories({1:'cannot read and write',2:'can sign only',3:'can read  only',4:'can read and write'})\n",
    "\n",
    "df_r2_SE['education_high'] = df_r2_SE['education_high'].cat.rename_categories({0:'reads in class i',1:'completed class i',2:'completed class 2',3:'completed class 3',4:'completed class 4',5:'completed class 5',\n",
    "                                                                6:'completed class 6',7:'completed class 7',8:'completed class 8',9:'completed class 9',10:'completed ssc/dakhil',12:'completed hsc/alim',14:'ba/bsc pass/fazil',\n",
    "                                                                15:'ba/bsc honors/fazil',16:'ma/msc and above/kamil',22:'ssc candidate',33:'hsc candidate',66:'preschool class (general)',67:'preschool (mosque based)',\n",
    "                                                                71:'medical/mbbs',72:'nursing',73:'diploma engineer',74:'diploma Enginee',75:'vocational (scolarship based /technical Education',76:'others(specify)',99:'never attended school'})\n",
    "\n",
    "df_r2_SE['curr_att_school'] = df_r2_SE['curr_att_school'].cat.rename_categories({1:'yes',2:'no'})\n",
    "\n",
    "df_r2_SE['hhm_status'] = df_r2_SE['hhm_status'].replace({0:'Previous and current round member',1:'New member (new born)',2:'New member through marriage',3:'New member upon return from divorce or seperation',4:'Household merged/combined',5:'Other reasons (permanent)',\n",
    "                                                                6:'Residing elsewhere for the pursuit of studies',7:'Death',8:'Married and left household',9:'Divorced and left household',10:'Household split',11:'Left household for employment',12:'Other reasns for leaving the household',\n",
    "                                                                66:'New sample household and current round member'})\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r2_SE.hh_id.str.len().max() - 2\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r2_SE[\"hh_id\"] = df_r2_SE.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r2_SE.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each hh member code to ensure they all have the same length and update the 'hhm_id' column with the corrected codes\n",
    "df_r2_SE[\"hhm_id\"] = df_r2_SE.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r2_SE[\"member_id\"] = df_r2_SE.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "Previous and current round member                    23210\n",
       "New sample household and current round member         2387\n",
       "New member (new born)                                 1676\n",
       "Household split                                       1410\n",
       "Other reasons (permanent)                             1358\n",
       "Left household for employment                          880\n",
       "Married and left household                             837\n",
       "New member through marriage                            630\n",
       "Other reasns for leaving the household                 574\n",
       "Death                                                  494\n",
       "Household merged/combined                              442\n",
       "Residing elsewhere for the pursuit of studies          302\n",
       "Divorced and left household                             62\n",
       "New member upon return from divorce or seperation       35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r2_SE.hhm_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6943"
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_r2_SE.shape\n",
    "#34297\n",
    "#df_r2_anthr1.shape\n",
    "#24591\n",
    "#df_r2_anthr2.shape\n",
    "#2763\n",
    "#24591+2763\n",
    "\n",
    "#df_r2_SE[df_r2_SE.hhm_status.isna()]\n",
    "24591+2763 - 34297"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 3 datasets\n",
    "\n",
    "## HH identification data: Reading household identification data from a Stata file\n",
    "df_r3 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 3 (2018)\\\\BIHSRound3\\\\male\\\\009_bihs_r3_male_mod_a.dta', convert_categoricals=False) \n",
    "df_r3_f = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 3 (2018)\\\\BIHSRound3\\\\female\\\\092_bihs_r3_female_mod_a.dta', convert_categoricals=False) \n",
    "\n",
    "\n",
    "## FTF: Feed the Futur (FTF) census data from a Stata file\n",
    "#df_r2_ftf = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 2 (2015)\\\\001_census_ftf.dta') \n",
    "\n",
    "\n",
    "### Sociaux economics characteristics\n",
    "df_r3_SE = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 3 (2018)\\\\BIHSRound3\\\\male\\\\010_bihs_r3_male_mod_b1.dta', convert_categoricals=False) \n",
    "\n",
    "## Anthro data: Reading anthropometric data from Stata files\n",
    "### 1 for all household members\n",
    "df_r3_anthr1 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 3 (2018)\\\\BIHSRound3\\\\female\\\\099_bihs_r3_female_mod_w1.dta') \n",
    "\n",
    "### 2 for under five children\n",
    "df_r3_anthr2 = pd.read_stata(os.getcwd() + '\\\\input\\\\individual_panel_data\\\\data\\\\BHIS round 3 (2018)\\\\BIHSRound3\\\\female\\\\100_bihs_r3_female_mod_w2.dta') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Male enumerators data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\1780561567.py:10: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3['hh_id'] = df_r3['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\1780561567.py:24: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df_r3['interview_status'] = df_r3['interview_status'].replace({1:'Complete',2:'Refused',3:'Not at home',4:'Migrated',5:'Partial',6:'Refused'})\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\1780561567.py:50: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3[\"hh_id\"] = df_r3.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\1780561567.py:55: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3[\"upazila_code\"] = df_r3.upazila_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\1780561567.py:60: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3[\"village_code\"] = df_r3.village_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\1780561567.py:65: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3[\"union_code\"] = df_r3.union_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r3\n",
    "df_r3 = df_r3.loc[:, ['a01', 'div', 'village', 'union', 'div_name', 'district', 'upazila', 'a13', 'a15','a16_1_dd','a16_1_mm','a16_1_yy','a27', 'hh_type']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r3 = df_r3.rename(columns={'a01': \"hh_id\",'div':'division', \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\", 'district':'district_code', \n",
    "                    'upazila':'upazila_code','village':'village_code', 'union':'union_code', 'a16_1_dd': 'first_v_day' ,'a16_1_mm': 'first_v_month','a16_1_yy': 'first_v_year','a27':'interview_status','hh_type':'sample_type'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r3' to correct the 'hh_id\n",
    "df_r3['hh_id'] = df_r3['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r3['hh_id'] = df_r3['hh_id'].astype(\"string\")\n",
    "df_r3['division'] = df_r3['division'].astype(\"int64\").astype(\"string\")\n",
    "df_r3['village_code'] = df_r3['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3['upazila_code'] = df_r3['upazila_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3['union_code'] = df_r3['union_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3['interview_status'] = df_r3['interview_status'].astype(\"category\")\n",
    "df_r3['sample_type'] = df_r3['sample_type'].astype(\"category\")\n",
    "\n",
    "df_r3['hh_head_religion'] = df_r3['hh_head_religion'].astype(\"category\")\n",
    "df_r3['hh_ethnic_group'] = df_r3['hh_ethnic_group'].astype(\"category\")\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r3['interview_status'] = df_r3['interview_status'].replace({1:'Complete',2:'Refused',3:'Not at home',4:'Migrated',5:'Partial',6:'Refused'})\n",
    "df_r3['hh_head_religion'] = df_r3['hh_head_religion'].cat.rename_categories({1:'Muslim',2:'Hindu',3:'Christian',4:'Buddhist',5:'Other(specify)'})\n",
    "df_r3['hh_ethnic_group'] = df_r3['hh_ethnic_group'].cat.rename_categories({1:'Bangali',2:'Bihari',3:'Sawtal',4:'Khasia',5:'Rakhain',6:'Bown',\n",
    "                                                            7:'Chak',8:'Chakma',9:'Khumi',10:'Kheyanf',11:'Lusai/Pankho',12:'Marma',13:'Mru(Murong)',\n",
    "                                                            14:'Tonchonga',15:'Tripura',16:'Bonojogi',17:'Others'})\n",
    "\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type!='FTF additional')]\n",
    "#df_r1.sample_type.value_counts()\n",
    "\n",
    "#df_r2['first_v_day'] = df_r2['first_v_day'].astype(\"float64\")\n",
    "#df_r2['first_v_month'] = df_r2['first_v_month'].astype(\"float64\")\n",
    "#df_r2['first_v_year'] = df_r2['first_v_year'].astype(\"float64\")\n",
    "df_r3['first_v_day'] = df_r3['first_v_day'].astype(\"int64\")\n",
    "df_r3['first_v_month'] = df_r3['first_v_month'].astype(\"int64\")\n",
    "df_r3['first_v_year'] = df_r3['first_v_year'].astype(\"int64\")\n",
    "# Filtering to only consider household in the national representative sample\n",
    "#df_r1 = df_r1.loc[(df_r1.sample_type=='FTF Original') | (df_r1.sample_type=='National Representative')]\n",
    "\n",
    "df_r3['first_v_day'] = df_r3.first_v_day.replace({99999:'01'})\n",
    "# creating the interview date\n",
    "df_r3['date_int'] = pd.to_datetime(dict(year=df_r3.first_v_year, month=df_r3.first_v_month, day=df_r3.first_v_day))\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r3.hh_id.str.len().max() - 3\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r3[\"hh_id\"] = df_r3.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r3.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r3[\"upazila_code\"] = df_r3.upazila_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r3.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r3[\"village_code\"] = df_r3.village_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r3.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union' column with the corrected codes\n",
    "df_r3[\"union_code\"] = df_r3.union_code.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Household initial id to get the parent household\n",
    "df_r3[\"hh_id_parent\"] = df_r3.hh_id.apply(lambda row: row[:row.find('.')] if row.find('.')!=-1 else row)\n",
    "df_r3[\"hh_split\"] = df_r3.apply(lambda row: '1' if row[\"hh_id_parent\"] != row[\"hh_id\"] else '0', axis=1).astype('category')\n",
    "\n",
    "## hh_split_bis capture infant hh (1 if the hh is an infant hh and 0 otherwise)\n",
    "df_r3[\"hh_split_bis\"] = df_r3.apply(lambda row: '1' if (row[\"hh_id_parent\"]+\".1\" != row[\"hh_id\"]) & (row[\"hh_id_parent\"] != row[\"hh_id\"]) else '0', axis=1).astype('category')\n",
    "\n",
    "# Build an unique IDs for each hh\n",
    "df_r3[\"hh_id_cmplt\"] = df_r3.apply(lambda row : str(row.loc[\"union_code\"]) + str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r3[\"survey_round\"] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Female enumerators data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\1445982298.py:27: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df_r3_f['interview_status'] = df_r3_f['interview_status'].replace({1:'Complete',2:'Refused',3:'Not at home',4:'Migrated',5:'Partial',6:'Refused'})\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r3_f = df_r3_f.loc[:, ['a01', 'div', 'village', 'union', 'div_name', 'district', 'upazila','a02', 'a13', 'a15', 'a27','hh_type']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r3_f = df_r3_f.rename(columns={'a01': \"hh_id\",'div':'division', \"a02\": \"census_nbr\", \"a13\": \"hh_head_religion\", \"a15\": \"hh_ethnic_group\", 'district':'district_code', \n",
    "                    'upazila':'upazila_code','village':'village_code', 'union':'union_code', 'a27':'interview_status', 'hh_type':'sample_type'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r3_f' to correct the 'hh_id\n",
    "df_r3_f['hh_id'] = df_r3_f['hh_id'].astype(\"category\").apply( lambda n: n if n % 1 else int(n))\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r3_f['hh_id'] = df_r3_f['hh_id'].astype(\"string\")\n",
    "df_r3_f['division'] = df_r3_f['division'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_f['village_code'] = df_r3_f['village_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_f['upazila_code'] = df_r3_f['upazila_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_f['union_code'] = df_r3_f['union_code'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_f['census_nbr'] = df_r3_f['census_nbr'].astype(\"category\")\n",
    "df_r3_f['sample_type'] = df_r3_f['sample_type'].astype(\"category\")\n",
    "df_r3_f['interview_status'] = df_r3_f['interview_status'].astype(\"category\")\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r3_f['interview_status'] = df_r3_f['interview_status'].cat.rename_categories({1:'Complete',2:'Partial',3:'Refused',4:'Not at home',5:'Migrated'})\n",
    "\n",
    "df_r3_f['hh_head_religion'] = df_r3_f['hh_head_religion'].astype(\"category\")\n",
    "df_r3_f['hh_ethnic_group'] = df_r3_f['hh_ethnic_group'].astype(\"category\")\n",
    "# Replace the numeric value by there label\n",
    "df_r3_f['interview_status'] = df_r3_f['interview_status'].replace({1:'Complete',2:'Refused',3:'Not at home',4:'Migrated',5:'Partial',6:'Refused'})\n",
    "df_r3_f['hh_head_religion'] = df_r3_f['hh_head_religion'].cat.rename_categories({1:'Muslim',2:'Hindu',3:'Christian',4:'Buddhist',5:'Other(specify)'})\n",
    "df_r3_f['hh_ethnic_group'] = df_r3_f['hh_ethnic_group'].cat.rename_categories({1:'Bengali',2:'Bihari',3:'Sawtal',4:'Khasia',5:'Rakhain',6:'Bown',\n",
    "                                                            7:'Chak',8:'Chakma',9:'Khumi',10:'Kheyanf',11:'Lusai/Pankho',12:'Marma',13:'Mru(Murong)',\n",
    "                                                            14:'Tonchonga',15:'Tripura',16:'Bonojogi',17:'Others'})\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r3_f.hh_id.str.len().max() - 3\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r3_f[\"hh_id\"] = df_r3_f.hh_id.astype(\"category\").apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the upazila_code in the dataframe\n",
    "nbr_max = df_r3_f.upazila_code.str.len().max()\n",
    "## Apply the correct_id function to each upazila_code to ensure they all have the same length and update the 'upazila_code' column with the corrected IDs\n",
    "df_r3_f[\"upazila_code\"] = df_r3_f.upazila_code.astype(\"category\").apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r3_f.village_code.str.len().max()\n",
    "## Apply the correct_id function to each village code to ensure they all have the same length and update the 'village_code' column with the corrected codes\n",
    "df_r3_f[\"village_code\"] = df_r3_f.village_code.astype(\"category\").apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "## Find the maximum length of the union codes in the dataframe\n",
    "nbr_max = df_r3_f.union_code.str.len().max()\n",
    "## Apply the correct_id function to each union code to ensure they all have the same length and update the 'union' column with the corrected codes\n",
    "df_r3_f[\"union_code\"] = df_r3_f.union_code.astype(\"category\").apply(lambda row: correct_id(row, nbr_max))\n",
    "\n",
    "# Household initial id to get the parent household\n",
    "df_r3_f[\"hh_id_parent\"] = df_r3_f.hh_id.apply(lambda row: row[:row.find('.')] if row.find('.')!=-1 else row)\n",
    "df_r3_f[\"hh_split\"] = df_r3_f.apply(lambda row: '1' if row[\"hh_id_parent\"] != row[\"hh_id\"] else '0', axis=1).astype('category')\n",
    "\n",
    "# Build an unique IDs for each hh\n",
    "df_r3_f[\"hh_id_cmplt\"] = df_r3_f.apply(lambda row : str(row.loc[\"union_code\"]) + str(row.loc[\"village_code\"]) + str(row.loc[\"hh_id\"]), axis=1)\n",
    "\n",
    "# To build a variable for the current round\n",
    "df_r3_f[\"survey_round\"] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anthropometric data (more than 5 years old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2738657749.py:10: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr1['hh_id'] = df_r3_anthr1['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2738657749.py:33: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr1[\"hh_id\"] = df_r3_anthr1.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2738657749.py:37: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr1[\"hhm_id\"] = df_r3_anthr1.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r1\n",
    "df_r3_anthr1 = df_r3_anthr1.loc[:, ['a01', 'mid_w1' ,'w1_01', 'w1_02', 'w1_03','w1_04', 'w1_05']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r3_anthr1 = df_r3_anthr1.rename(columns={'a01': \"hh_id\",'mid_w1':'hhm_id', \"w1_01\": \"are_you_pregnant\", \"w1_02\": \"are_you_lactating\",'mem_stat':'mem_stat',\n",
    "                            \"w1_03\": \"weight_kg\", 'w1_04':'height_cm', 'w1_05':'if_not_measured_why'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r3_anthr1' to correct the 'hh_id\n",
    "df_r3_anthr1['hh_id'] = df_r3_anthr1['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "#df_r3_anthr1['if_not_measured_why'] = df_r3_anthr1['if_not_measured_why'].cat.set_categories({\"have measured\",\"absent\",\"sick\",\"other\",\"refused\",\"missing\"})\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r3_anthr1['hh_id'] = df_r3_anthr1['hh_id'].astype(\"string\")\n",
    "df_r3_anthr1['hhm_id'] = df_r3_anthr1['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_anthr1['are_you_pregnant'] = df_r3_anthr1['are_you_pregnant'].astype(\"category\")\n",
    "df_r3_anthr1['are_you_lactating'] = df_r3_anthr1['are_you_lactating'].astype(\"category\")\n",
    "df_r3_anthr1['weight_kg'] = df_r3_anthr1['weight_kg'].astype(\"float64\")\n",
    "df_r3_anthr1['height_cm'] = df_r3_anthr1['height_cm'].astype(\"float64\")\n",
    "\n",
    "# computing the BMI\n",
    "df_r3_anthr1['bmi'] = df_r3_anthr1['weight_kg']*10000/(df_r3_anthr1['height_cm']**2)\n",
    "\n",
    "df_r3_anthr1['if_not_measured_why'] = df_r3_anthr1['if_not_measured_why'].astype(\"category\")\n",
    "# Correcting the no measured output\n",
    "df_r3_anthr1['if_not_measured_why'] = df_r3_anthr1['if_not_measured_why'].cat.rename_categories({\"Have measured\":\"have measured\",\"Absent\":\"absent\",\"Sick\":\"sick\",\"Other (specify)\":\"other\",\"Refused to  give measurement\":\"refused\"})\n",
    "df_r3_anthr1['if_not_measured_why'] = df_r3_anthr1['if_not_measured_why'].cat.set_categories({\"have measured\",\"absent\",\"sick\",\"other\",\"refused\",\"missing\"})\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r3_anthr1.hh_id.str.len().max() - 3\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r3_anthr1[\"hh_id\"] = df_r3_anthr1.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r3_anthr1.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each hh member code to ensure they all have the same length and update the 'hhm_id' column with the corrected codes\n",
    "df_r3_anthr1[\"hhm_id\"] = df_r3_anthr1.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r3_anthr1[\"member_id\"] = df_r3_anthr1.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "df_r3_anthr1['if_not_measured_why'] = df_r3_anthr1['if_not_measured_why'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anthropometric data (more less than 5 years old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\4161378374.py:10: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr2['hh_id'] = df_r3_anthr2['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\4161378374.py:60: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr2[\"hh_id\"] = df_r3_anthr2.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\4161378374.py:64: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_anthr2[\"hhm_id\"] = df_r3_anthr2.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the first dataframe\n",
    "## Select columns of interest from the dataframe df_r3\n",
    "df_r3_anthr2 = df_r3_anthr2.loc[:, ['a01', 'mid_w2', 'w2_01', 'w2_05','w2_07','w2_08', 'w2_10','w2_02','w2_03','w2_04','w2_05','w2_13','w2_15' ,'haz06', 'waz06', 'bmiz06','flag_zs']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r3_anthr2 = df_r3_anthr2.rename(columns={'a01': \"hh_id\",'mid_w2':'hhm_id', \"w2_01\": \"mother_id\",'w2_02':'child',\"w2_02\": \"childbirth_day\",\"w2_13\":\"birth_order\", 'w2_03':'if_not_week_month', 'w2_04':'month_birth', 'w2_05':'year_birth',\n",
    "                                            'haz06':'HAZ', 'waz06':'WAZ', 'bmiz06':'BMIZ',\"w2_07\": \"weight_kg\",\"w2_15\":\"age_month\", 'w2_08':'height_cm', 'w2_10':'if_not_measured_why','flag_zs':'flag'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r3_anthr2' to correct the 'hh_id\n",
    "df_r3_anthr2['hh_id'] = df_r3_anthr2['hh_id'].apply( lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "## Filtering to get surveyed household with an underfive child\n",
    "df_r3_anthr2 = df_r3_anthr2[df_r3_anthr2.hhm_id !='There are no <5 child in this household']\n",
    "df_r3_anthr2.reset_index(inplace=True)\n",
    "\n",
    "# Apply encoding to correct label\n",
    "label_to_int = {label: label for int, label in enumerate(df_r3_anthr2[\"hhm_id\"].unique())}\n",
    "df_r3_anthr2[\"hhm_id\"] = df_r3_anthr2[\"hhm_id\"].map(label_to_int)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r3_anthr2['hh_id'] = df_r3_anthr2['hh_id'].astype(\"string\")\n",
    "df_r3_anthr2['hhm_id'] = df_r3_anthr2['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "#df_r3_anthr2['mother_id'] = df_r3_anthr2['mother_id'].astype(\"int64\").astype(\"string\")\n",
    "#df_r3_anthr2['childbirth_day'] = df_r3_anthr2['childbirth_day'].astype(\"int64\")\n",
    "#df_r3_anthr2['month_birth'] = df_r3_anthr2['month_birth'].astype(\"int64\")\n",
    "#df_r3_anthr2['year_birth'] = df_r3_anthr2['year_birth'].astype(\"int64\")\n",
    "#df_r3_anthr2['if_not_week_month'] = df_r3_anthr2['if_not_week_month'].astype(\"category\")\n",
    "df_r3_anthr2['weight_kg'] = df_r3_anthr2['weight_kg'].astype(\"float64\")\n",
    "df_r3_anthr2['height_cm'] = df_r3_anthr2['height_cm'].astype(\"float64\")\n",
    "\n",
    "# computing the BMI\n",
    "df_r3_anthr2['bmi'] = df_r3_anthr2['weight_kg']*10000/(df_r3_anthr2['height_cm']**2)\n",
    "df_r3_anthr2['if_not_measured_why'] = df_r3_anthr2['if_not_measured_why'].astype(\"category\")\n",
    "# Correcting the no measured output\n",
    "df_r3_anthr2['if_not_measured_why'] = df_r3_anthr2['if_not_measured_why'].cat.rename_categories({\"Have measured\":\"have measured\",\"Absent\":\"absent\",\"Sick\":\"sick\",\"Other (specify)\":\"other\",\"Refused to  give measurement\":\"refused\"})\n",
    "df_r3_anthr2['if_not_measured_why'] = df_r3_anthr2['if_not_measured_why'].cat.set_categories({\"have measured\",\"absent\",\"sick\",\"other\",\"refused\",\"missing\"})\n",
    "\n",
    "#df_r3_anthr2.loc[df_r3_anthr2['age_month']==0,'age_month'] = 1\n",
    "df_r3_anthr2['age_days'] = df_r3_anthr2['age_month'].apply(lambda curr_age:  curr_age*30)\n",
    "\n",
    "# Fill Nan in the child birth day\n",
    "#df_r3_anthr2.childbirth_day = df_r3_anthr2.childbirth_day.fillna(1)\n",
    "\n",
    "# some correction\n",
    "## there is a child who have as day 31 and month september, I correct it to 30 to make sure that the information is well defined\n",
    "### df_r3_anthr2.loc[(df_r3_anthr2.childbirth_day==31) & (df_r3_anthr2.month_birth==9),\"childbirth_day\"]=30\n",
    "## there is only 28 days in february 2007\n",
    "### df_r3_anthr2.loc[(df_r3_anthr2.childbirth_day==29) & (df_r3_anthr2.month_birth==2) & (df_r3_anthr2.year_birth==2007),\"childbirth_day\"]=28\n",
    "\n",
    "## A child have 0.5 as month of birth\n",
    "###df_r3_anthr2.loc[(df_r3_anthr2.month_birth==0.5),\"month_birth\"]=1\n",
    "###df_r3_anthr2['date_birth'] = pd.to_datetime(dict(year=df_r3_anthr2.year_birth, month=df_r3_anthr2.month_birth, day=df_r3_anthr2.childbirth_day))\n",
    "\n",
    "\n",
    "\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r3_anthr2.hh_id.str.len().max() - 3\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r3_anthr2[\"hh_id\"] = df_r3_anthr2.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r3_anthr2.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each hh member code to ensure they all have the same length and update the 'hhm_id' column with the corrected codes\n",
    "df_r3_anthr2[\"hhm_id\"] = df_r3_anthr2.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r3_anthr2[\"member_id\"] = df_r3_anthr2.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)\n",
    "\n",
    "df_r3_anthr2['if_not_measured_why'] = df_r3_anthr2['if_not_measured_why'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other household data (more than 5 years old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2909285177.py:11: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_SE['hh_id'] = df_r3_SE['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2909285177.py:55: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_SE[\"hh_id\"] = df_r3_SE.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18824\\2909285177.py:59: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_r3_SE[\"hhm_id\"] = df_r3_SE.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n"
     ]
    }
   ],
   "source": [
    "# Sociaux economics caracteristiques of the household members\n",
    "\n",
    "## Select columns of interest from the dataframe df_r3\n",
    "df_r3_SE = df_r3_SE.loc[:, ['a01', 'mid', 'mem_stat' ,'b1_01', 'b1_02', 'b1_03', 'b1_04', 'b1_07','b1_08','b1_09']]\n",
    "\n",
    "## Rename specific columns for better readability and understanding\n",
    "df_r3_SE = df_r3_SE.rename(columns={'a01': \"hh_id\",'mid':'hhm_id', \"b1_01\": \"hhm_sex\", \"b1_02\": \"hhm_age\", 'b1_03':'relation_hhh', 'b1_04':'marital_status_hhm', 'b1_07':'literacy_hhm',\n",
    "                            \"b1_08\": \"education_high\", 'b1_09':'curr_att_school','mem_stat':'hhm_status'})\n",
    "\n",
    "# Apply a function to each element in the 'hh_id' column of the DataFrame 'df_r3_SE' to correct the 'hh_id\n",
    "df_r3_SE['hh_id'] = df_r3_SE['hh_id'].apply(lambda n: n if n % 1 else int(n), convert_dtype=False)\n",
    "\n",
    "## Convert specific columns to categorical data type for optimized performance and memory usage\n",
    "df_r3_SE['hh_id'] = df_r3_SE['hh_id'].astype(\"string\")\n",
    "df_r3_SE['hhm_id'] = df_r3_SE['hhm_id'].astype(\"int64\").astype(\"string\")\n",
    "df_r3_SE['hhm_sex'] = df_r3_SE['hhm_sex'].astype(\"category\")\n",
    "df_r3_SE['hhm_age'] = df_r3_SE['hhm_age'].astype(\"int64\", errors='ignore')\n",
    "\n",
    "df_r3_SE['relation_hhh'] = df_r3_SE['relation_hhh'].astype(\"category\")\n",
    "df_r3_SE['marital_status_hhm'] = df_r3_SE['marital_status_hhm'].astype(\"category\")\n",
    "df_r3_SE['literacy_hhm'] = df_r3_SE['literacy_hhm'].astype(\"category\")\n",
    "\n",
    "df_r3_SE['education_high'] = df_r3_SE['education_high'].astype(\"category\")\n",
    "df_r3_SE['curr_att_school'] = df_r3_SE['curr_att_school'].astype(\"category\")\n",
    "\n",
    "\n",
    "# Replace the numeric value by there label in the 'interview_status'.\n",
    "df_r3_SE['relation_hhh'] = df_r3_SE['relation_hhh'].cat.rename_categories({1:'primary respondent',2:'primary respondent husband/wife',3:'son/daughter',4:'daughter/son-in-law',5:'grandson/daughter',6:'father/mother',\n",
    "                                                            7:'brother/sister',8:'niece/nephew',9:'primary respondent?s cousin',10:'father-in-law/mother-in-law',11:'brother/sister-in-law',\n",
    "                                                            12:'husband/wife?s  niece/nephew',13:'primary respondent?s husband/wife\\'s cousin',14:'other relative',15:'permanent servant',16:'other non relative/friends'})\n",
    "\n",
    "df_r3_SE['marital_status_hhm'] = df_r3_SE['marital_status_hhm'].cat.rename_categories({1:'unmarried (never married)',2:'married',3:'widow/widower',4:'divorced',5:'separated/deserted'})\n",
    "\n",
    "df_r3_SE['literacy_hhm'] = df_r3_SE['literacy_hhm'].cat.rename_categories({1:'cannot read and write',2:'can sign only',3:'can read  only',4:'can read and write'})\n",
    "\n",
    "df_r3_SE['education_high'] = df_r3_SE['education_high'].cat.rename_categories({0:'reads in class i',1:'completed class i',2:'completed class 2',3:'completed class 3',4:'completed class 4',5:'completed class 5',\n",
    "                                                                6:'completed class 6',7:'completed class 7',8:'completed class 8',9:'completed class 9',10:'completed ssc/dakhil',12:'completed hsc/alim',14:'ba/bsc pass/fazil',\n",
    "                                                                15:'ba/bsc honors/fazil',16:'ma/msc and above/kamil',22:'ssc candidate',33:'hsc candidate',66:'preschool class (general)',67:'preschool (mosque based)',\n",
    "                                                                71:'medical/mbbs',72:'nursing',73:'diploma engineer',74:'diploma Enginee',75:'vocational (scolarship based /technical Education',76:'others(specify)',99:'never attended school'})\n",
    "\n",
    "df_r3_SE['curr_att_school'] = df_r3_SE['curr_att_school'].cat.rename_categories({1:'yes',2:'no'})\n",
    "\n",
    "\n",
    "df_r3_SE['hhm_status'] = df_r3_SE['hhm_status'].replace({0:'Previous and current round member',1:'New member (new born)',2:'New member through marriage',3:'New member upon return from divorce or seperation',4:'Household merged/combined',5:'Other reasons (permanent)',\n",
    "                                                                6:'Residing elsewhere for the pursuit of studies',7:'Death',8:'Married and left household',9:'Divorced and left household',10:'Household split',11:'Left household for employment',12:'Other reasns for leaving the household',\n",
    "                                                                66:'New sample household and current round member'})\n",
    "\n",
    "#{0:'Prvs and crrnt rnd Mmbr',1:'Nw mmbr (Nw Brn)',2:'Nw mmbr thrgh mrrg',3:'Nw mmbr upon rtrn frm dvrc or sprtn',4:'Hshld mrgd/cmbnd',5:'Other reasons (Permanent)',\n",
    "#                                                                6:'Residing elsewhere for the pursuit of studies',7:'Death',8:'Married and left household',9:'Divorced and left household',10:'Household split',11:'Left household for employment',12:'Other reasns for leaving the household',\n",
    "#                                                                66:'New sample Household and current round member'}\n",
    "# Correcting the id by adding '0' to obtain a unique Id.\n",
    "## Find the maximum length of the household IDs in the dataframe\n",
    "nbr_max = df_r3_SE.hh_id.str.len().max() - 3\n",
    "## Apply the correct_id function to each household ID to ensure they all have the same length and update the 'hh_id' column with the corrected IDs\n",
    "df_r3_SE[\"hh_id\"] = df_r3_SE.hh_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "## Find the maximum length of the village codes in the dataframe\n",
    "nbr_max = df_r3_SE.hhm_id.str.len().max()\n",
    "## Apply the correct_id function to each hh member code to ensure they all have the same length and update the 'hhm_id' column with the corrected codes\n",
    "df_r3_SE[\"hhm_id\"] = df_r3_SE.hhm_id.apply(lambda row: correct_id(row, nbr_max), convert_dtype=False)\n",
    "\n",
    "# Build an unique IDs for each hh member\n",
    "df_r3_SE[\"member_id\"] = df_r3_SE.apply(lambda row : str(row.loc[\"hh_id\"]) + str(row.loc[\"hhm_id\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "Previous and current round member                    19284\n",
       "Household split                                       3310\n",
       "New member (new born)                                 1738\n",
       "Left household for employment                         1603\n",
       "Married and left household                            1552\n",
       "Other reasons (permanent)                             1372\n",
       "Other reasns for leaving the household                1091\n",
       "Death                                                  993\n",
       "New member through marriage                            609\n",
       "Residing elsewhere for the pursuit of studies          552\n",
       "Household merged/combined                              120\n",
       "Divorced and left household                             94\n",
       "New member upon return from divorce or seperation       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 952,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_SE['hhm_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building some variales characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is in the last round of the survey around 6011 households for a total of 5503 distinct household. Which represent an attrition rate of 15.37 percent (not data on the reason for not being surveyed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_r2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_r2\u001b[49m\u001b[38;5;241m.\u001b[39minterview_status\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_r2' is not defined"
     ]
    }
   ],
   "source": [
    "df_r2.interview_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the round 2, there is a total of 6715 household survey. Among these, 396 household splitted from the round 1 (around 6 percent of the nbr in the first round): \n",
    "* Complete       6436 (99 percent); 6040 (92.88 percent) among the hh who doesn't split ; 6224 distinct hh in r1\n",
    "* Migrated        237 (3.64 percent); 237 (3.64 percent) ;\n",
    "* Not at home      30 (0.5 percent); 30 (0.5 percent);\n",
    "* Refused          12 (0.2 percent); 12 (0.2 percent);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying hh where present in the first round but migrate or leave the house, refused in r2\n",
    "\n",
    "lst_complete = df_r2[df_r2.interview_status==\"Complete\"].hh_id_parent.drop_duplicates().values\n",
    "lst_migrate = df_r2[df_r2.interview_status==\"Migrated\"].hh_id_parent.drop_duplicates().values\n",
    "lst_nthome = df_r2[df_r2.interview_status==\"Not at home\"].hh_id_parent.drop_duplicates().values\n",
    "lst_refused = df_r2[df_r2.interview_status==\"Refused\"].hh_id_parent.drop_duplicates().values\n",
    "\n",
    "lst_split = df_r2[df_r2.hh_split==\"1\"].hh_id_parent.drop_duplicates().values\n",
    "\n",
    "df_r1[\"interview_status_r2\"] = df_r1.hh_id.apply(lambda row: \"Complete\" if row in lst_complete else \"Migrated\" if row in lst_migrate else \"Not at home\" if row in lst_nthome else \"Refused\" if row in lst_refused else \"Not found\")\n",
    "df_r1[\"hh_split_r2\"] = df_r1.hh_id.apply(lambda row: \"1\" if row in lst_split else \"0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hh_split_r2\n",
       "0    6319\n",
       "1     184\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 955,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r1[\"hh_split_r2\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the round 3, there is a total of 6011 household survey. Among these, 935 households splitted from the round 2 (around 14.37 percent of the nbr in the first round): \n",
    "* Complete       5604 (86.17 percent); 4690 (72.12 percent) among the hh who doesn't split \n",
    "* Migrated        360 (5.53 percent); 346 (5.32 percent) \n",
    "* Not at home      24 (0.36 percent); 24 (0.36 percent)\n",
    "* Refused          22 (0.34 percent); 15 (0.34 percent)\n",
    "* Partial           1 (0.01 percent); 1 (0.01 percent)\n",
    "* **Not found**: 1421 (22 percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying hh who were present in the first round but migrate or leave the house, refused or partial refused\n",
    "\n",
    "lst_complete = df_r3[df_r3.interview_status==\"Complete\"].hh_id_parent.drop_duplicates().values\n",
    "lst_migrate = df_r3[df_r3.interview_status==\"Migrated\"].hh_id_parent.drop_duplicates().values\n",
    "lst_nthome = df_r3[df_r3.interview_status==\"Not at home\"].hh_id_parent.drop_duplicates().values\n",
    "lst_refused = df_r3[df_r3.interview_status==\"Refused\"].hh_id_parent.drop_duplicates().values\n",
    "lst_split = df_r3[df_r3.hh_split==\"1\"].hh_id_parent.drop_duplicates().values\n",
    "\n",
    "df_r1[\"interview_status_r3\"] = df_r1.hh_id.apply(lambda row: \"Complete\" if row in lst_complete else \"Migrated\" if row in lst_migrate else \"Not at home\" if row in lst_nthome else \"Refused\" if row in lst_refused else \"Not found\")\n",
    "df_r1[\"hh_split_r3\"] = df_r1.hh_id.apply(lambda row: \"1\" if row in lst_split else \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary for hh of interest\n",
    "df_r1[\"hh_interest\"] = df_r1.apply(lambda row: \"1\" if (row.loc[\"interview_status_r3\"]==\"Complete\") & (row.loc[\"hh_split_r3\"]==\"0\") & (row.loc[\"hh_split_r3\"]==\"0\") else \"0\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hh_split_r2_r3\n",
       "0    6054\n",
       "1     449\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 958,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary for the household who split at least one time\n",
    "df_r1[\"hh_split_r2_r3\"] = df_r1.apply(lambda row: \"1\" if (row.loc[\"hh_split_r2\"]==\"1\") | (row.loc[\"hh_split_r3\"]==\"1\") else \"0\", axis=1)\n",
    "df_r1[\"hh_split_r2_r3\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1\n",
    "\n",
    "## Group the DataFrame 'df_r1_SE' by 'hh_id'\n",
    "df_r1_SE_gr = df_r1_SE.groupby('hh_id')\n",
    "\n",
    "## Aggregate the grouped data\n",
    "# aggregated_result_r1 = df_r1_SE_gr.agg(\n",
    "#     ## Count the number of females in each household\n",
    "#     nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),\n",
    "\n",
    "#     ## Count the number of household members under 5 years old\n",
    "#     nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 5 and 10 years old\n",
    "#     nbr_yngchldrn_5_10 = ('hhm_age', lambda x: ((x >= 5)  & (x < 10)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 10 and 19 years old\n",
    "#     nbr_teenager_10_19 =('hhm_age', lambda x: ((x >= 10)  & (x < 19)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 20 and 65 years old\n",
    "#     nbr_adults_20_65 = ('hhm_age', lambda x: ((x >= 20)  & (x < 65)).sum()),\n",
    "\n",
    "#     ## Count the number of household members 65 years old and over\n",
    "#     nbr_elderly_65_over = ('hhm_age', lambda x: (x >= 65).sum()),\n",
    "# ).reset_index()\n",
    "\n",
    "\n",
    "# Round 2\n",
    "\n",
    "## Group the DataFrame 'df_r1_SE' by 'hh_id'\n",
    "df_r2_SE_gr = df_r2_SE.groupby('hh_id')\n",
    "\n",
    "# ## Aggregate the grouped data\n",
    "# aggregated_result_r2 = df_r2_SE_gr.agg(\n",
    "#     ## Count the number of females in each household\n",
    "#     nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),\n",
    "\n",
    "#     ## Count the number of household members under 5 years old\n",
    "#     nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 5 and 10 years old\n",
    "#     nbr_yngchldrn_5_10 = ('hhm_age', lambda x: ((x >= 5)  & (x < 10)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 10 and 19 years old\n",
    "#     nbr_teenager_10_19 =('hhm_age', lambda x: ((x >= 10)  & (x < 19)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 20 and 65 years old\n",
    "#     nbr_adults_20_65 = ('hhm_age', lambda x: ((x >= 20)  & (x < 65)).sum()),\n",
    "\n",
    "#     ## Count the number of household members 65 years old and over\n",
    "#     nbr_elderly_65_over = ('hhm_age', lambda x: (x >= 65).sum()),\n",
    "# ).reset_index()\n",
    "\n",
    "\n",
    "# Round 3\n",
    "\n",
    "## Group the DataFrame 'df_r1_SE' by 'hh_id'\n",
    "df_r3_SE_gr = df_r3_SE.groupby('hh_id')\n",
    "\n",
    "## Aggregate the grouped data\n",
    "# aggregated_result_r3 = df_r3_SE_gr.agg(\n",
    "#     ## Count the number of females in each household\n",
    "#     nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),\n",
    "\n",
    "#     ## Count the number of household members under 5 years old\n",
    "#     nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 5 and 10 years old\n",
    "#     nbr_yngchldrn_5_10 = ('hhm_age', lambda x: ((x >= 5)  & (x < 10)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 10 and 19 years old\n",
    "#     nbr_teenager_10_19 =('hhm_age', lambda x: ((x >= 10)  & (x < 19)).sum()),\n",
    "\n",
    "#     ## Count the number of household members between 20 and 65 years old\n",
    "#     nbr_adults_20_65 = ('hhm_age', lambda x: ((x >= 20)  & (x < 65)).sum()),\n",
    "\n",
    "#     ## Count the number of household members 65 years old and over\n",
    "#     nbr_elderly_65_over = ('hhm_age', lambda x: (x >= 65).sum()),\n",
    "# ).reset_index()\n",
    "\n",
    "# Display the aggregated result\n",
    "#aggregated_result_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1\n",
    "df_r1_SE = (\n",
    "    df_r1_SE\n",
    "        # Merge the original dataframe with the aggregated counts of household members by age and sex\n",
    "        .merge(\n",
    "                df_r1_SE_gr\n",
    "                .agg(\n",
    "                    nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),  # Count of females in the household\n",
    "                    nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),  # Count of children under 5 years old\n",
    "                    nbr_yngchldrn_5_10=('hhm_age', lambda x: ((x >= 5) & (x < 10)).sum()),  # Count of children aged 5-10\n",
    "                    nbr_teenager_10_20=('hhm_age', lambda x: ((x >= 10) & (x < 20)).sum()),  # Count of teenagers aged 10-20\n",
    "                    nbr_adults_20_65=('hhm_age', lambda x: ((x >= 20) & (x < 65)).sum()),  # Count of adults aged 20-65\n",
    "                    nbr_elderly_65_over=('hhm_age', lambda x: (x >= 65).sum())  # Count of elderly aged 65 and over\n",
    "                )\n",
    "                .reset_index()  # Reset index to prepare for merging\n",
    "        )\n",
    "        # Merge with the count of females under 5 years old\n",
    "        .merge(\n",
    "            df_r1_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]<5)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_underfive'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 5-10\n",
    "        .merge(\n",
    "            df_r1_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=5) & (row[\"hhm_age\"] < 10)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_yngchldrn_5_10'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 10-20\n",
    "        .merge(\n",
    "            df_r1_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=10) & (row[\"hhm_age\"] < 20)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_teenager_10_20'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 20-65\n",
    "        .merge(\n",
    "            df_r1_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=20) & (row[\"hhm_age\"] < 65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_adults_20_65'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 65 and over\n",
    "        .merge(\n",
    "            df_r1_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_elderly_65_over'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    ")\n",
    "\n",
    "\n",
    "# Round 2\n",
    "\n",
    "df_r2_SE = (\n",
    "    df_r2_SE\n",
    "        # Merge the original dataframe with the aggregated counts of household members by age and sex\n",
    "        .merge(\n",
    "                df_r2_SE_gr\n",
    "                .agg(\n",
    "                    nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),  # Count of females in the household\n",
    "                    nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),  # Count of children under 5 years old\n",
    "                    nbr_yngchldrn_5_10=('hhm_age', lambda x: ((x >= 5) & (x < 10)).sum()),  # Count of children aged 5-10\n",
    "                    nbr_teenager_10_20=('hhm_age', lambda x: ((x >= 10) & (x < 20)).sum()),  # Count of teenagers aged 10-20\n",
    "                    nbr_adults_20_65=('hhm_age', lambda x: ((x >= 20) & (x < 65)).sum()),  # Count of adults aged 20-65\n",
    "                    nbr_elderly_65_over=('hhm_age', lambda x: (x >= 65).sum())  # Count of elderly aged 65 and over\n",
    "                )\n",
    "                .reset_index()  # Reset index to prepare for merging\n",
    "        )\n",
    "        # Merge with the count of females under 5 years old\n",
    "        .merge(\n",
    "            df_r2_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]<5)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_underfive'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 5-10\n",
    "        .merge(\n",
    "            df_r2_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=5) & (row[\"hhm_age\"] < 10)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_yngchldrn_5_10'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 10-20\n",
    "        .merge(\n",
    "            df_r2_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=10) & (row[\"hhm_age\"] < 20)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_teenager_10_20'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 20-65\n",
    "        .merge(\n",
    "            df_r2_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=20) & (row[\"hhm_age\"] < 65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_adults_20_65'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 65 and over\n",
    "        .merge(\n",
    "            df_r2_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_elderly_65_over'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    ")\n",
    "\n",
    "# Round 3\n",
    "df_r3_SE = (\n",
    "    df_r3_SE\n",
    "        # Merge the original dataframe with the aggregated counts of household members by age and sex\n",
    "        .merge(\n",
    "                df_r3_SE_gr\n",
    "                .agg(\n",
    "                    nbr_female=('hhm_sex', lambda x: (x == 'female').sum()),  # Count of females in the household\n",
    "                    nbr_underfive=('hhm_age', lambda x: (x < 5).sum()),  # Count of children under 5 years old\n",
    "                    nbr_yngchldrn_5_10=('hhm_age', lambda x: ((x >= 5) & (x < 10)).sum()),  # Count of children aged 5-10\n",
    "                    nbr_teenager_10_20=('hhm_age', lambda x: ((x >= 10) & (x < 20)).sum()),  # Count of teenagers aged 10-20\n",
    "                    nbr_adults_20_65=('hhm_age', lambda x: ((x >= 20) & (x < 65)).sum()),  # Count of adults aged 20-65\n",
    "                    nbr_elderly_65_over=('hhm_age', lambda x: (x >= 65).sum())  # Count of elderly aged 65 and over\n",
    "                )\n",
    "                .reset_index()  # Reset index to prepare for merging\n",
    "        )\n",
    "        # Merge with the count of females under 5 years old\n",
    "        .merge(\n",
    "            df_r3_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]<5)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_underfive'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 5-10\n",
    "        .merge(\n",
    "            df_r3_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=5) & (row[\"hhm_age\"] < 10)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_yngchldrn_5_10'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 10-20\n",
    "        .merge(\n",
    "            df_r3_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=10) & (row[\"hhm_age\"] < 20)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_teenager_10_20'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 20-65\n",
    "        .merge(\n",
    "            df_r3_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=20) & (row[\"hhm_age\"] < 65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_adults_20_65'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    "        # Merge with the count of females aged 65 and over\n",
    "        .merge(\n",
    "            df_r3_SE_gr\n",
    "            .apply(lambda row: ((row[\"hhm_sex\"]==\"female\") & (row[\"hhm_age\"]>=65)).sum(), include_groups=False)\n",
    "            .reset_index(name='nbr_female_elderly_65_over'), on=\"hh_id\", how=\"left\"\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hh_id', 'hhm_id', 'hhm_status', 'hhm_sex', 'hhm_age', 'relation_hhh',\n",
       "       'marital_status_hhm', 'literacy_hhm', 'education_high',\n",
       "       'curr_att_school', 'member_id', 'nbr_female', 'nbr_underfive',\n",
       "       'nbr_yngchldrn_5_10', 'nbr_teenager_10_20', 'nbr_adults_20_65',\n",
       "       'nbr_elderly_65_over', 'nbr_female_underfive',\n",
       "       'nbr_female_yngchldrn_5_10', 'nbr_female_teenager_10_20',\n",
       "       'nbr_female_adults_20_65', 'nbr_female_elderly_65_over'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 961,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_SE.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Interview status R3</th>\n",
       "      <th>Complete</th>\n",
       "      <th>Migrated</th>\n",
       "      <th>Not at home</th>\n",
       "      <th>Not found</th>\n",
       "      <th>Refused</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interview status R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Complete</th>\n",
       "      <td>5025</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "      <td>965</td>\n",
       "      <td>11</td>\n",
       "      <td>6224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Migrated</th>\n",
       "      <td>68</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not at home</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Refused</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>5117</td>\n",
       "      <td>346</td>\n",
       "      <td>24</td>\n",
       "      <td>1001</td>\n",
       "      <td>15</td>\n",
       "      <td>6503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Interview status R3  Complete  Migrated  Not at home  Not found  Refused   All\n",
       "Interview status R2                                                           \n",
       "Complete                 5025       200           23        965       11  6224\n",
       "Migrated                   68       137            1         31        0   237\n",
       "Not at home                17         8            0          4        1    30\n",
       "Refused                     7         1            0          1        3    12\n",
       "All                      5117       346           24       1001       15  6503"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_r1[\"interview_status_r2\"], df_r1[\"interview_status_r3\"], margins=True, rownames=[\"Interview status R2\"], colnames=[\"Interview status R3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merging with the crosswalk\n",
    "df_r1 = df_r1.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "df_r2 = df_r2.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "\n",
    "## merging to count the nbr of hhm\n",
    "df_r1 = df_r1.merge(df_r1_SE.groupby(['hh_id'], as_index=False).size(), how=\"left\", on=[\"hh_id\"]).rename(columns={'size':'hh_size'}) \n",
    "df_r2 = df_r2.merge(df_r2_SE.groupby(['hh_id'], as_index=False).size(), how=\"left\", on=[\"hh_id\"]).rename(columns={'size':'hh_size'}) \n",
    "df_r3 = df_r3.merge(df_r3_SE.groupby(['hh_id'], as_index=False).size(), how=\"left\", on=[\"hh_id\"]).rename(columns={'size':'hh_size'}) \n",
    "\n",
    "df_r1[\"hh_size\"] = df_r1[\"hh_size\"].astype(\"int64\", errors='ignore')\n",
    "df_r2[\"hh_size\"] = df_r2[\"hh_size\"].astype(\"int64\", errors='ignore')\n",
    "df_r3[\"hh_size\"] = df_r3[\"hh_size\"].astype(\"int64\", errors='ignore')\n",
    "\n",
    "\n",
    "# merging to collect the same informations in r1 and r3\n",
    "df_r3 = df_r3.merge(df_r1[['hh_id','district_name','upazila_name','union_name','union_code','code adm','affected_upazila']], \n",
    "                    right_on='hh_id', left_on='hh_id_parent', how='left', suffixes=('_left', '_right')).rename(columns={'hh_id_left':'hh_id', 'union_code_right':'union_code'}).drop(columns=['hh_id_right', 'union_code_left'], axis=1)\n",
    "\n",
    "\n",
    "df_r1[\"treat_group\"] = 0\n",
    "df_r2[\"treat_group\"] = df_r2.apply(lambda x: 1 if (x[\"affected_upazila\"]==1) & (x[\"interview_status\"]==\"Complete\") else 0 if (x[\"affected_upazila\"]==0) & (x[\"interview_status\"]==\"Complete\") else np.nan, axis=1)\n",
    "df_r3[\"treat_group\"] = df_r3.apply(lambda x: 1 if (x[\"affected_upazila\"]==1) & (x[\"interview_status\"]==\"Complete\") else 0 if (x[\"affected_upazila\"]==0) & (x[\"interview_status\"]==\"Complete\") else np.nan, axis=1)\n",
    "\n",
    "df_r1[\"survey_year\"] = 2011\n",
    "df_r2[\"survey_year\"] = 2015\n",
    "df_r3[\"survey_year\"] = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First merge to obtain the dataset for adults\n",
    "## Merging to obtains the anthro and the informations on age, sex and id\n",
    "df_r1_anthr1 = df_r1_anthr1.merge(df_r1_SE, on=['hh_id','hhm_id'], how='left')\n",
    "df_r2_anthr1 = df_r2_anthr1.merge(df_r2_SE, on=['hh_id','hhm_id'], how='left')\n",
    "df_r3_anthr1 = df_r3_anthr1.merge(df_r3_SE, on=['hh_id','hhm_id'], how='left')\n",
    "\n",
    "## Merging to obtains the information on the household localisation (division, district, upazilla, union, moza, village)\n",
    "df_r1_anthr1 = df_r1_anthr1.merge(df_r1, on=['hh_id'], how='left').drop([\"index_x\",\"index_y\",\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})\n",
    "df_r2_anthr1 = df_r2_anthr1.merge(df_r2, on=['hh_id'], how='left').drop([\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})\n",
    "df_r3_anthr1 = df_r3_anthr1.merge(df_r3, on=['hh_id'], how='left').drop([\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})\n",
    "\n",
    "\n",
    "# Second merge to obtain the dataset for children\n",
    "## Merging to obtains the anthro and the informations on age, sex and id\n",
    "df_r1_anthr2 = df_r1_anthr2.merge(df_r1_SE, on=['hh_id','hhm_id'], how='left')\n",
    "df_r2_anthr2 = df_r2_anthr2.merge(df_r2_SE, on=['hh_id','hhm_id'], how='left')\n",
    "df_r3_anthr2 = df_r3_anthr2.merge(df_r3_SE, on=['hh_id','hhm_id'], how='left')\n",
    "\n",
    "## Merging to obtains the information on the household localisation (division, district, upazilla, union, moza, village)\n",
    "df_r1_anthr2 = df_r1_anthr2.merge(df_r1, on=['hh_id'], how='left').drop([\"index_x\",\"index_y\",\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})\n",
    "df_r2_anthr2 = df_r2_anthr2.merge(df_r2, on=['hh_id'], how='left').drop([\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})\n",
    "df_r3_anthr2 = df_r3_anthr2.merge(df_r3, on=['hh_id'], how='left').drop([\"member_id_y\"], axis=1).rename(columns={\"member_id_x\":\"member_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing age in days by subtracting the birth date from the interview date\n",
    "df_r1_anthr2[\"age_days\"] = (df_r1_anthr2.date_int - df_r1_anthr2.date_birth).dt.days\n",
    "\n",
    "# Mapping household member sex to strings \"male\" and \"female\"\n",
    "df_r1_anthr2[\"hhm_sex\"] = df_r1_anthr2[\"hhm_sex\"].map({\"male\": \"male\", \"female\": \"female\"})\n",
    "\n",
    "df_r2_anthr1[\"hhm_sex\"] = df_r2_anthr1[\"hhm_sex\"].map({1: \"male\", 2: \"female\"})\n",
    "df_r2_anthr2[\"hhm_sex\"] = df_r2_anthr2[\"hhm_sex\"].map({1: \"male\", 2: \"female\"})\n",
    "\n",
    "df_r3_anthr1[\"hhm_sex\"] = df_r3_anthr1[\"hhm_sex\"].map({1: \"male\", 2: \"female\"})\n",
    "df_r3_anthr2[\"hhm_sex\"] = df_r3_anthr2[\"hhm_sex\"].map({1: \"male\", 2: \"female\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "New member (new born)                1674\n",
       "Previous and current round member     914\n",
       "Other reasons (permanent)             128\n",
       "Household merged/combined              47\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 966,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r2_anthr2.hhm_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "Previous and current round member                    18782\n",
       "Other reasons (permanent)                             1242\n",
       "New member through marriage                            609\n",
       "Household merged/combined                              107\n",
       "New member (new born)                                   18\n",
       "New member upon return from divorce or seperation       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 967,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_anthr1.hhm_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unborn children status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "if_not_measured_why\n",
       "have measured    2277\n",
       "absent             64\n",
       "refused             9\n",
       "sick                3\n",
       "other               1\n",
       "missing             0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 968,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_anthr2[\"if_not_measured_why\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 969,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_1 = df_r1.hh_id.values\n",
    "\n",
    "lst_2 = df_r2.hh_id_parent.values\n",
    "\n",
    "diff = list(set(lst_1) - set(lst_2))\n",
    "\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merging with the crosswalk\n",
    "#df_r1_anthr1 = df_r1_anthr1.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "#df_r2_anthr1 = df_r2_anthr1.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "#df_r3_anthr1 = df_r3_anthr1.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "\n",
    "## merging with the crosswalk\n",
    "#df_r1_anthr2 = df_r1_anthr2.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "#df_r2_anthr2 = df_r2_anthr2.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "#df_r3_anthr2 = df_r3_anthr2.merge(crosswalk, on=['upazila_code'], how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   count\n",
      "affected_upazila interview_status       \n",
      "0                Complete           3877\n",
      "                 Migrated            146\n",
      "                 Not at home          16\n",
      "                 Refused               4\n",
      "1                Complete           1350\n",
      "                 Migrated             54\n",
      "                 Not at home          10\n",
      "                 Refused               6\n"
     ]
    }
   ],
   "source": [
    "#### Status of households surveyed (migrated)\n",
    "print(df_r2[df_r2.sample_type!=2].drop_duplicates(subset=['hh_id_parent'], keep='first').groupby(['affected_upazila', 'interview_status'], observed=False)['hh_id_parent'].agg(['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Status of households surveyed (migrated)\n",
    "# df_r2[df_r2.interview_status==\"Completed\"].groupby(['affected_upazila','hh_split']).agg(nbr_splited=(\"hh_split\",\"count\"))\n",
    "#print(df_r2.groupby(['affected_upazila', 'interview_status'], observed=False)['hh_split'].agg(['count']))\n",
    "# pd.crosstab(df_r2.affected_upazila, df_r2.hh_split, margins=True, normalize=\"index\", rownames=['Affected upazila'], colnames=['Hh split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   count\n",
      "affected_upazila interview_status       \n",
      "0                Complete           4129\n",
      "                 Refused               9\n",
      "                 Not at home          14\n",
      "                 Migrated            271\n",
      "                 Partial               1\n",
      "1                Complete           1475\n",
      "                 Refused              13\n",
      "                 Not at home          10\n",
      "                 Migrated             89\n",
      "                 Partial               0\n"
     ]
    }
   ],
   "source": [
    "#### Status of households surveyed (migrated)\n",
    "print(df_r3.groupby(['affected_upazila', 'interview_status'], observed=False)['hh_id'].agg(['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the global dataframe (for the 3 survey round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "Previous and current round member                    18782\n",
       "Other reasons (permanent)                             1242\n",
       "New member through marriage                            609\n",
       "Household merged/combined                              107\n",
       "New member (new born)                                   18\n",
       "New member upon return from divorce or seperation       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 974,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_anthr1.hhm_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i keep only the hh with completed the survey and don't split during the study. Set remove the duplicates in the list\n",
    "lst_hhm_id_r3 = np.unique(np.concatenate((df_r3_anthr1[(df_r3_anthr1.interview_status==\"Complete\") & (df_r3_anthr1.hh_split==\"0\") & (df_r3_anthr1.hhm_age < 20) & (df_r3_anthr1.hhm_status==\"Previous and current round member\")][\"member_id\"].values, \n",
    "                                        df_r3_anthr2[(df_r3_anthr2.interview_status==\"Complete\") & (df_r3_anthr2.hh_split==\"0\") & (df_r3_anthr2.hhm_age >= 3) & (df_r3_anthr2.hhm_status==\"Previous and current round member\")][\"member_id\"].values)))\n",
    "\n",
    "\n",
    "# lst of underfive during the first or secind round\n",
    "## fist (underfive first round); second (under 12 first round) and underfive in round 2\n",
    "# 9238 children not splitted and before the natural disaster\n",
    "#lst_underfive_int =  np.unique(np.concatenate((df_r1_anthr2.member_id.values, \n",
    "#                                            df_r1_anthr1[df_r1_anthr1.hhm_age <= 12].member_id.values, \n",
    "#                                            df_r2_anthr2[(df_r2_anthr2.interview_status==\"Complete\") &  (df_r2_anthr2.hh_split==\"0\")].member_id.values)))\n",
    "lst_intvar = ['union_code','union_name','hh_id', 'hhm_id','weight_kg','height_cm','bmi' ,'if_not_measured_why', 'member_id','hhm_sex','hhm_status', 'hhm_age', 'relation_hhh', 'marital_status_hhm','hh_size',\n",
    "    'literacy_hhm', 'education_high', 'curr_att_school', 'hh_head_religion','hh_ethnic_group','nbr_female', 'nbr_underfive',\n",
    "    'nbr_yngchldrn_5_10', 'nbr_teenager_10_20', 'nbr_adults_20_65','nbr_elderly_65_over', 'nbr_female_underfive','nbr_female_yngchldrn_5_10', \n",
    "    'nbr_female_teenager_10_20','nbr_female_adults_20_65', 'nbr_female_elderly_65_over','sample_type', 'survey_round', 'code adm','affected_upazila']\n",
    "\n",
    "#df_r1_global = pd.concat([df_r1_anthr1[df_r1_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True), \n",
    "#                    df_r1_anthr2[df_r1_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True)\n",
    "\n",
    "#df_r2_global = pd.concat([df_r2_anthr1[df_r2_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True),\n",
    "#                    df_r2_anthr2[df_r2_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True)\n",
    "\n",
    "#df_r3_global = pd.concat([df_r3_anthr1[df_r3_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True),\n",
    "#                    df_r3_anthr2[df_r3_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True)\n",
    "\n",
    "df_global_corrected = pd.concat([pd.concat([df_r1_anthr1[df_r1_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True), \n",
    "                    df_r1_anthr2[df_r1_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True), \n",
    "\n",
    "                    pd.concat([df_r2_anthr1[df_r2_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True),\n",
    "                    df_r2_anthr2[df_r2_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True), \n",
    "                    \n",
    "                    pd.concat([df_r3_anthr1[df_r3_anthr1.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True),\n",
    "                    df_r3_anthr2[df_r3_anthr2.member_id.isin(lst_hhm_id_r3)][lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True)], ignore_index = True, verify_integrity=True)\n",
    "\n",
    "\n",
    "df_global = pd.concat([pd.concat([df_r1_anthr1[lst_intvar].reset_index(drop=True), \n",
    "                    df_r1_anthr2[lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True), \n",
    "\n",
    "                    pd.concat([df_r2_anthr1[lst_intvar].reset_index(drop=True),\n",
    "                    df_r2_anthr2[lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True), \n",
    "                    \n",
    "                    pd.concat([df_r3_anthr1[lst_intvar].reset_index(drop=True),\n",
    "                    df_r3_anthr2[lst_intvar].reset_index(drop=True)], ignore_index = True, verify_integrity=True)], ignore_index = True, verify_integrity=True)\n",
    "\n",
    "# treatment binary\n",
    "\n",
    "df_global[\"treat_group\"] = df_global.apply(lambda x: 1 if (x[\"survey_round\"]==\"3\") & (x[\"affected_upazila\"]==1) else 0, axis=1)\n",
    "df_global_corrected[\"treat_group\"] = df_global_corrected.apply(lambda x: 1 if (x[\"survey_round\"]==\"3\") & (x[\"affected_upazila\"]==1) else 0, axis=1)\n",
    "\n",
    "# survey year\n",
    "df_global[\"survey_year\"] = df_global.apply(lambda x: 2018 if x[\"survey_round\"]==\"3\" else 2015 if x[\"survey_round\"]==\"2\" else 2011, axis=1)\n",
    "df_global_corrected[\"survey_year\"] = df_global_corrected.apply(lambda x: 2018 if x[\"survey_round\"]==\"3\" else 2015 if x[\"survey_round\"]==\"2\" else 2011, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data on FTF additional and National Representative data was collected during the third round of the BIHS. Data on FTF original was not collected, so i remove then from the previous rounds of the BIHS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhm_status\n",
       "Previous and current round member                    18782\n",
       "Other reasons (permanent)                             1242\n",
       "New member through marriage                            609\n",
       "Household merged/combined                              107\n",
       "New member (new born)                                   18\n",
       "New member upon return from divorce or seperation       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 976,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r3_anthr1.hhm_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=df_global.groupby([\"member_id\"], observed=False)[\"survey_round\"].agg([\"sum\"])\n",
    "#test[].shape()\n",
    "test[\"sum\"] = test[\"sum\"].astype(\"int64\")\n",
    "testview= test[ test[\"sum\"] >= 4]\n",
    "#print(df_r3.groupby(['affected_upazila', 'interview_status'], observed=False)['hh_id'].agg(['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_global.groupby([\"survey_round\"], observed=False).size())\n",
    "\n",
    "df_r1_anthr1[lst_intvar]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing to obtain exposure information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Importing other shapefiles\n",
    "\n",
    "# crosswalk\n",
    "\n",
    "crosswalk = pd.read_excel(os.getcwd() + '\\\\input\\\\crosswalk\\\\crosswalk_union.xlsx', usecols=\"A:G\", dtype={'union_name':'str','union_code_r1':'str','union_code_r2':'str','union_code_r3':'str',\n",
    "                                                                                        'code_union_adm':'str','union_name_adm':'str','surveyed':'str'}) \n",
    "\n",
    "\n",
    "# Importing the Bangladesh raw map: Reading a shapefile containing administrative boundaries of Bangladesh\n",
    "bgd_adm = gpd.read_file(os.getcwd() + '\\\\input\\\\shapefile_data\\\\shapefile_zip\\\\gadm41_BGD_shp\\\\gadm41_BGD_4.shp')\n",
    "\n",
    "\n",
    "## Raining data\n",
    "raining_dta = pd.read_stata(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\BGD4_mswep_2010_2019.dta', \n",
    "                        columns=[\"GID_4\",\"M2017140\",\"M2017141\",\"M2017142\",\"M2017143\",\"M2017144\",\"M2017145\",\"M2017146\",\"M2017147\",\n",
    "                                \"M2017148\",\"M2017149\",\"M2017150\",\"M2017151\"])\n",
    "\n",
    "## NTL data\n",
    "ntl_dta = pd.read_stata(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\BGD4_ntl_bm_2013_2019.dta', \n",
    "                        columns=[\"gid_4\",\"n2017_05_29\",\"n2017_05_30\",\"n2017_05_31\",\"n2017_06_01\"])\n",
    "\n",
    "## Pop density data\n",
    "popdens_dta = pd.read_csv(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\bgd4_pop_landscan_2000_2020.csv',sep=\";\", usecols=[\"GID_4\",\"landscan_global_2017\"])\n",
    "\n",
    "## Storm data\n",
    "exposure_time_mora_W_N = DBF(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\stats_stormR\\\\exposure_msw_mora_W_N.dbf', load=True)\n",
    "speed_mora_W_N = DBF(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\stats_stormR\\\\msw_mora_W_N.dbf', load=True)\n",
    "#mora_raster_W_N = rio.open(os.getcwd() + '\\\\input\\\\shapefile_data\\\\other data\\\\bgd\\\\bgd\\\\msw_stormR_18_tif\\\\msw_mora_W_Np.tif')\n",
    "\n",
    "\n",
    "# \n",
    "exposure_time_mora_W_N = pd.DataFrame(iter(exposure_time_mora_W_N))\n",
    "speed_mora_W_N = pd.DataFrame(iter(speed_mora_W_N))\n",
    "\n",
    "#rename columns\n",
    "exposure_time_mora_W_N = exposure_time_mora_W_N[[\"GID_4\",\"COUNT\",\"AREA\",\"MIN\",\"MAX\",\"MEAN\"]]\n",
    "speed_mora_W_N = speed_mora_W_N[[\"GID_4\",\"COUNT\",\"AREA\",\"MIN\",\"MAX\",\"MEAN\"]]\n",
    "\n",
    "exposure_time_mora_W_N.rename(columns={\"COUNT\":\"count_time\",\"AREA\":\"area_time\",\"MIN\":\"min_time\",\"MAX\":\"max_time\",\"MEAN\":\"mean_time\"}, inplace=True)\n",
    "speed_mora_W_N.rename(columns={\"COUNT\":\"count_speed\",\"AREA\":\"area_speed\",\"MIN\":\"min_speed\",\"MAX\":\"max_speed\",\"MEAN\":\"mean_speed\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "## Merging with the crosswalk to get surveyed unions\n",
    "\n",
    "bgd_adm =(\n",
    "    crosswalk\n",
    "    .merge(bgd_adm, how='right', left_on=\"code_union_adm\", right_on=\"GID_4\")\n",
    "    .merge(raining_dta, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    .merge(ntl_dta, how='left', left_on=\"GID_4\", right_on=\"gid_4\")\n",
    "    .merge(popdens_dta, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    .merge(exposure_time_mora_W_N, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    .merge(speed_mora_W_N, how='left', left_on=\"GID_4\", right_on=\"GID_4\")\n",
    "    )\n",
    "\n",
    "\n",
    "bgd_adm[\"surveyed\"] = bgd_adm[\"surveyed\"].fillna(\"0\").replace({'1':'Surveyed union','0':'Unsurveyed union'}).astype(str)\n",
    "bgd_adm[\"union_code_r1\"] = bgd_adm[\"union_code_r1\"].astype(str)\n",
    "bgd_adm[\"union_name\"] = bgd_adm[\"union_name\"].astype(str)\n",
    "\n",
    "bgd_adm[\"average_rain\"] = bgd_adm[[\"M2017148\",\"M2017149\",\"M2017150\"]].aggregate(\"mean\",axis=\"columns\")\n",
    "bgd_adm[\"average_ntl\"] = bgd_adm[[\"n2017_05_29\",\"n2017_05_30\",\"n2017_05_31\"]].aggregate(\"mean\",axis=\"columns\")\n",
    "\n",
    "\n",
    "\n",
    "bgd_adm = gpd.GeoDataFrame(bgd_adm, geometry='geometry')\n",
    "\n",
    "# Building quantiles for highly exposed and unexposed regions\n",
    "quantiles_high = np.nanquantile(bgd_adm[bgd_adm['mean_speed']>0].mean_speed, [i * 0.25 + 0.25 for i in range(4)]) \n",
    "#rounded_quantiles = np.array([int(np.around(i, 0)) for i in quantiles])\n",
    "\n",
    "bgd_adm[\"high_exposed\"] = bgd_adm[\"mean_speed\"].apply(lambda row: \"1\" if row >= int(quantiles_high[2]) else \"0\")\n",
    "bgd_adm[\"exposed_at_least_zero\"] = bgd_adm[\"mean_speed\"].apply(lambda row: \"1\" if row > 0 else \"0\")\n",
    "\n",
    "# removing already existing object for an efficient usage of the computer memory\n",
    "\n",
    "del raining_dta\n",
    "del ntl_dta\n",
    "del popdens_dta\n",
    "del exposure_time_mora_W_N\n",
    "del speed_mora_W_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging adm data with survey data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1 data\n",
    "df_r1 = df_r1.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r1_anthr1 = df_r1_anthr1.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r1_anthr2 = df_r1_anthr2.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "# test = df_r3.merge(\n",
    "#         bgd_adm[['union_name_r1', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "#         'high_exposed', 'exposed_at_least_zero']],\n",
    "#         how='left',\n",
    "#         left_on=['union_name', 'union_code'],\n",
    "#         right_on=['union_name_r1', 'union_code_r1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Round 2 data\n",
    "df_r2 = df_r2.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r2_anthr1 = df_r2_anthr1.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r2_anthr2 = df_r2_anthr2.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "\n",
    "# Round 3 data\n",
    "df_r3 = df_r3.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r3_anthr1 = df_r3_anthr1.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "df_r3_anthr2 = df_r3_anthr2.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1', 'code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "\n",
    "# Correct a mistake in the name\n",
    "#df_global_corrected['union_name'] = df_global_corrected['union_name'].replace({\"Bhayna  (Sujanagar)\":\"Bhayna\"})\n",
    "\n",
    "df_global_corrected = df_global_corrected.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1','code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "\n",
    "df_global = df_global.merge(\n",
    "        bgd_adm[['union_name', 'union_code_r1','code_union_adm', 'union_name_adm',\n",
    "        'high_exposed', 'exposed_at_least_zero']],\n",
    "        how='left',\n",
    "        left_on=['union_name', 'union_code'],\n",
    "        right_on=['union_name', 'union_code_r1'])\n",
    "\n",
    "# bgd adm information cleaning\n",
    "bgd_adm[\"union_name_adm\"] = bgd_adm[\"union_name_adm\"].astype(str)\n",
    "bgd_adm[\"union_name\"] = bgd_adm[\"union_name\"].astype(str)\n",
    "bgd_adm[\"union_code_r1\"] = bgd_adm[\"union_code_r1\"].astype(str)\n",
    "bgd_adm[\"union_code_r2\"] = bgd_adm[\"union_code_r2\"].astype(str)\n",
    "bgd_adm[\"union_code_r3\"] = bgd_adm[\"union_code_r3\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcting union_name before exporting\n",
    "\n",
    "df_global_corrected['union_name'] = df_global_corrected['union_name'].astype(str)\n",
    "#df_global_corrected['union_name_r1'] = df_global_corrected['union_name_r1'].astype(str)\n",
    "\n",
    "\n",
    "df_global['union_name'] = df_global['union_name'].astype(str)\n",
    "#df_global['union_name_r1'] = df_global['union_name_r1'].astype(str)\n",
    "\n",
    "\n",
    "df_global['union_name'] = df_global['union_name'].astype(str)\n",
    "#df_global['union_name_r1'] = df_global['union_name_r1'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Saving the processed dataframes to Excel files\n",
    "\n",
    "## Anthro for > 5 hhm\n",
    "df_r1_anthr1.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r1_overfive.xlsx\", index=False)\n",
    "df_r2_anthr1.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r2_overfive.xlsx\", index=False)\n",
    "df_r3_anthr1.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r3_overfive.xlsx\", index=False)\n",
    "\n",
    "## Anthro for <= 5 hhm\n",
    "df_r1_anthr2.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r1_underfive.xlsx\", index=False)\n",
    "df_r2_anthr2.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r2_underfive.xlsx\", index=False)\n",
    "df_r3_anthr2.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r3_underfive.xlsx\", index=False)\n",
    "\n",
    "## Hh level data  \n",
    "df_r1.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r1_hh.xlsx\", index=False)\n",
    "df_r2.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r2_hh.xlsx\", index=False)\n",
    "df_r3.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\r3_hh.xlsx\", index=False)\n",
    "\n",
    "## Concatenated data\n",
    "\n",
    "df_global_corrected.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\df_global_corrected.xlsx\", index=False)\n",
    "df_global.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\df_global.xlsx\", index=False)\n",
    "bgd_adm.to_file(os.getcwd() + \"\\\\output\\\\data\\\\bgd_data.shp\")\n",
    "# Release memory using del\n",
    "del crosswalk\n",
    "del df_r1\n",
    "del df_r1_f\n",
    "del df_r1_ftf\n",
    "del df_r1_SE\n",
    "del df_r2\n",
    "del df_r2_f\n",
    "del df_r2_SE\n",
    "del df_r3\n",
    "del df_r3_f\n",
    "del df_r3_SE\n",
    "del df_global\n",
    "del df_global_corrected\n",
    "\n",
    "del df_r1_anthr1\n",
    "del df_r1_anthr2\n",
    "del df_r2_anthr1\n",
    "del df_r2_anthr2\n",
    "del df_r3_anthr1\n",
    "del df_r3_anthr2\n",
    "\n",
    "del nbr_max\n",
    "del label_to_int\n",
    "del lst_1\n",
    "del lst_2\n",
    "del diff\n",
    "del lst_complete\n",
    "del lst_migrate\n",
    "del lst_refused\n",
    "del lst_nthome\n",
    "del lst_hhm_id_r3\n",
    "del lst_intvar\n",
    "del lst_split\n",
    "del test\n",
    "del testview\n",
    "\n",
    "del df_r1_SE_gr\n",
    "del df_r2_SE_gr\n",
    "del df_r3_SE_gr\n",
    "# Only non nan values\n",
    "#df_r1_anthr2=df_r1_anthr2[(df_r1_anthr2.weight_kg.notnull()) & (df_r1_anthr2.height_cm.notnull()) & (df_r1_anthr2.age_days.notnull())]\n",
    "# df_r2_anthr2=df_r2_anthr2[(df_r2_anthr2.weight_kg.notnull()) & (df_r2_anthr2.height_cm.notnull()) & (df_r2_anthr2.age_days.notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Computing Zscores for underfive children\n",
    "#df_r1_anthr2[\"WAZ\"] = df_r1_anthr2.apply(lambda row: float(Observation(sex=row[\"sex\"], age_in_days=row[\"age_days\"]).weight_for_age(Decimal(str(row[\"weight_kg\"])))), axis=1)\n",
    "#df_r1_anthr2[\"HAZ\"] = df_r1_anthr2.apply(lambda row: float(Observation(sex=row[\"sex\"], age_in_days=row[\"age_days\"]).length_or_height_for_age(Decimal(str(row[\"height_cm\"])))), axis=1)\n",
    "#df_r1_anthr2[\"BMI\"] = df_r1_anthr2.apply(lambda row: round(row.weight_kg/((row.height_cm/100)**2),3), axis=1)\n",
    "\n",
    "\n",
    "#df_r2_anthr2[\"WAZ\"] = df_r2_anthr2.apply(lambda row: float(Observation(sex=row[\"sex\"], age_in_days=row[\"age_days\"]).weight_for_age(Decimal(str(row[\"weight_kg\"])))), axis=1)\n",
    "#df_r2_anthr2[\"HAZ\"] = df_r2_anthr2.apply(lambda row: float(Observation(sex=row[\"sex\"], age_in_days=row[\"age_days\"]).length_or_height_for_age(Decimal(str(row[\"height_cm\"])))), axis=1)\n",
    "#df_r2_anthr2[\"BMI\"] = df_r2_anthr2.apply(lambda row: round(row.weight_kg/((row.height_cm/100)**2),3), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Now that we have an understanding of the business problem we want to solve, and we have loaded the datasets, the next step to follow is to have a better understanding of the dataset, i.e., what is the distribution of the variables, what are different relationships that exist between variables, etc. If there are any data anomalies like missing values or outliers, how do we treat them to prepare the dataset for building the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#outcome variable \n",
    "outcomes_cols = ['HWHAZWHO', 'HWWAZWHO', 'HWWHZWHO','HWBMIZWHO']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[871], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Summarize data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Summarize data\n",
    "df.describe().T\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.YEAR.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Set labels’ font parameters\n",
    "sns.set_style(\"whitegrid\")\n",
    "font_color = '#525252'\n",
    "csfont = {'fontname':'Georgia'}\n",
    "hfont = {'fontname':'Calibri'}\n",
    "facecolor = '#eaeaf2'\n",
    "\n",
    "g1 = sns.boxplot(data=df, x=\"YEAR\", y=\"HWHAZWHO\", hue=\"KIDSEX\")\n",
    "\n",
    "g1.set_xlabel(\"DHS Year\", fontsize=10, color=font_color, **hfont)\n",
    "g1.set_ylabel(\"Height for Age Z-score\", fontsize=10, color=font_color, **hfont)\n",
    "\n",
    "\n",
    "\n",
    "#title = 'Average Foreign Exchange Rates, 2000–2019'\n",
    "#fig.suptitle(title, y=.97, fontsize=22, color=font_color, **csfont)\n",
    "#subtitle = 'Source: Author own calculation'\n",
    "#plt.title(subtitle, fontsize=18, pad=10, color=font_color, **hfont)\n",
    "#plt.subplots_adjust(top=0.85)\n",
    "\n",
    "# title\n",
    "#new_title = ''\n",
    "#g1.legend.set_title(new_title)\n",
    "# replace labels\n",
    "# new_labels = ['Urban 555', 'Rural']\n",
    "# for t, l in zip(g1.texts, new_labels):\n",
    "#     t.set_text(l)\n",
    "plt.legend(labels= ['Male', 'Female'], loc = (1.04,0.85))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "filename = 'Height for Age Z-score dist'\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg', facecolor=facecolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = sns.boxplot(data=df, x=\"YEAR\", y=\"HWWAZWHO\", hue=\"KIDSEX\")\n",
    "\n",
    "g2.set_xlabel(\"DHS Year\", fontsize=10, color=font_color, **hfont)\n",
    "g2.set_ylabel(\"Weight for Age Z-score\", fontsize=10, color=font_color, **hfont)\n",
    "\n",
    "\n",
    "plt.legend(labels= ['Male', 'Female'], loc = (1.04,0.85))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "filename = 'Weight for Age Z-score dist'\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg', facecolor=facecolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = sns.boxplot(data=df, x=\"YEAR\", y=\"HWWHZWHO\", hue=\"KIDSEX\")\n",
    "g3.set_xlabel(\"DHS Year\", fontsize=10, color=font_color, **hfont)\n",
    "g3.set_ylabel(\"Weight for Height Z-score\", fontsize=10, color=font_color, **hfont)\n",
    "\n",
    "\n",
    "plt.legend(labels= ['Male', 'Female'], loc = (1.04,0.85))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "filename = 'Weight for Height Z-score dist'\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg', facecolor=facecolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g4 = sns.boxplot(data=df, x=\"YEAR\", y=\"HWBMIZWHO\", hue=\"KIDSEX\")\n",
    "g4.set_xlabel(\"DHS Year\", fontsize=10, color=font_color, **hfont)\n",
    "g4.set_ylabel(\"Body Mass Index Z-score\", fontsize=10, color=font_color, **hfont)\n",
    "\n",
    "\n",
    "plt.legend(labels= ['Male', 'Female'], loc = (1.04,0.85))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "filename = 'Body Mass Index Z-score dist'\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg', facecolor=facecolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=['Height for Age Z-score','Weight for Age Z-score','Weight for Height Z-score','Body Mass Index Z-score']\n",
    "df[['HWHAZWHO', 'HWWAZWHO', 'HWWHZWHO','HWBMIZWHO']].hist(figsize=(12,12))\n",
    "\n",
    "#plt.title(title, fontsize=18, pad=10, color=font_color, **hfont)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g5 = sns.kdeplot(data=df, x=\"HWHAZWHO\", hue=\"KIDSEX\")\n",
    "g5.set_xlabel(\"Height for Age Z-score \", fontsize=10, color=font_color, **hfont)\n",
    "\n",
    "\n",
    "plt.legend(labels= ['Male', 'Female'], loc = (1.04,0.85))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "filename = 'Body Mass Index Z-score dist'\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg', facecolor=facecolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g10 = sns.FacetGrid(data=df, col='YEAR', hue='KIDSEX', col_wrap=3)\n",
    "\n",
    "#g10.map(sns.kdeplot, x=\"HWWAZWHO\", cut=0, fill=True,  alpha=1,  clip=(-5, 5))\n",
    "# g5.set_xlabel(\"Weight for Age Z-score\", fontsize=10, color=font_color, **hfont)\n",
    "\n",
    "\n",
    "# plt.legend(labels= ['Male', 'Female'], loc = (1.04,0.85))\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "g5 = sns.displot(data=df, col='YEAR', hue='KIDSEX', x='HWWAZWHO', kind='kde', col_wrap=3,fill=True, facet_kws={'sharey': False, 'sharex': False})\n",
    "# control the title of each facet\n",
    "#g5 = g5.set_titles(\"{col_name}\")\n",
    "#g5.set_xlabel(\"Weight for Age Z-score\", fontsize=10, color=font_color, **hfont)\n",
    "plt.legend(labels= ['Male', 'Female'], loc = (1.04,0.85))\n",
    "filename = 'Weight for Age Z-score density'\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg', facecolor=facecolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g5 = sns.kdeplot(data=df, x='HWWHZWHO', hue=\"KIDSEX\")\n",
    "g5.set_xlabel(\"Height for Age Z-score \", fontsize=10, color=font_color, **hfont)\n",
    "\n",
    "\n",
    "plt.legend(labels= ['Male', 'Female'], loc = (1.04,0.85))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "filename = 'Weight for Height Z-score density'\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg', facecolor=facecolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g5 = sns.kdeplot(data=df, x='HWBMIZWHO', hue=\"KIDSEX\")\n",
    "g5.set_xlabel(\"Height for Age Z-score \", fontsize=10, color=font_color, **hfont)\n",
    "\n",
    "\n",
    "plt.legend(labels= ['Male', 'Female'], loc = (1.04,0.85))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "filename = 'Body Mass Index Z-score density'\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg', facecolor=facecolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation graphique des regions du Bangladesh\n",
    "\n",
    "## fig et ax représentent la base de la carte\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "bgd_adm.apply(lambda x: ax.annotate(text=x['NAME_2'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1)\n",
    "bgd_plot=bgd_adm.plot(ax=ax, color='lightgrey', edgecolor='black',lw=0.7, alpha=0.45)\n",
    "\n",
    "# Sans axe \n",
    "ax.axis('off')\n",
    "\n",
    "# using set_facecolor() method\n",
    "ax.set_facecolor(\"red\")\n",
    "\n",
    "# Rajouter le titre\n",
    "ax.set_title('', fontsize=23)\n",
    "\n",
    "# Rajouter les coordonnées GPS des points sur la carte\n",
    "csm = shp_2007.plot(ax=bgd_plot, column='DHSREGNA', cmap='Paired', markersize=20, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_2004 = shp_2004.drop(shp_2004[(shp_2004['LATNUM']==0) & (shp_2004['LONGNUM']==0)].index, axis=0)\n",
    "shp_2014 = shp_2014.drop(shp_2014[(shp_2014['LATNUM']==0) & (shp_2014['LONGNUM']==0)].index, axis=0)\n",
    "\n",
    "# resetting the DataFrame index\n",
    "shp_2004 = shp_2004.reset_index()\n",
    "shp_2014 = shp_2014.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building and id for each\n",
    "\n",
    "#bgd_adm.assign(ID = str(bgd_adm[\"ID_1\"] + bgd_adm[\"ID_2\"]))\n",
    "\n",
    "bgd_adm[\"ID\"] = bgd_adm[\"ID_1\"].astype(\"str\") + bgd_adm[\"ID_2\"].astype(\"str\")\n",
    "bgd_adm[\"ID\"] = bgd_adm[\"ID\"].astype(\"str\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the most exposed district names\n",
    "housing_dmg = housing_dmg.assign(damaged_hh = housing_dmg[\"Fully Damaged Households\"] / housing_dmg[\"District Total Population 2007 est.\"])\n",
    "# list of the most exposed district names in the list \n",
    "housing_dmg_2 = housing_dmg_2.assign(damaged_hh = housing_dmg_2[\"No. of Affected People\"] / housing_dmg_2[\"District Total Population 2007 est.\"])\n",
    "#lst_mst_affctd = housing_dmg[housing_dmg[\"Fully Damaged Households\"] >= housing_dmg[\"Fully Damaged Households\"].quantile(1/5)][\"District corrected\"].astype(\"str\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly specifying the columns for joining\n",
    "merged = bgd_adm.merge(housing_dmg, left_on='ID', right_on='ID_adm', how='left')\n",
    "\n",
    "# Replace na by 0\n",
    "\n",
    "merged[\"Fully Damaged Households\"] = merged[\"Fully Damaged Households\"].fillna(0)\n",
    "merged[\"damaged_hh\"] = merged[\"damaged_hh\"].fillna(0)\n",
    "# Représentation graphique des regions du Bangladesh\n",
    "\n",
    "## fig et ax représentent la base de la carte\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(17.8,8))\n",
    "cmap = 'Greys'\n",
    "merged.plot(column=\"Fully Damaged Households\", scheme=\"quantiles\", k=8, cmap=cmap, ax=ax[0], edgecolor='0.8', legend=True,legend_kwds={\"loc\": \"center left\", \"bbox_to_anchor\": (1, 0.5), \"interval\": False})\n",
    "merged.plot(column=\"damaged_hh\", scheme=\"quantiles\", k=8, cmap=cmap, ax = ax[1], edgecolor='0.8', legend=True,legend_kwds={\"loc\": \"center left\", \"bbox_to_anchor\": (1, 0.5), \"interval\": False})\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[0].set_title('Number of Fully Damaged Households', fontdict={'fontsize': '15'})\n",
    "ax[1].set_title('Number of Fully Damaged Households per capita', fontdict={'fontsize': '15'})\n",
    "# Create an annotation for the data source\n",
    "#ax.annotate('Government of Bangladesh Report (2008)',xy=(0, 0))\n",
    "#,xy=(0.1, .08),  xycoords='figure fraction', horizontalalignment='left', verticalalignment='bottom', fontsize=12, color='#555555'\n",
    "# Create colorbar as a legend\n",
    "#sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=merged[\"Fully Damaged Households\"].astype(\"int64\").min(), vmax=merged[\"Fully Damaged Households\"].astype(\"int64\").max()))\n",
    "\n",
    "# Empty array for the data range\n",
    "#sm._A = []\n",
    "# Add the colorbar to the figure\n",
    "#cbaxes = fig.add_axes([0.15, 0.25, 0.01, 0.4])\n",
    "#cbar = fig.colorbar(sm, cax=cbaxes)\n",
    "filename=\"fully_damaged_hh\"\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly specifying the columns for joining\n",
    "merged_2 = bgd_adm.merge(housing_dmg_2, left_on='ID', right_on='ID_adm', how='left')\n",
    "\n",
    "# Replace na by 0\n",
    "merged_2[\"No. of Affected People\"] = merged_2[\"No. of Affected People\"].fillna(0)\n",
    "merged_2[\"damaged_hh\"] = merged_2[\"damaged_hh\"].fillna(0)\n",
    "# Représentation graphique des regions du Bangladesh\n",
    "# Représentation graphique des regions du Bangladesh\n",
    "\n",
    "## fig et ax représentent la base de la carte\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(17.8,8))\n",
    "cmap = 'Greys'\n",
    "merged_2.plot(column=\"No. of Affected People\", scheme=\"quantiles\", k=7, cmap=cmap, ax = ax[0], edgecolor='0.8', legend=True,legend_kwds={\"loc\": \"center left\", \"bbox_to_anchor\": (1, 0.5), \"interval\": False})\n",
    "merged_2.plot(column=\"damaged_hh\", scheme=\"quantiles\", k=7, cmap=cmap, ax = ax[1], edgecolor='0.8', legend=True,legend_kwds={\"loc\": \"center left\", \"bbox_to_anchor\": (1, 0.5), \"interval\": False})\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('No. of Affected People', fontdict={'fontsize': '15'})\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('No. of Affected People per capita', fontdict={'fontsize': '15'})\n",
    "# Create an annotation for the data source\n",
    "#ax.annotate('Ministry of Food and Disaster Management (2007)',xy=(0, 0))\n",
    "\n",
    "filename=\"affected_people_hh\"\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mapclassify as mc\n",
    "# import geoplot as gplt\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
    "# scheme = mc.Quantiles(merged['Fully Damaged Households'], k=6)\n",
    "\n",
    "# # Map\n",
    "# gplt.choropleth(merged, \n",
    "#     hue=\"Fully Damaged Households\", \n",
    "#     linewidth=.1,\n",
    "#     scheme=scheme, cmap='inferno_r',\n",
    "#     legend=True,\n",
    "#     edgecolor='black',\n",
    "#     ax=ax\n",
    "# );\n",
    "\n",
    "# ax.set_title('Unemployment rate in US counties', fontsize=13);\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# merged.plot(ax=ax,column='Fully Damaged Households',cmap='OrRd',scheme='quantiles',legend=True,legend_kwds=dict(loc='upper right',bbox_to_anchor=(1.5, .9),fontsize='small',title=\"Legend\",frameon=False))\n",
    "# ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged.loc[merged.index.get_loc(merged[\"Fully Damaged Households\"].fillna(0).astype(\"int64\").idxmin()) - 1, \"Fully Damaged Households\"]\n",
    "merged = merged.sort_values(by=\"Fully Damaged Households\", ascending=True)\n",
    "merged.iloc[merged.index.get_loc(merged[\"Fully Damaged Households\"].astype(\"int64\").idxmin()) + 1]\n",
    "\n",
    "merged[merged[\"District corrected\"]==\"Narshingdi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bgd_adm.NAME_2.sort_values(ascending=True).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data={'% of Missing Values':round(merged.isna().sum()/merged.isna().count()*100,2),'Nbr of Missing Values':merged.isna().sum()}).sort_values(by='% of Missing Values',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[['District Total Population 2007 est.','Total Households', 'Total Vulnerable Households','Fully Damaged Households', 'Partially Damaged Households','Fully Damaged Vulnerable Households','Partially Damaged Vulnerable Households','Fully Damaged Non‐ vulnerable Households','Partially Damaged Non‐ Vulnerable Households']] = merged[['District Total Population 2007 est.','Total Households', 'Total Vulnerable Households','Fully Damaged Households', 'Partially Damaged Households','Fully Damaged Vulnerable Households','Partially Damaged Vulnerable Households','Fully Damaged Non‐ vulnerable Households','Partially Damaged Non‐ Vulnerable Households']].replace(np.NaN, '0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the city, state, and country names from latitude and longitude using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_1999[\"city\"]=\"\"\n",
    "shp_1999[\"village\"]=\"\"\n",
    "shp_1999[\"county\"]=\"\"\n",
    "shp_1999[\"state_district\"]=\"\"\n",
    "shp_1999[\"state\"]=\"\"\n",
    "shp_1999[\"country\"]=\"\"\n",
    "\n",
    "shp_2004[\"city\"]=\"\"\n",
    "shp_2004[\"village\"]=\"\"\n",
    "shp_2004[\"county\"]=\"\"\n",
    "shp_2004[\"state_district\"]=\"\"\n",
    "shp_2004[\"state\"]=\"\"\n",
    "shp_2004[\"country\"]=\"\"\n",
    "\n",
    "shp_2007[\"city\"]=\"\"\n",
    "shp_2007[\"village\"]=\"\"\n",
    "shp_2007[\"county\"]=\"\"\n",
    "shp_2007[\"state_district\"]=\"\"\n",
    "shp_2007[\"state\"]=\"\"\n",
    "shp_2007[\"country\"]=\"\"\n",
    "\n",
    "shp_2011[\"city\"]=\"\"\n",
    "shp_2011[\"village\"]=\"\"\n",
    "shp_2011[\"county\"]=\"\"\n",
    "shp_2011[\"state_district\"]=\"\"\n",
    "shp_2011[\"state\"]=\"\"\n",
    "shp_2011[\"country\"]=\"\"\n",
    "\n",
    "shp_2014[\"city\"]=\"\"\n",
    "shp_2014[\"village\"]=\"\"\n",
    "shp_2014[\"county\"]=\"\"\n",
    "shp_2014[\"state_district\"]=\"\"\n",
    "shp_2014[\"state\"]=\"\"\n",
    "shp_2014[\"country\"]=\"\"\n",
    "\n",
    "shp_2017[\"city\"]=\"\"\n",
    "shp_2017[\"village\"]=\"\"\n",
    "shp_2017[\"county\"]=\"\"\n",
    "shp_2017[\"state_district\"]=\"\"\n",
    "shp_2017[\"state\"]=\"\"\n",
    "shp_2017[\"country\"]=\"\"\n",
    "\n",
    "\n",
    "# Create the address columns in a single operation for efficiency\n",
    "# shp_1999['city', 'village', 'county', 'state_district', 'state', 'country']=['','','','','','']\n",
    "# shp_2004['city', 'village', 'county', 'state_district', 'state', 'country']=[\"\",\"\",\"\",\"\",\"\",\"\"]\n",
    "# shp_2007['city', 'village', 'county', 'state_district', 'state', 'country']=[\"\",\"\",\"\",\"\",\"\",\"\"]\n",
    "# shp_2011['city', 'village', 'county', 'state_district', 'state', 'country']=[\"\",\"\",\"\",\"\",\"\",\"\"]\n",
    "# shp_2014['city', 'village', 'county', 'state_district', 'state', 'country']=[\"\",\"\",\"\",\"\",\"\",\"\"]\n",
    "# shp_2017['city', 'village', 'county', 'state_district', 'state', 'country']=[\"\",\"\",\"\",\"\",\"\",\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get district related to each cluster\n",
    "get_local_information(shp_1999, 10)\n",
    "get_local_information(shp_2004, 10)\n",
    "get_local_information(shp_2007, 10)\n",
    "get_local_information(shp_2011, 10)\n",
    "get_local_information(shp_2014, 10)\n",
    "get_local_information(shp_2017, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(shp_2004)):\n",
    "#     print(i)\n",
    "\n",
    "shp_2004.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(shp_2017.state_district.value_counts(normalize=True).to_markdown())\n",
    "\n",
    "temp1 = [str(element).replace(' District','').replace(' Hill','') for element in shp_2007.state_district.unique()]\n",
    "temp2 = bgd_adm.NAME_2.unique()\n",
    "\n",
    "list(set(temp1) - set(temp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shp_1999['state_district'] = shp_1999['state_district'].replace(' District','').replace(' Hill','') \n",
    "# shp_2004['state_district'] = shp_2004['state_district'].replace(' District','').replace(' Hill','') \n",
    "# shp_2007['state_district'] = shp_2007['state_district'].replace(' District','').replace(' Hill','') \n",
    "# shp_2011['state_district'] = shp_2011['state_district'].replace(' District','').replace(' Hill','') \n",
    "# shp_2014['state_district'] = shp_2014['state_district'].replace(' District','').replace(' Hill','') \n",
    "# shp_2017['state_district'] = shp_2017['state_district'].replace(' District','').replace(' Hill','') \n",
    "\n",
    "\n",
    "# Clean 'state_district' column for various DataFrames\n",
    "\n",
    "# DataFrame loop (assuming shp_1999, shp_2004, etc. are your DataFrames)\n",
    "for df in [shp_1999, shp_2004, shp_2007, shp_2011, shp_2014, shp_2017]:\n",
    "    # Replace \" District\" and \" Hill\" substrings in the 'state_district' column (case-sensitive)\n",
    "    df['state_district'] = df['state_district'].str.replace(' District', '', regex=False).str.replace(' Hill', '', regex=False)\n",
    "\n",
    "# Now all your DataFrames have the 'state_district' column cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shp_2017.replace('Satkhira','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Habiganj','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Joypurhat','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Manikganj','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Cumilla','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Narsingdi','Shatkhira', inplace=True)\n",
    "\n",
    "\n",
    "# shp_2017.replace('Rangpur','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Kushtia','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Munshiganj','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Chapai Nawabganj','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Narayanganj','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Sirajganj','Shatkhira', inplace=True)\n",
    "\n",
    "\n",
    "# shp_2017.replace('লালমনিরহাট জেলা','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Chuadanga','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Barguna','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Chattogram','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Bogura','Shatkhira', inplace=True)\n",
    "# shp_2017.replace('Jashore','Shatkhira', inplace=True)\n",
    "temp1 = [str(element).replace(' District','').replace(' Hill','') for element in shp_2017.state_district.unique()]\n",
    "temp2 = bgd_adm.NAME_2.unique()\n",
    "\n",
    "list(set(temp1) - set(temp2))\n",
    "\n",
    "len(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shp_1999.state_district.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace nan in state_district to the value in state\n",
    "shp_1999.state_district = shp_1999.state_district.fillna(shp_1999.state.replace(' Division',''))\n",
    "shp_2004.state_district = shp_2004.state_district.fillna(shp_2004.state.replace(' Division',''))\n",
    "shp_2007.state_district = shp_2007.state_district.fillna(shp_2007.state.replace(' Division',''))\n",
    "shp_2011.state_district = shp_2011.state_district.fillna(shp_2011.state.replace(' Division',''))\n",
    "shp_2014.state_district = shp_2014.state_district.fillna(shp_2014.state.replace(' Division',''))\n",
    "shp_2017.state_district = shp_2017.state_district.fillna(shp_2017.state.replace(' Division',''))\n",
    "\n",
    "# merging with the crosswal\n",
    "\n",
    "shp_1999 = shp_1999.merge(crosswalk, left_on=\"state_district\", right_on=\"Name_shp\", how=\"left\")\n",
    "shp_2004 = shp_2004.merge(crosswalk, left_on=\"state_district\", right_on=\"Name_shp\", how=\"left\")\n",
    "shp_2007 = shp_2007.merge(crosswalk, left_on=\"state_district\", right_on=\"Name_shp\", how=\"left\")\n",
    "shp_2011 = shp_2011.merge(crosswalk, left_on=\"state_district\", right_on=\"Name_shp\", how=\"left\")\n",
    "shp_2014 = shp_2014.merge(crosswalk, left_on=\"state_district\", right_on=\"Name_shp\", how=\"left\")\n",
    "shp_2017 = shp_2017.merge(crosswalk, left_on=\"state_district\", right_on=\"Name_shp\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the dataframes shapefiles dataset to excel \n",
    "merged_shapefile = pd.concat([shp_1999, shp_2004,shp_2007, shp_2011,shp_2014,shp_2017])\n",
    "merged_shapefile.to_excel(os.getcwd() + '\\\\input\\\\shapefile_data\\\\shapefiles_corrected.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing merged_shapefile\n",
    "merged_shapefile = pd.read_excel(os.getcwd() + '\\\\input\\\\shapefile_data\\\\shapefiles_corrected.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the exposition variables\n",
    "\n",
    "\n",
    "lst_mst_affctd_1 = housing_dmg[housing_dmg[\"damaged_hh\"] >= housing_dmg[\"damaged_hh\"].quantile(4/5)][\"District corrected\"].astype(\"str\")\n",
    "lst_mst_affctd_2 = housing_dmg_2[housing_dmg_2[\"damaged_hh\"] >= housing_dmg_2[\"damaged_hh\"].quantile(4/5)][\"District corrected\"].astype(\"str\")\n",
    "\n",
    "\n",
    "lst_mst_affctd_bis_1 = housing_dmg[housing_dmg[\"damaged_hh\"] >= housing_dmg[\"damaged_hh\"].quantile(1/5)][\"District corrected\"].astype(\"str\")\n",
    "lst_mst_affctd_bis_2 = housing_dmg_2[housing_dmg_2[\"damaged_hh\"] >= housing_dmg_2[\"damaged_hh\"].quantile(1/5)][\"District corrected\"].astype(\"str\")\n",
    "\n",
    "\n",
    "lst_mst_affctd_bis_3 = housing_dmg[housing_dmg[\"Fully Damaged Households\"] >= housing_dmg[\"Fully Damaged Households\"].quantile(4/5)][\"District corrected\"].astype(\"str\")\n",
    "lst_mst_affctd_bis_4 = housing_dmg_2[housing_dmg_2[\"No. of Affected People\"] >= housing_dmg_2[\"No. of Affected People\"].quantile(4/5)][\"District corrected\"].astype(\"str\")\n",
    "\n",
    "lst_mst_affctd_bis_5 = housing_dmg_2[housing_dmg_2[\"No. of Affected People\"] >= housing_dmg_2[\"No. of Affected People\"].median()][\"District corrected\"].astype(\"str\")\n",
    "## superieur au quantile sup\n",
    "merged_shapefile[\"exposed_distr_1\"] = np.where(merged_shapefile[\"state_district\"].isin(lst_mst_affctd_1),1,0)\n",
    "merged_shapefile[\"exposed_distr_1\"] = merged_shapefile[\"exposed_distr_1\"].astype(\"category\")\n",
    "\n",
    "merged_shapefile[\"exposed_distr_2\"] = np.where(merged_shapefile[\"state_district\"].isin(lst_mst_affctd_2),1,0)\n",
    "merged_shapefile[\"exposed_distr_2\"] = merged_shapefile[\"exposed_distr_2\"].astype(\"category\")\n",
    "\n",
    "## At least one damaged\n",
    "\n",
    "merged_shapefile[\"exposed_distr_bis_1\"] = np.where(merged_shapefile[\"state_district\"].isin(lst_mst_affctd_bis_1),1,0)\n",
    "merged_shapefile[\"exposed_distr_bis_1\"] = merged_shapefile[\"exposed_distr_bis_1\"].astype(\"category\")\n",
    "\n",
    "merged_shapefile[\"exposed_distr_bis_2\"] = np.where(merged_shapefile[\"state_district\"].isin(lst_mst_affctd_bis_2),1,0)\n",
    "merged_shapefile[\"exposed_distr_bis_2\"] = merged_shapefile[\"exposed_distr_bis_2\"].astype(\"category\")\n",
    "\n",
    "## fully and number of affected hh\n",
    "\n",
    "merged_shapefile[\"exposed_distr_bis_3\"] = np.where(merged_shapefile[\"state_district\"].isin(lst_mst_affctd_bis_3),1,0)\n",
    "merged_shapefile[\"exposed_distr_bis_3\"] = merged_shapefile[\"exposed_distr_bis_3\"].astype(\"category\")\n",
    "\n",
    "merged_shapefile[\"exposed_distr_bis_4\"] = np.where(merged_shapefile[\"state_district\"].isin(lst_mst_affctd_bis_4),1,0)\n",
    "merged_shapefile[\"exposed_distr_bis_4\"] = merged_shapefile[\"exposed_distr_bis_4\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_mst_affctd_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merging shapefiles with individual datasets\n",
    "df_shp = df.merge(merged_shapefile, on=\"DHSID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_mst_affctd_bis_2.sort_values()\n",
    "\n",
    "# 24      Bagerhat\n",
    "# 2        Borgona\n",
    "# 3      Jhalakati\n",
    "# 17     Madaripur\n",
    "# 4     Patuakhali\n",
    "# 5       Pirojpur\n",
    "\n",
    "# damaged_hh fully damaged by district population\n",
    "# 0       Bagerhat\n",
    "# 3        Borgona\n",
    "# 13     Jhalakati\n",
    "# 25    Patuakhali\n",
    "# 26      Pirojpur\n",
    "# 28    Shariatpur\n",
    "\n",
    "# affected people normalized by pop size\n",
    "# 2        Borgona\n",
    "# 3      Jhalakati\n",
    "# 4     Patuakhali\n",
    "# 5       Pirojpur\n",
    "# 17     Madaripur\n",
    "# 24      Bagerhat\n",
    "\n",
    "\n",
    "# Fully Damaged Households\n",
    "# 0      Bagerhat\n",
    "# 1       Borgona\n",
    "# 2       Barisal\n",
    "# 5     Jhalakati\n",
    "# 8    Patuakhali\n",
    "# 9      Pirojpur\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These unmatched shapefile are those for wich the data are not well defined\n",
    "lst1 = df.DHSID.unique()\n",
    "lst2 = merged_shapefile.DHSID.unique()\n",
    "\n",
    "print(list(set(lst1) - set(lst2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.state_district.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_shapefile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection relevant DHS sample\n",
    "df_interest = df_shp[(df_shp[\"YEAR\"]==2007) | (df_shp[\"YEAR\"]==2011)]\n",
    "\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_interest.to_excel(os.getcwd() + \"\\\\output\\\\data\\\\df_interest.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_mean_line_survived(x, **kwargs):\n",
    "    ls = {\"Male\":\"-\",\"Female\":\"-\"}\n",
    "    plt.axvline(x.mean(), linestyle = ls[kwargs.get(\"label\")], \n",
    "                color = kwargs.get(\"color\",\"g\"))\n",
    "    #txkw = dict(size=12, color = kwargs.get(\"color\", \"g\"), rotation=90)\n",
    "    #tx = \"mean: {:.2f}, std: {:.2f}\".format(x.mean(),x.std())\n",
    "    #plt.text(x.mean()+1, 0.052, tx, **txkw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shp.columns = [n if n != \"KIDSEX\" else \"Gender\" for n in df_shp.columns]\n",
    "df_shp.columns = [n if n != \"exposed_distr_1\" else \"exposed_district\" for n in df_shp.columns]\n",
    "df_shp.columns = [n if n != \"born_after\" else \"born_after_2007\" for n in df_shp.columns]\n",
    "\n",
    "g = sns.FacetGrid(df_shp, col=\"exposed_district\", row=\"born_after_2007\", hue=\"Gender\", margin_titles=True,ylim=(0, 0.35), xlim=(-5, 5),sharex=True, sharey=True)\n",
    "g.map(sns.kdeplot, \"HWHAZWHO\",  fill=True)\n",
    "g.add_legend()\n",
    "\n",
    "g.map(vertical_mean_line_survived, 'HWHAZWHO')\n",
    "#g5 = sns.displot(data=df, col='YEAR', hue='KIDSEX', x='HWWAZWHO', kind='kde', col_wrap=3,fill=True, facet_kws={'sharey': False, 'sharex': False})\n",
    "\n",
    "# Add text to each plot for relevant popultion size\n",
    "# NOTE - don't need to filter on ['Age'].isnull() for children, as 'is_child'=True only possible for children with 'Age' data\n",
    "# for row in range(grid.axes.shape[0]):\n",
    "#     grid.axes[row, 0].text(60.2, 0.052, 'Survived n = '+str(titanic_data.loc[titanic_data['Pclass']==row+1].loc[titanic_data['is_child_def']==False].loc[titanic_data['Age'].isnull()==False].loc[titanic_data['Survived']==1]['is_male'].sum()), size = 12)\n",
    "#     grid.axes[row, 1].text(60.2, 0.052, 'Survived n = '+str(titanic_data.loc[titanic_data['Pclass']==row+1].loc[titanic_data['is_child_def']==False].loc[titanic_data['Age'].isnull()==False].loc[titanic_data['Survived']==1]['is_female'].sum()), size = 12)\n",
    "#     grid.axes[row, 0].text(60.2, 0.047, 'Perished n = '+str(titanic_data.loc[titanic_data['Pclass']==row+1].loc[titanic_data['is_child_def']==False].loc[titanic_data['Age'].isnull()==False].loc[titanic_data['Survived']==0]['is_male'].sum()), size = 12)\n",
    "#     grid.axes[row, 1].text(60.2, 0.047, 'Perished n = '+str(titanic_data.loc[titanic_data['Pclass']==row+1].loc[titanic_data['is_child_def']==False].loc[titanic_data['Age'].isnull()==False].loc[titanic_data['Survived']==0]['is_female'].sum()), size = 12)\n",
    "#ax.set(xlabel='Height for Age Z-score (HAZ)', ylabel='common ylabel')\n",
    "g.set_axis_labels(\"Height for Age Z-score (HAZ)\")\n",
    "g.set_ylabels('density', size=12)\n",
    "#plt.legend(labels= ['Male', 'Female'], loc = (1.04,0.85))\n",
    "#filename = 'Weight for Age Z-score density'\n",
    "#plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg', facecolor=facecolor)\n",
    "filename=\"by_gender_distribution_haz_norm\"\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cumulative\n",
    "\n",
    "g = sns.FacetGrid(df_shp, col=\"exposed_district\", row=\"born_after_2007\", hue=\"Gender\", margin_titles=True,sharex=True, sharey=True)\n",
    "g.map(sns.ecdfplot, \"HWHAZWHO\",   stat='proportion')\n",
    "g.add_legend()\n",
    "\n",
    "#g.map(vertical_mean_line_survived, 'HWHAZWHO')\n",
    "\n",
    "g.set_axis_labels(\"Height for Age Z-score (HAZ)\")\n",
    "g.set_ylabels('density', size=12)\n",
    "\n",
    "filename=\"by_gender_distribution_haz_cum\"\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_interest.columns = [n if n != \"KIDSEX\" else \"Gender\" for n in df_interest.columns]\n",
    "# df_interest.columns = [n if n != \"exposed_distr_1\" else \"exposed_district\" for n in df_interest.columns]\n",
    "# df_interest.columns = [n if n != \"born_after\" else \"born_after_2007\" for n in df_interest.columns]\n",
    "\n",
    "g = sns.FacetGrid(df_shp, col=\"exposed_district\", row=\"born_after_2007\", hue=\"Gender\", margin_titles=True,ylim=(0, 0.4), xlim=(-5, 5),sharex=True, sharey=True)\n",
    "g.map(sns.kdeplot, \"HWWAZWHO\",  fill=True)\n",
    "g.add_legend()\n",
    "\n",
    "g.map(vertical_mean_line_survived, 'HWWAZWHO')\n",
    "\n",
    "g.set_axis_labels(\"Weight for Age Z-score (WAZ)\")\n",
    "g.set_ylabels('proportion', size=12)\n",
    "\n",
    "#g.axes[0,1].set_xlabel('axes label 1')\n",
    "\n",
    "filename=\"by_gender_distribution_waz_interested\"\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cumulative\n",
    "\n",
    "g = sns.FacetGrid(df_shp, col=\"exposed_district\", row=\"born_after_2007\", hue=\"Gender\", margin_titles=True,sharex=True, sharey=True)\n",
    "g.map(sns.ecdfplot, \"HWWAZWHO\",   stat='proportion')\n",
    "g.add_legend()\n",
    "\n",
    "#g.map(vertical_mean_line_survived, 'HWHAZWHO')\n",
    "\n",
    "g.set_axis_labels(\"Weight for Age Z-score (WAZ)\")\n",
    "g.set_ylabels('proportion', size=12)\n",
    "\n",
    "filename=\"by_gender_distribution_waz_cum\"\n",
    "plt.savefig( os.getcwd() + '\\\\output\\\\img\\\\' + filename+'.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Représentation graphique des regions du Bangladesh\n",
    "\n",
    "## fig et ax représentent la base de la carte\n",
    "fig, ax = plt.subplots(1, figsize=(10,8))\n",
    "cmap = 'Greys'\n",
    "df_interest.plot(column=\"HWWAZWHO\", scheme=\"quantiles\", k=8, cmap=cmap, ax = ax, edgecolor='0.8', legend=True,legend_kwds={\"loc\": \"center left\", \"bbox_to_anchor\": (1, 0.5), \"interval\": False})\n",
    "ax.axis('off')\n",
    "ax.set_title('Fully Damaged Households', fontdict={'fontsize': '15'})\n",
    "# Create an annotation for the data source\n",
    "ax.annotate('Source: London Datastore, 2014',xy=(0.1, .08),  xycoords='figure fraction', horizontalalignment='left', verticalalignment='bottom', fontsize=12, color='#555555')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interest.groupby(['DHSID','geometry'])[['HWWAZWHO', 'HWHAZWHO']].mean().plot(column=\"HWWAZWHO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interest = gpd.GeoDataFrame(df, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_shp.born_after, [df_shp.exposed_distr_1, df_shp.KIDSEX], values=df_shp.HWWAZWHO, aggfunc='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using data for Ministry of Food and Disaster Management"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
